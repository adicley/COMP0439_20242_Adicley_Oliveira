"id","title","body","created_at","closed_at","days_to_close","issue_priority","milestone","author_name","user_atribute_to_resolve_issue","issue_classification"
2,"Enable unused variable checking in ruff","### üöÄ The feature, motivation and pitch

pytorch uses ruff, but with several potentially useful flags turned off.

In particular, [F841](https://docs.astral.sh/ruff/rules/unused-variable/#unused-variable-f841) detects unused variables, which have caused issues in the past (e.g. https://github.com/pytorch/pytorch/pull/132744) and are if nothing else a waste of space.

There are a few hundred unused variables in the pytorch code; most are trivial, a few are real issues.

I have already started on doing this as a spare-time project. The main pull requests follow:

- [x] https://github.com/pytorch/pytorch/pull/133492
- [x] https://github.com/pytorch/pytorch/pull/136963
- [x] https://github.com/pytorch/pytorch/pull/134665
- [x] https://github.com/pytorch/pytorch/pull/136964
- [x] https://github.com/pytorch/pytorch/pull/136359
- [x] https://github.com/pytorch/pytorch/pull/136965

In addition, several tiny commits were extracted from this on request in code reviews:

- [x] https://github.com/pytorch/pytorch/pull/138480
- [x] https://github.com/pytorch/pytorch/pull/138473
- [x] https://github.com/pytorch/pytorch/pull/138474
- [x] https://github.com/pytorch/pytorch/pull/138475
- [x] https://github.com/pytorch/pytorch/pull/138094
- [x] https://github.com/pytorch/pytorch/pull/138086
- [x] https://github.com/pytorch/pytorch/pull/138085
- [x] https://github.com/pytorch/pytorch/pull/138000
- [x] https://github.com/pytorch/pytorch/pull/137998
- [x] https://github.com/pytorch/pytorch/pull/137989
- [x] https://github.com/pytorch/pytorch/pull/137985
- [x] https://github.com/pytorch/pytorch/pull/143191
- [x] https://github.com/pytorch/pytorch/pull/143389
- [x] https://github.com/pytorch/pytorch/pull/143396
- [x] https://github.com/pytorch/pytorch/pull/143399
- [x] https://github.com/pytorch/pytorch/pull/143407


### Alternatives

Leaving things the way they were. :-)
","2024-11-30","2024-12-22","22","priorized",NULL,"Tom Ritchford","albanD","OTHERS"
3,"Not all methods from `mps/operations/MultiTensorApply.h` are covered by tests","### üêõ Describe the bug

While working on https://github.com/pytorch/pytorch/pull/140896
I've accidentally deleted body of
https://github.com/pytorch/pytorch/blob/e80b1b2870ad568aebdbb7f5205f6665f843e0ea/aten/src/ATen/native/mps/operations/MultiTensorApply.h#L76-L85 (see https://github.com/pytorch/pytorch/pull/140896/commits/44c516332926e856ee5a456c20a5d6dfea0e5706#diff-13e6ddc055c896f13df4431cb5cd90f6432ac0ab32ac77d726f75a25fa470667 ), but CI turned out green.

I see three explanations for this behavior:
 - MultiTensorApply for MPS is not covered by tests
 - MTA tests are not executed during `ciflow/mps` (or TD decided to skip the test)
 - This method is not used at all by the codebase, and should be deleted

### Versions

CI

cc @mruberry @ZainRizvi @crcrpar @mcarilli @janeyx99 @kulinseth @albanD @DenisVieriu97 @jhavukainen","2024-11-18","2024-11-19","1","priorized",NULL,"Nikita Shulga","qqaatw","SOFTWARE ARCHITECTURE"
4,"Duplicate declaration of double_tensor in the ""Promotion examples"" documentation","### üìö The doc issue

I noticed a small issue in the PyTorch documentation on [Promotion examples](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtyp) In the provided example script, the variable `double_tensor` is declared twice.
```python
float_tensor = torch.ones(1, dtype=torch.float)
double_tensor = torch.ones(1, dtype=torch.double)
complex_float_tensor = torch.ones(1, dtype=torch.complex64)
complex_double_tensor = torch.ones(1, dtype=torch.complex128)
int_tensor = torch.ones(1, dtype=torch.int)
long_tensor = torch.ones(1, dtype=torch.long)
uint_tensor = torch.ones(1, dtype=torch.uint8)
double_tensor = torch.ones(1, dtype=torch.double)
bool_tensor = torch.ones(1, dtype=torch.bool)
long_zerodim = torch.tensor(1, dtype=torch.long)
int_zerodim = torch.tensor(1, dtype=torch.int)

torch.add(5, 5).dtype
(int_tensor + 5).dtype
(int_tensor + long_zerodim).dtype
(long_tensor + int_tensor).dtype
(bool_tensor + long_tensor).dtype
(bool_tensor + uint_tensor).dtype
(float_tensor + double_tensor).dtype
(complex_float_tensor + complex_double_tensor).dtype
(bool_tensor + int_tensor).dtype
torch.add(long_tensor, float_tensor).dtype
```


### Suggest a potential alternative/fix

I would like to submit a PR to remove the second declaration of double_tensor. Please let me know if this fix sounds good.

cc @svekars @brycebortree @sekyondaMeta @AlannaBurke","2024-11-17","2024-11-18","1","priorized",NULL,"AAnirudh07","AAnirudh07","SOFTWARE ARCHITECTURE"
5,"ruff linter gives traceback on Python files which can't be parsed","### üêõ Describe the bug

Running the `ruff` linter over a commit which includes a Python file which won't parse (due to i.e. mismatched brackets or bad indenting) prints a traceback and suggests filing an issue against the linter.

The correct action would be to report some sort of general ""Invalid Python syntax"" on the file itself in a format similar to other ruff errors.

This is pretty minor, but it irks me when I see it, and since I'm working with linters these last couple of weeks, I can just fix it while I'm in that directory, anyway. üôÇ 

```
>>> General linter failure:

  Error (RUFF) Linter failed
    Linter failed. This a bug, please file an issue against the linter
    maintainer.
    
    CONTEXT:
    Linter command failed with non-zero exit code.
    STDERR:
    <MainThread:DEBUG> $ /home/rec/.conda/envs/pytorch-dev/bin/python3 -m ruff
    check --exit-zero --quiet --output-format=json --config=pyproject.toml /
    home/rec/git/pytorch/tools/linter/adapters/_common_code.py /home/rec/git/
    pytorch/tools/linter/adapters/set_linter.py
    <MainThread:DEBUG> took 58ms
    Traceback (most recent call last):
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    465, in <module>
        main()
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    424, in main
        lint_messages = check_files(
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    273, in check_files
        return [
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    288, in <listcomp>
        severity=severities.get(vuln[""code""],
    get_issue_severity(vuln[""code""])),
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    172, in get_issue_severity
        if any(
      File ""/home/rec/git/pytorch/tools/linter/adapters/ruff_linter.py"", line
    173, in <genexpr>
        code.startswith(x)
    AttributeError: 'NoneType' object has no attribute 'startswith'
    
    
    STDOUT:
```    


### Versions

```
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (conda-forge gcc 12.3.0-7) 12.3.0
Clang version: Could not collect
CMake version: version 3.30.5
Libc version: glibc-2.35

Python version: 3.9.20 | packaged by conda-forge | (main, Sep 30 2024, 17:49:10)  [GCC 13.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35
Is CUDA available: N/A
CUDA runtime version: 12.4.131
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 2060
GPU 1: NVIDIA GeForce RTX 2060

Nvidia driver version: 560.35.03
cuDNN version: Probably one of the following:
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.7.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_adv.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_cnn.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_engines_precompiled.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_engines_runtime_compiled.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_graph.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_heuristic.so.9
/usr/local/cuda-12.3.2/targets/x86_64-linux/lib/libcudnn_ops.so.9
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        43 bits physical, 48 bits virtual
Byte Order:                           Little Endian
CPU(s):                               64
On-line CPU(s) list:                  0-63
Vendor ID:                            AuthenticAMD
Model name:                           AMD Ryzen Threadripper 3970X 32-Core Processor
CPU family:                           23
Model:                                49
Thread(s) per core:                   2
Core(s) per socket:                   32
Socket(s):                            1
Stepping:                             0
Frequency boost:                      enabled
CPU max MHz:                          3700.0000
CPU min MHz:                          2200.0000
BogoMIPS:                             7400.24
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es
Virtualization:                       AMD-V
L1d cache:                            1 MiB (32 instances)
L1i cache:                            1 MiB (32 instances)
L2 cache:                             16 MiB (32 instances)
L3 cache:                             128 MiB (8 instances)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-63
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Not affected
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection
Vulnerability Spec rstack overflow:   Mitigation; safe RET
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Not affected

Versions of relevant libraries:
[pip3] flake8==6.1.0
[pip3] flake8-bugbear==23.3.23
[pip3] flake8-comprehensions==3.15.0
[pip3] flake8-executable==2.1.3
[pip3] flake8-logging-format==0.9.0
[pip3] flake8-pyi==23.3.1
[pip3] flake8-simplify==0.19.3
[pip3] mypy==1.11.2
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.26.0
[pip3] optree==0.13.0
[pip3] pytorch-triton==3.1.0+cf34004b8a
[conda] cuda-cudart               12.4.127             he02047a_2    conda-forge
[conda] cuda-cudart-dev           12.4.127             he02047a_2    conda-forge
[conda] cuda-cudart-dev_linux-64  12.4.127             h85509e4_2    conda-forge
[conda] cuda-cudart-static        12.4.127             he02047a_2    conda-forge
[conda] cuda-cudart-static_linux-64 12.4.127             h85509e4_2    conda-forge
[conda] cuda-cudart_linux-64      12.4.127             h85509e4_2    conda-forge
[conda] cuda-cupti                12.4.127             he02047a_2    conda-forge
[conda] cuda-cupti-dev            12.4.127             he02047a_2    conda-forge
[conda] cuda-libraries-dev        12.4.1               ha770c72_1    conda-forge
[conda] cuda-nvrtc                12.4.127             he02047a_2    conda-forge
[conda] cuda-nvrtc-dev            12.4.127             he02047a_2    conda-forge
[conda] cuda-nvtx                 12.4.127             he02047a_2    conda-forge
[conda] cuda-nvtx-dev             12.4.127             ha770c72_2    conda-forge
[conda] cuda-opencl               12.4.127             he02047a_1    conda-forge
[conda] cuda-opencl-dev           12.4.127             he02047a_1    conda-forge
[conda] cudnn                     9.3.0.75             h93bb076_0    conda-forge
[conda] libcublas                 12.4.5.8             he02047a_2    conda-forge
[conda] libcublas-dev             12.4.5.8             he02047a_2    conda-forge
[conda] libcufft                  11.2.1.3             he02047a_2    conda-forge
[conda] libcufft-dev              11.2.1.3             he02047a_2    conda-forge
[conda] libcurand                 10.3.5.147           he02047a_2    conda-forge
[conda] libcurand-dev             10.3.5.147           he02047a_2    conda-forge
[conda] libcusolver               11.6.1.9             he02047a_2    conda-forge
[conda] libcusolver-dev           11.6.1.9             he02047a_2    conda-forge
[conda] libcusparse               12.3.1.170           he02047a_2    conda-forge
[conda] libcusparse-dev           12.3.1.170           he02047a_2    conda-forge
[conda] libmagma                  2.8.0                h0af6554_0    conda-forge
[conda] libmagma_sparse           2.8.0                h0af6554_0    conda-forge
[conda] libnvjitlink              12.4.127             he02047a_2    conda-forge
[conda] libnvjitlink-dev          12.4.127             he02047a_2    conda-forge
[conda] magma                     2.8.0                h51420fd_0    conda-forge
[conda] mkl                       2024.2.2            ha957f24_15    conda-forge
[conda] mkl-include               2024.2.2            ha957f24_15    conda-forge
[conda] numpy                     1.26.0                   pypi_0    pypi
[conda] optree                    0.13.0           py39h74842e3_0    conda-forge
[conda] pytorch-triton            3.1.0+cf34004b8a          pypi_0    pypi
[conda] torchfix                  0.4.0                    pypi_0    pypi
```
","2024-11-10","2024-12-08","28","priorized",NULL,"Tom Ritchford","Tom Ritchford","PROJECT PATTERNS"
6,"When running PyTorch on macOS with Metal validation enabled, it crashes","### üêõ Describe the bug

When Metal validation is enabled for macOS by setting `METAL_DEVICE_WRAPPER_TYPE=1`, it crashes with the following output:
```
validateComputeFunctionArguments:1056: failed assertion `Compute Function(gather_kernel_1): missing buffer binding at index 2 for size[0].'
Abort trap: 6
```

It appears to be caused by `-setBytes:length:` being given a length parameter of 0, which metal considers as not having set the buffer binding for that variable. Metal validation is turned on by default when running a macOS app from Xcode, so this crash can occur for PyTorch runs from Xcode without the developer understanding that it's due to Metal validation.

### Versions

Collecting environment information...
PyTorch version: 2.3.1
Is debug build: False
CUDA used to build PyTorch: None
ROCM used to build PyTorch: N/A

OS: macOS 14.5 (arm64)
GCC version: Could not collect
Clang version: 15.0.0 (clang-1500.0.40.1)
CMake version: version 3.27.4
Libc version: N/A

Python version: 3.12.0 (main, Jul  1 2024, 13:41:52) [Clang 15.0.0 (clang-1500.0.40.1)] (64-bit runtime)
Python platform: macOS-14.5-arm64-arm-64bit
Is CUDA available: False
CUDA runtime version: No CUDA
CUDA_MODULE_LOADING set to: N/A
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Apple M1 Max

Versions of relevant libraries:
[pip3] numpy==1.26.4
[pip3] torch==2.3.1
[pip3] torchaudio==2.3.1
[pip3] torchsde==0.2.6
[pip3] torchvision==0.18.1
[conda] Could not collect

cc @kulinseth @albanD @malfet @DenisVieriu97 @jhavukainen","2024-07-09","2024-07-10","1","priorized",NULL,"Michael Eisel","Michael Eisel","OTHERS"
7,"[BE] wrap deprecated function/class with `typing_extensions.deprecated` for better IDE integration","If you have a question or would like help and support, please ask at our
[forums](https://discuss.pytorch.org/).

If you are submitting a feature request, please preface the title with [feature request].
If you are submitting a bug report, please fill in the following details.

## Issue description

Provide a short description.

Currently, we use `warnings.warn(""deprecation message"", DeprecationWarning)` to warn users of functionality deprecation and pending removals.

Python 3.13 adds a new API [`warnings.deprecated`](https://docs.python.org/zh-cn/3.13/library/warnings.html#warnings.deprecated) for deprecation indication. This feature has better static type checker support. When a deprecated object is imported or called, the code will ~~be struck out~~ in the IDE.

[`warnings.deprecated`](https://docs.python.org/zh-cn/3.13/library/warnings.html#warnings.deprecated) is backported in `typing-extensions` 4.5.0 ([typing_extensions.deprecated](https://typing-extensions.readthedocs.io/en/latest/#typing_extensions.deprecated)), which is already one of our dependency.

https://github.com/pytorch/pytorch/blob/3e826c477aba0a0cf9bea4131cee57aac21c1c14/requirements.txt#L11

Screenshot to illustrate the IDE integration with ~~strick out annotation~~.

<img width=""1026"" alt=""image"" src=""https://github.com/pytorch/pytorch/assets/16078332/9445111f-670d-4339-96fc-3985ce3889bf"">

## Code example

Please try to provide a minimal example to repro the bug.
Error messages and stack traces are also helpful.

```python
from typing_extensions import deprecated


@deprecated(
    ""Function `deprecated_func` is deprecated and will be removed in a future release. Use `new_func` instead.""
)
def deprecated_func():
    return ""deprecated_func""


def new_func():
    return ""new_func""


@deprecated(
    ""Class `DeprecatedClass` is deprecated and will be removed in a future release. Use `NewClass` instead.""
)
class DeprecatedClass:
    pass


class InheritedDeprecatedClass(DeprecatedClass):
    pass


class NewClass:
    pass


class Foo:
    @deprecated(""Method `Foo.bar` is deprecated and will be removed in a future release."")
    def bar(self):
        print(""Hello, world!"")


if __name__ == ""__main__"":
    print(deprecated_func())
    print(new_func())
    instance = DeprecatedClass()
```

cc @ezyang @malfet @xuzhao9 @gramster","2024-05-22","2024-06-02","11","priorized",NULL,"Xuehai Pan","ezyang","OTHERS"
8,"Update test_cuda.py and test_torch.py optim tests to use OptimizerInfo and optim_db","### üöÄ The feature, motivation and pitch

**Update: all tasks below have in progress PRs currently!**

In test_cuda.py and test_torch.py, we have the following optimizer tests that should be migrated to use the new OptimizerInfo infrastructure. (See any of the tests in TestOptimRenewed for how to apply OptimizerInfos.) The basic structure to expect for an updated test will look like:

```
    @optims([optim for optim in optim_db if ...], dtypes=[torch.float32])
    def test_name(self, device, dtype, optim_info):
        optim_cls = optim_info.optim_cls
        optim_inputs = optim_info.optim_inputs_func(device=device)
        for optim_input in optim_inputs:
            params = ...
            // processing, like updating grads

            optimizer = optim_cls(params, ..., **optim_input.kwargs)

            // the actual test
            ...
            self.assertEquals(...)
```

test_cuda.py:
- [ ] test_grad_scaling_autocast_fused_optimizers will be addressed in https://github.com/pytorch/pytorch/pull/124904 and by @SandishKumarHN 
- [x] test_graph_grad_scaling
- [ ] test_graph_optims
- [ ] test_graph_optims_with_explicitly_capturable_param_groups
- [ ] test_graph_scaling_fused_optimizers

test_torch.py: #125538
- [x] tests that call `_grad_scaling_autocast_test` can be combined into one test
- [x] test_params_invalidated_with_grads_invalidated_between_unscale_and_step should be tested on more configurations (like SGD fused!) 
- there are many other tests that test just one optimizer config that we can leave for now.

Feel free to take up one or a subset of those tests to migrate--edit your username next to the checklist to claim an item to submit PRs for!

### Alternatives

_No response_

### Additional context

_No response_

cc @vincentqb @jbschlosser @albanD @crcrpar","2024-04-05","2024-08-28","145","priorized",NULL,"Jane (Yuan) Xu","janeyx99","SOFTWARE ARCHITECTURE"
9,"[BE] De-dup `run_subtests` from `common_dtensor.py`, `common_fsdp.py`","The two look like they are exact copies. We should de-dup them, e.g. possibly to `common_distributed.py`.

https://github.com/pytorch/pytorch/blob/b4160fd9c7983277271d21e56344839d49c98f64/torch/testing/_internal/distributed/_tensor/common_dtensor.py#L287-L316
https://github.com/pytorch/pytorch/blob/dccc1ca839def32f4adef6d850688d517854482b/torch/testing/_internal/common_fsdp.py#L993-L1022

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @penguinwu @fegin @XilunWu @wanchaol @fduwjj @wz337 @tianyu-l @wconstab @yf225 @chauhang","2024-03-11","2024-03-22","11","priorized",NULL,"Andrew Gu","Andrew Gu","OTHERS"
10,"Better unittest isolation for global state","## Context

I've been working on getting the Dynamo tests to reset the Dynamo state before and after each unittest, otherwise, the state from one test affects subsequent tests, leading to unexpected behavior.

However, Dynamo is not the only place where we have global state. We've ran into situations in the past where changing e.g. the global default dtype in one test had a downstream effect on other tests.

Can we make unittests better by better isolating them somehow?

## Pitch

Some ideas:
- One naive way to do this is to run each test in its own process, but this is probably not feasible due to the amount of overhead it adds to each test.
- Another idea is to have some centralized place where ""all pytorch state"" is kept and reset this state before and after each test runs.
- Linters for abuse of APIs that modify global state? For example, instead of using torch.set_default_dtype, we should use a context-manager version (if we have one) so it's clear the global state gets cleaned up. This would still require us to identify all of the APIs that do modify global state and put them somewhere.

cc @ZainRizvi @kit1980 @huydhn @clee2000 @janeyx99, @jbschlosser, @pytorch/pytorch-dev-infra for ideas ","2024-01-08","2024-01-09","1","priorized",NULL,"Richard Zou","PaliC","OTHERS"
11,"Add testing regarding SparseAdam state_dicts","### üöÄ The feature, motivation and pitch

SparseAdam, unlike the other optimizers, does not test any state_dict functionality. All other optims call _test_state_dict, but due to SparseAdam's only handling sparse gradients, it does not easily fall into the architecture that already exists.

We should consider giving it its own closure that would update parameters like in `rosenbrock` and creating special case tests for it.

### Alternatives

Leave SparseAdam state_dict handling untested

### Additional context

_No response_

cc @vincentqb @jbschlosser @albanD @crcrpar @mruberry @ZainRizvi","2023-12-28","2024-07-16","201","priorized",NULL,"Jane (Yuan) Xu","Jane (Yuan) Xu","SOFTWARE ARCHITECTURE"
12,"duplicate code in python_arg_parser.cpp ","### üêõ Describe the bug




‚Äòstatic bool is_int_list(
    PyObject* obj,
    int broadcast_size,
    int64_t* failed_idx = nullptr) {
  if (PyTuple_Check(obj) || PyList_Check(obj)) {
    auto len = PySequence_Size(obj);
    if (len == 0) {
      return true;
    }

    auto item = py::reinterpret_steal<py::object>(PySequence_GetItem(obj, 0));
    bool int_first = false;
    if (THPUtils_checkIndex(item.ptr())) {
      // we still have to check that the rest of items are NOT symint nodes
      int_first = true;
    }

    // Make sure none of the later arguments are SymInt
    // NB: do NOT check that the later arguments are ints, as this is
    // BC-breaking for FX
    for (Py_ssize_t i = 1; i < len; i++) {
      if (torch::is_symint(
              py::reinterpret_steal<py::object>(PySequence_GetItem(obj, i)))) {
        if (failed_idx != nullptr) {
          *failed_idx = i;
        }
        return false;
      }
    }

    if (int_first) {
      return true;
    }

    // in dynamo, FakeTensor is qualified for INT_LIST
    if (is_dynamo_compiling && THPVariable_Check(item.ptr())) {
      auto& var = THPVariable_Unpack(item.ptr());
      if (var.numel() != 1 || !var.sizes().empty() ||
          !at::isIntegralType(
              var.dtype().toScalarType(), /*include_bool*/ true)) {
        if (failed_idx != nullptr) {
          *failed_idx = 0;
        }
        return false;
      }
      return true;
    }

    // NOTE: JIT tracer allows arbitrary scalar tensors to act as ints
    // in an intlist argument. Even float or complex scalar tensors.
    bool r =
        (jit::tracer::isTracing() && THPVariable_Check(item.ptr()) &&
         THPVariable_Unpack(item.ptr()).sizes().empty());
    if (!r && failed_idx != nullptr) {
      *failed_idx = 0;
    }
    return r;
  }
  // if a size is specified (e.g. IntArrayRef[2]) we also allow passing a single
  // int
  return broadcast_size > 0 && THPUtils_checkLong(obj);
}
‚Äò
for above code, main part of it is duplicated with is_int_or_symint_list, except the `is_int_or_symint` check part.
it is annoying and hard to follow.

### Versions

branch main:
commit: [aa390ce](https://github.com/pytorch/pytorch/commit/aa390cec21108a17710be134190b4d7654222fed)

cc @ezyang @bhosmer @smessmer @ljk53 @bdhirsh","2023-12-01","2023-12-07","6","priorized",NULL,"bruceSz","zou3519","OTHERS"
13,"Why is flake8 F821 disabled","Flake8 F821 ""undefined name"" is one of the most useful flake8 rules (see recent https://github.com/pytorch/pytorch/pull/112336 as an example).

Yet the rule is disabled:
https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/.flake8#L10
https://github.com/pytorch/pytorch/blob/bbd5b935e49a54578ac88cb23ca962ab896a8c7a/pyproject.toml#L43

I think the rule needs to be at least selectively enabled. There are currently multiple actual issues in the codebase that can be caught by this rules.","2023-10-30","2024-01-01","63","priorized",NULL,"Sergii Dymchenko","Skylion007","OTHERS"
14,"Remove parameter `self` in `typeConvertIndices`","### üöÄ The feature, motivation and pitch

In the file `aten/src/ATen/TensorIndexing.h`, there's a function `typeConvertIndices`:

```c++
static inline c10::List<c10::optional<Tensor>> typeConvertIndices(
    const Tensor& /*self*/,
    std::vector<Tensor>&& indices) {
  c10::List<c10::optional<Tensor>> converted_inds;
  converted_inds.reserve(indices.size());
  for (const auto& i : indices) {
    converted_inds.push_back(std::move(i));
  }
  return converted_inds;
}
```

This function is later invoked by `dispatch_index_put_`:

```c++
static inline Tensor dispatch_index_put_(
    Tensor& self,
    std::vector<Tensor>&& indices,
    const Tensor& value) {
  return self.index_put_(
      impl::typeConvertIndices(self, std::move(indices)), value);
}
```

Upon inspection, I'm unsure about the necessity of passing the `self` parameter to `typeConvertIndices` as it seems to be unused within the function. Is it possible to simplify the function to:

```c++
static inline c10::List<c10::optional<Tensor>> typeConvertIndices(std::vector<Tensor>&& indices) {
  c10::List<c10::optional<Tensor>> converted_inds;
  converted_inds.reserve(indices.size());
  for (const auto& i : indices) {
    converted_inds.push_back(std::move(i));
  }
  return converted_inds;
}
```

by removing the `self` parameter?

### Alternatives

_No response_

### Additional context

_No response_","2023-08-25","2023-11-17","84","priorized",NULL,"Wentao Ye","yewentao256","OTHERS"
15,"Add .devcontainer to PyTorch and ensure local CPU development works","# Sumary

First step in this project: #106713 

## Status = Done
This support was added in this PR: https://github.com/pytorch/pytorch/pull/98252



Uploading pytorch_dev_container_start.mov‚Ä¶

","2023-08-07","2023-08-07","0","priorized",NULL,"Driss Guessous","drisspg","OTHERS"
16,"Enable more flake8-bugbear lints","### üöÄ The feature, motivation and pitch

* There are a lot of flake8-bugbear code reported by flake8-bugbear and ruff that need to be fixed. These would be a good starting issue for a new contributor.

This can be found be removing the codes from the ignore list in the .flake8 file and the .pyproject file for ruff.

- [ ] ""B007"", 
- [ ] ""B008"" 
- [ ] ""B017"",
- [ ] ""B018"", # Useless expression
- [ ] ""B019"", 
- [ ] ""B020"",
- [ ] ""B023"", 
- [ ] ""B024"", 
- [ ] ""B026"",
- [ ] ""B028"", # No explicit `stacklevel` keyword argument found
- [ ] ""B904"",

### Alternatives

_No response_

### Additional context

_No response_","2023-08-03","2023-11-07","96","priorized",NULL,"Aaron Gokaslan","Skylion007","OTHERS"
17,"Add actual dtype to RuntimeError(""mat1 and mat2 must have the same dtype"")","### üöÄ The feature, motivation and pitch

I was doing some testing and get the error message `RuntimeError(""mat1 and mat2 must have the same dtype"")` from a linear layer. The problem is fixed, but I would have found the issue faster, if the error message told me that one mat was float32 and the other float64. I.e. `RuntimeError(""mat1 and mat2 must have the same dtype, but got torch.float32 and torch.float64"")`.



### Alternatives

_No response_

### Additional context

_No response_

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki","2023-07-14","2023-07-17","3","low priority",NULL,"danielkallendorf","danielkallendorf","OTHERS"
18,"Refactor Adam and AdamW by abstracting out common code","AdamW differs with Adam only in the weight_decay handling. Everything else is the same. We should reuse code instead of hosting the same exact logic in two places.

One way to do this is to have certain functions in adam.py that can be accessed in adamw.py. The main work that would need to be done would be consolidating the forloop and foreach implementations. The fused implementation already takes advantage of the commonality. 

cc @vincentqb @jbschlosser @albanD @crcrpar","2023-07-10","2024-12-23","532","priorized",NULL,"Jane (Yuan) Xu","qqaatw","OTHERS"
19,"Validate buffer dtype in pre-forward hook for FSDP mixed precision tests","### üöÄ The feature, motivation and pitch

As per the comment in https://github.com/pytorch/pytorch/pull/104682/files, `test_full_precision_in_eval_buffers` would be more robust if we used an nn.Module pre-forward hook to validate the expected buffer dtype, instead of doing so after the forward pass. This would ensure that the computation took place in the right dtype.

### Alternatives

_No response_

### Additional context

_No response_

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu","2023-07-06","2023-08-03","28","priorized",NULL,"Rohan Varma","awgu","OTHERS"
20,"FlopCounterMode should support removing its hooks","### üöÄ The feature, motivation and pitch

`FlopCounterMode` leaves hooks installed on the `nn.Module` used:

```python
import torch

from torch.utils.flop_counter import FlopCounterMode

model = torch.nn.Linear(100, 100)
x = torch.randn(3, 100)

flop_counter = FlopCounterMode(model)
with flop_counter:
    model(x).sum().backward()

print(model._forward_hooks)  # OrderedDict([(1, <function FlopCounterMode._exit_module.<locals>.f at 0x7f681ab4aca0>)])
```

Introduce `FlopCounterMode.remove()` to remove these two hooks

https://github.com/pytorch/pytorch/blob/f2900420da9cd96465e84071b6fe1f7c110ed527/torch/utils/flop_counter.py#L255-L256

### Alternatives

Another option would be to install these hooks and remove them on `__enter__` and `__exit__`, removing the need to call `.remove()` after.

### Additional context

cc @Chillee as the author in https://github.com/pytorch/pytorch/commit/1b1b9c870695f8d0e2796f06f36d9f7709b5080e","2023-06-15","2023-06-20","5","priorized",NULL,"Carlos Mochol√≠","Carlos Mochol√≠","OTHERS"
21,"[BE] Refactor logic for MultiTensorApply","### üöÄ The feature, motivation and pitch

Context: https://github.com/pytorch/pytorch/pull/100811#discussion_r1187576042

It would be a valuable next step to refactor the multitensorapply code to allow for better engineering in terms of clarity. The code currently is convoluted and we should break up the for-loop into separate preprocessing and kernel call steps. Desirable outcomes:
- allow people to more easily understand what is going on
- lessen the chance that bugs will be introduced accidentally

### Alternatives

do nothing

### Additional context

_No response_

cc @crcrpar @mcarilli @ngimel","2023-05-10","2023-12-14","218","priorized",NULL,"Jane (Yuan) Xu","crcrpar","OTHERS"
22,"[BE] More informative error messages in `THPVariable_set_grad`","https://github.com/pytorch/pytorch/blob/31f311a816c026bbfca622d6121d6a7fab44260d/torch/csrc/autograd/python_variable.cpp#L896
When the gradient metadata does not match the corresponding tensor's metadata, `THPVariable_set_grad()` raises an error, e.g.:
```
RuntimeError: assigned grad has data of a different type
```
Including what leads to the mismatch would be very helpful. For example, for the above message, if we could see something like:
```
RuntimeError: assigned grad has data of type torch.float16 that differs from the required torch.float32
```
A similar idea applies for mismatched devices and sizes.


cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7 @malfet","2023-04-27","2023-05-11","14","priorized",NULL,"Andrew Gu","albanD","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
23,"PT2 CI Improvements Tracker","Use this for tracking CI improvement tasks. If you want to request a CI improvement, you can comment in this issue.

- [ ] Fix flaky accuracy failures, e.g. dla120 (@desertfire )
- [ ] #97504
- [x] #97459
- [ ] Auto update the CI skip list for benchmark tests (this is blocked by the flakiness debugging)
- [ ] Create a separate list for tests that fail on A10g but pass on A100 because of OOM, and run those tests separately on A100
- [x] Add dynamo-eager benchmark accuracy test into `periodic`
","2023-03-22","2023-07-07","107","priorized",NULL,"Bin Bao","Bin Bao","OTHERS"
24,"Unify _cast_fp_inputs_to_dtype and buffer dtype casting with DDP","### üêõ Describe the bug

DDP mixed precision uses similar methods as FSDP to cast inputs and buffers, we should unify these under torch.distributed.utils.

### Versions

n/a

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu","2023-03-08","2023-06-04","88","priorized",NULL,"Rohan Varma","awgu","OTHERS"
25,"[BE] Make ActivationWrapper an abstract class","### üöÄ The feature, motivation and pitch

As mentioned in the comments for `ActivationWrapper` in torch.distributed's activation checkpoint code, it is not meant to be instantiated directly, so we should just prevent this via using an abstract class.

### Alternatives

_No response_

### Additional context

_No response_

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu","2023-02-24","2024-07-02","494","low priority",NULL,"Rohan Varma","Rohan Varma","SOFTWARE ARCHITECTURE, PATTERNS AND ARCHITECTURAL STYLES"
26,"[ONNX] Circular import issue between torch.onnx and torch.onnx._internal.fx","torch.onnx and torch.onnx._internal.fx can't share common files right now due to circular import problems. If this can be fixed, the common functions in files, such as onnx_proto_utils.py/verification.py/options.py, could be used from both sides.

cc @BowenBao @justinchuby @wschin ","2023-02-14","2023-02-14","0","low priority",NULL,"Ti-Tai Wang","BowenBao","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
27,"[BE]: Add ruff to lintrunner - use for additional plugins like pyupgrade etc.","### üöÄ The feature, motivation and pitch

* The PyTorch codebase is huge. Formatting and linting the repo takes a lot of time. Additionally, adding new linters is a bit of challenge. https://github.com/charliermarsh/ruff is a Rust linter/fixer that implements rules for the most common of flake8 linters (pyupgrade, flake8, isort, popular flake8-plugins etc). Many of our existing linters can be combined here. The only downside is that it does reduce the number of flake8-plugins we can use (since they are all reimplemented in the framework), but it does many of the autofixes. Even if we don't replace flake8 etc with this plugin, we can use it supplement it, by using it only for other modules like pyupgrade (where you can select individual pyupgrade rules etc.). I've been using to implement various PRs and it's pretty reasonable so far. See https://github.com/pytorch/pytorch/issues/94040

### Alternatives

Keep the existing linters

### Additional context

It's also really, really fast!

cc @malfet @seemethere","2023-02-13","2023-04-24","70","priorized",NULL,"Aaron Gokaslan","Aaron Gokaslan","OTHERS"
28,"Consider upgrade to Python 3.8+ syntax with `pyupgrade`","### üöÄ The feature, motivation and pitch

PyTorch now supports Python 3.8+ only.

https://github.com/pytorch/pytorch/blob/ca8450849bcfd694c4ddfb9160cd87648cfcec48/setup.py#L216-L222

But there are still many legacy Python syntaxes in the codebase. Such as:

Redundant `object` inherence:

```diff
- class A(object):
+ class A:
      ...
```

`super` call:

```diff
  class A:
      pass

  class B(A):
      def __init__(self):
-         super(B, self).__init__()
+         super().__init__()
```

string format:

```diff
- '%s' % x
+ f""{x}""
- '{}'.format(x)
+ f""{x}""
```

There is a tool [`pyupgrade`](https://github.com/asottile/pyupgrade) that can automatically update the code. The migration can be easily done by:

```bash
pyupgraede --py38-plus $(find torch -name '*.py' -o -name '*.pyi')
lintrunner -a
```

It would be nice if you consider `pre-commit` integration to apply this for future code changes.

### Alternatives

_No response_

### Additional context

_No response_

cc @malfet @seemethere @ezyang @bhosmer @smessmer @ljk53 @bdhirsh","2023-02-03","2023-07-31","178","priorized",NULL,"Xuehai Pan","Skylion007","OTHERS"
29,"fully_shard: load optim state not tested","### üöÄ The feature, motivation and pitch

https://github.com/pytorch/pytorch/blob/master/test/distributed/_composable/fully_shard/test_fully_shard_optim_checkpoint.py are unittests for fully_shard + optim checkpoint, but these do not appear to actually test the load API. 

Marking as high pri due to the composability effort requiring this.

### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @vincentqb @jbschlosser @albanD @janeyx99","2023-01-30","2023-06-04","125","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
30,"[BE] Don't use a vector to hold retains_grad hooks","Currently retains_grad field on autograd_meta is stored as a vector of hooks due to historical reasons. But since that field should only a single hook, we can simplify the code to no longer use a vector.

Versions:
After https://github.com/pytorch/pytorch/pull/85849 lands

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @Lezcano @Varal7","2023-01-11","2023-03-08","56","priorized",NULL,"Jeffrey Wan","soulitzer","OTHERS"
31,"[Dispatchable Collectives] Remove redundancy in ProcessGroup Ops","After https://github.com/pytorch/pytorch/pull/88330 is landed, there are redundant dispatcher operations in ProcessGroup.hpp and Ops.cpp. We should remove the implementations in Ops.cpp, update the pybinded definitions of ProcessGroup to call ProcessGroup colllectives directly, and update any references to Ops.cpp dispatching

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @awgu","2022-12-15","2023-02-09","56","priorized",NULL,"Howard Huang","Howard Huang","OTHERS"
32,"Test that `import torch` does not change global logging state","## Issue description

`import torch` should not change global logging state. In general, no imports of torch or submodules should change global logging state (and imports should generally not change global state).

We've ran into a couple of situations in the past where people modify the global logging module:
- https://github.com/pytorch/pytorch/issues/85952

This issue is to track the addition of a test that checks that `import torch` doesn't change global logging state.
","2022-10-24","2022-10-26","2","low priority",NULL,"Richard Zou","Richard Zou","OTHERS"
33,"Replace signature in autocast registrations ","## Issue description

Autocast uses a macro to define rules for each operator:

https://github.com/pytorch/pytorch/blob/8c6d352bcfa3a8d2f7322d3577117b2d432cd002/aten/src/ATen/autocast_mode.cpp#L305-L309

The macro needs to accept SIGNATURE. Since we've added the at::_ops namespace, we can actually get the SIGNATURE for an op easily, it is `decltype(&ATEN_FN(op))`. This is what we do in functorch.

Deleting the `SIGNATURE` argument would let us clean up all the registrations https://github.com/pytorch/pytorch/blob/8c6d352bcfa3a8d2f7322d3577117b2d432cd002/aten/src/ATen/autocast_mode.cpp#L330-L339

One reason why we might not want to use `decltype(&ATEN_FN(op))` is because the at::_ops namespace doesn't have a good interaction with SymInt operators right now. So I'm not sure this cleanup is worth it. cc @mcarilli @ptrblck @ezyang @bdhirsh 
","2022-10-05","2022-10-07","2","priorized",NULL,"Richard Zou","zou3519","OTHERS"
34,"`dtype` arg in `cumsum` is passed around in the implementation, but it's not used.","### üêõ Describe the bug

https://github.com/pytorch/pytorch/blob/8da704cdb7f68bfa09516e7be17f004b98c48eb3/aten/src/ATen/native/ReduceOps.cpp#L389-L405

We should not pass it around as it's not used.

cc @ysiraichi 

### Versions

master","2022-10-04","2022-10-07","3","priorized",NULL,"Mario Lezcano Casado","lezcano","OTHERS"
35,"[ONNX] Parameterize test_utility_functions","Test suits in test_utility_functions can be parameterized","2022-09-26","2022-10-15","19","priorized",NULL,"Justin Chu","justinchuby","OTHERS"
36,"[FSDP] MixedPrecision, CPUOffload, BackwardPrefetch etc should be documented","### üìö The doc issue

These are auxiliary data structures that do not show up in master documentation, should fix before 1.13 release: https://pytorch.org/docs/master/fsdp.html?highlight=fullyshardeddataparallel#torch.distributed.fsdp.FullyShardedDataParallel

### Suggest a potential alternative/fix

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-09-20","2023-11-07","413","priorized",NULL,"Rohan Varma","spzala","OTHERS"
37,"[FSDP] Fix communication hook docs","### üìö The doc issue

Communication hook is not documented correctly, the hook also takes in a `state` object which is not indicated: https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/fully_sharded_data_parallel.py#L4136

### Suggest a potential alternative/fix

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-09-20","2022-12-02","73","priorized",NULL,"Rohan Varma","mrshenli","OTHERS"
38,"[ONNX] Comprehensive and automated testing","- https://github.com/pytorch/pytorch/issues/88118

Currently the ONNX conversion tests are manually created and sometimes fails with untested input data types. We should leverage PyTorch's testing infrastructure to create more comprehensive tests.

This will help us detect errors like #85316

How MPS tests the ops: https://github.com/pytorch/pytorch/pull/83743/files

Leverage OpInfo and module infos: 

https://github.com/pytorch/pytorch/blob/d3dec8097b847fc46755ef06ea6ff90eebc846eb/torch/testing/_internal/opinfo/core.py#L297

https://github.com/pytorch/pytorch/blob/d3dec8097b847fc46755ef06ea6ff90eebc846eb/torch/testing/_internal/common_modules.py#L1037

cc @BowenBao @wschin @abock ","2022-09-20","2022-12-16","87","priorized",NULL,"Justin Chu","titaiwangms","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
39,"Replace nondeterministic alert test decorator `expectedAlertNondeterministic` with simpler function","It has been bugging me for a while that the `expectedAlertNondeterministic` decorator used for nondeterministic alert tests in `test/test_torch.py` is more complicated and confusing than it needs to be. If I remember correctly, it used to really need to be a decorator, but we don't need it to be a decorator any longer, so I would like to replace it with a simpler nondeterministic alert testing function that just runs a given lambda or function and checks that it produces the alert in the expected cases. This should make writing and reading the code for these tests easier.

cc @mruberry","2022-09-10","2022-09-15","5","priorized",NULL,"kurtamohler","kurtamohler","PROJECT PATTERNS"
40,"[ONNX] Speed up unit tests","Reduce runtime for tests in onnx.

## Profile

```
Program: pytest test/onnx/test_pytorch_onnx_onnxruntime.py -k version_17

789.755 <module>  <string>:1
   [2773 frames hidden]  <string>, runpy, pytest, _pytest, plu...
      778.248 __call__  unittest/case.py:650
      ‚îî‚îÄ 778.248 run  torch/testing/_internal/common_utils.py:2034
         ‚îî‚îÄ 778.242 _run_with_retry  torch/testing/_internal/common_utils.py:1961
            ‚îî‚îÄ 778.242 run  unittest/case.py:558
                  [57 frames hidden]  unittest, contextlib, <built-in>, _py...
                     776.002 _callTestMethod  unittest/case.py:549
                     ‚îú‚îÄ 303.927 instantiated_test  torch/testing/_internal/common_utils.py:247
                     ‚îÇ  ‚îî‚îÄ 295.628 wrapper  pytorch_test_common.py:112
                     ‚îÇ     ‚îî‚îÄ 295.627 wrapper  pytorch_test_common.py:51
                     ‚îÇ        ‚îî‚îÄ 295.627 test_rnn  test_pytorch_onnx_onnxruntime.py:12500
                     ‚îÇ           ‚îî‚îÄ 295.627 _dispatch_rnn_test  test_pytorch_onnx_onnxruntime.py:9173
                     ‚îÇ              ‚îú‚îÄ 109.771 _lstm_test  test_pytorch_onnx_onnxruntime.py:9282
                     ‚îÇ              ‚îÇ  ‚îî‚îÄ 109.450 run_test  onnx_test_common.py:82
                     ‚îÇ              ‚îÇ     ‚îî‚îÄ 109.450 _run_test  onnx_test_common.py:99
                     ‚îÇ              ‚îÇ        ‚îî‚îÄ 109.450 run_model_test  onnx_test_common.py:36
                     ‚îÇ              ‚îÇ           ‚îî‚îÄ 108.890 verify  torch/onnx/verification.py:605
                     ‚îÇ              ‚îÇ              ‚îî‚îÄ 103.088 _export  torch/onnx/utils.py:1380
                     ‚îÇ              ‚îÇ                 ‚îî‚îÄ 102.224 _model_to_graph  torch/onnx/utils.py:1040
                     ‚îÇ              ‚îÇ                    ‚îú‚îÄ 76.864 _optimize_graph  torch/onnx/utils.py:541
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îú‚îÄ 38.422 _run_symbolic_function  torch/onnx/utils.py:1730
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ  ‚îî‚îÄ 34.782 lstm  torch/onnx/symbolic_opset9.py:4466
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ     ‚îî‚îÄ 34.776 wrapper  torch/onnx/symbolic_helper.py:301
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îú‚îÄ 23.189 _lstm_packed  torch/onnx/symbolic_opset9.py:4434
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ  ‚îî‚îÄ 23.126 _generic_rnn  torch/onnx/symbolic_opset9.py:4171
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ     ‚îî‚îÄ 20.277 transform_weights  torch/onnx/symbolic_opset9.py:4286
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ        ‚îî‚îÄ 17.787 <genexpr>  torch/onnx/symbolic_opset9.py:4292
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ           ‚îî‚îÄ 17.779 reform_weights  torch/onnx/symbolic_opset9.py:4265
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ              ‚îî‚îÄ 17.590 <listcomp>  torch/onnx/symbolic_opset9.py:4267
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                 ‚îî‚îÄ 17.533 _slice_helper  torch/onnx/symbolic_helper.py:704
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                    ‚îî‚îÄ 17.297 _slice  torch/onnx/symbolic_opset10.py:336
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                       ‚îî‚îÄ 14.634 _graph_op  torch/onnx/_patch_torch.py:19
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                          ‚îî‚îÄ 10.123 PyCapsule._jit_pass_onnx_node_shape_type_inference  <built-in>:0
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                                [2 frames hidden]  <built-in>
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îî‚îÄ 11.568 _lstm_full  torch/onnx/symbolic_opset9.py:4402
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ           ‚îî‚îÄ 11.548 _generic_rnn  torch/onnx/symbolic_opset9.py:4171
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ              ‚îî‚îÄ 10.117 transform_weights  torch/onnx/symbolic_opset9.py:4286
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                 ‚îî‚îÄ 8.870 <genexpr>  torch/onnx/symbolic_opset9.py:4292
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îî‚îÄ 8.867 reform_weights  torch/onnx/symbolic_opset9.py:4265
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                       ‚îî‚îÄ 8.769 <listcomp>  torch/onnx/symbolic_opset9.py:4267
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                          ‚îî‚îÄ 8.742 _slice_helper  torch/onnx/symbolic_helper.py:704
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                             ‚îî‚îÄ 8.633 _slice  torch/onnx/symbolic_opset10.py:336
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îî‚îÄ 26.808 PyCapsule._jit_pass_onnx_graph_shape_type_inference  <built-in>:0
                     ‚îÇ              ‚îÇ                    ‚îÇ        [2 frames hidden]  <built-in>
                     ‚îÇ              ‚îÇ                    ‚îî‚îÄ 18.103 _create_jit_graph  torch/onnx/utils.py:916
                     ‚îÇ              ‚îÇ                       ‚îî‚îÄ 18.047 _trace_and_get_graph_from_model  torch/onnx/utils.py:848
                     ‚îÇ              ‚îÇ                          ‚îî‚îÄ 18.015 _get_trace_graph  torch/jit/_trace.py:1138
                     ‚îÇ              ‚îÇ                             ‚îî‚îÄ 18.005 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ              ‚îÇ                                ‚îî‚îÄ 18.005 forward  torch/jit/_trace.py:94
                     ‚îÇ              ‚îÇ                                   ‚îî‚îÄ 16.789 wrapper  torch/jit/_trace.py:104
                     ‚îÇ              ‚îÇ                                      ‚îî‚îÄ 12.148 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ              ‚îÇ                                         ‚îî‚îÄ 12.147 _slow_forward  torch/nn/modules/module.py:1164
                     ‚îÇ              ‚îú‚îÄ 98.451 _gru_test  test_pytorch_onnx_onnxruntime.py:9350
                     ‚îÇ              ‚îÇ  ‚îî‚îÄ 98.128 run_test  onnx_test_common.py:82
                     ‚îÇ              ‚îÇ     ‚îî‚îÄ 98.128 _run_test  onnx_test_common.py:99
                     ‚îÇ              ‚îÇ        ‚îî‚îÄ 98.128 run_model_test  onnx_test_common.py:36
                     ‚îÇ              ‚îÇ           ‚îî‚îÄ 97.712 verify  torch/onnx/verification.py:605
                     ‚îÇ              ‚îÇ              ‚îî‚îÄ 92.431 _export  torch/onnx/utils.py:1380
                     ‚îÇ              ‚îÇ                 ‚îî‚îÄ 91.787 _model_to_graph  torch/onnx/utils.py:1040
                     ‚îÇ              ‚îÇ                    ‚îú‚îÄ 71.784 _optimize_graph  torch/onnx/utils.py:541
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îú‚îÄ 36.852 _run_symbolic_function  torch/onnx/utils.py:1730
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ  ‚îî‚îÄ 33.353 symbolic  torch/onnx/symbolic_opset9.py:4564
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ     ‚îî‚îÄ 33.350 wrapper  torch/onnx/symbolic_helper.py:301
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îú‚îÄ 22.248 _rnn_packed  torch/onnx/symbolic_opset9.py:4536
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ  ‚îî‚îÄ 22.202 _generic_rnn  torch/onnx/symbolic_opset9.py:4171
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ     ‚îî‚îÄ 20.135 transform_weights  torch/onnx/symbolic_opset9.py:4286
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ        ‚îî‚îÄ 17.671 <genexpr>  torch/onnx/symbolic_opset9.py:4292
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ           ‚îî‚îÄ 17.669 reform_weights  torch/onnx/symbolic_opset9.py:4265
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ              ‚îî‚îÄ 17.510 <listcomp>  torch/onnx/symbolic_opset9.py:4267
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                 ‚îî‚îÄ 17.477 _slice_helper  torch/onnx/symbolic_helper.py:704
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                    ‚îî‚îÄ 17.342 _slice  torch/onnx/symbolic_opset10.py:336
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                       ‚îî‚îÄ 15.157 _graph_op  torch/onnx/_patch_torch.py:19
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                          ‚îî‚îÄ 10.545 PyCapsule._jit_pass_onnx_node_shape_type_inference  <built-in>:0
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îÇ                                [2 frames hidden]  <built-in>
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ        ‚îî‚îÄ 11.101 _rnn_full  torch/onnx/symbolic_opset9.py:4507
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ           ‚îî‚îÄ 11.081 _generic_rnn  torch/onnx/symbolic_opset9.py:4171
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ              ‚îî‚îÄ 10.036 transform_weights  torch/onnx/symbolic_opset9.py:4286
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                 ‚îî‚îÄ 8.806 <genexpr>  torch/onnx/symbolic_opset9.py:4292
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                    ‚îî‚îÄ 8.806 reform_weights  torch/onnx/symbolic_opset9.py:4265
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                       ‚îî‚îÄ 8.737 <listcomp>  torch/onnx/symbolic_opset9.py:4267
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                          ‚îî‚îÄ 8.712 _slice_helper  torch/onnx/symbolic_helper.py:704
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îÇ                             ‚îî‚îÄ 8.634 _slice  torch/onnx/symbolic_opset10.py:336
                     ‚îÇ              ‚îÇ                    ‚îÇ  ‚îî‚îÄ 24.573 PyCapsule._jit_pass_onnx_graph_shape_type_inference  <built-in>:0
                     ‚îÇ              ‚îÇ                    ‚îÇ        [2 frames hidden]  <built-in>
                     ‚îÇ              ‚îÇ                    ‚îî‚îÄ 14.916 _create_jit_graph  torch/onnx/utils.py:916
                     ‚îÇ              ‚îÇ                       ‚îî‚îÄ 14.862 _trace_and_get_graph_from_model  torch/onnx/utils.py:848
                     ‚îÇ              ‚îÇ                          ‚îî‚îÄ 14.817 _get_trace_graph  torch/jit/_trace.py:1138
                     ‚îÇ              ‚îÇ                             ‚îî‚îÄ 14.811 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ              ‚îÇ                                ‚îî‚îÄ 14.811 forward  torch/jit/_trace.py:94
                     ‚îÇ              ‚îÇ                                   ‚îî‚îÄ 13.683 wrapper  torch/jit/_trace.py:104
                     ‚îÇ              ‚îÇ                                      ‚îî‚îÄ 9.260 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ              ‚îÇ                                         ‚îî‚îÄ 9.260 _slow_forward  torch/nn/modules/module.py:1164
                     ‚îÇ              ‚îî‚îÄ 87.405 _elman_rnn_test  test_pytorch_onnx_onnxruntime.py:9181
                     ‚îÇ                 ‚îî‚îÄ 86.774 run_test  onnx_test_common.py:82
                     ‚îÇ                    ‚îî‚îÄ 86.774 _run_test  onnx_test_common.py:99
                     ‚îÇ                       ‚îî‚îÄ 86.773 run_model_test  onnx_test_common.py:36
                     ‚îÇ                          ‚îî‚îÄ 85.647 verify  torch/onnx/verification.py:605
                     ‚îÇ                             ‚îî‚îÄ 78.367 _export  torch/onnx/utils.py:1380
                     ‚îÇ                                ‚îî‚îÄ 77.105 _model_to_graph  torch/onnx/utils.py:1040
                     ‚îÇ                                   ‚îú‚îÄ 42.625 _optimize_graph  torch/onnx/utils.py:541
                     ‚îÇ                                   ‚îÇ  ‚îú‚îÄ 20.128 _run_symbolic_function  torch/onnx/utils.py:1730
                     ‚îÇ                                   ‚îÇ  ‚îÇ  ‚îî‚îÄ 13.245 symbolic  torch/onnx/symbolic_opset9.py:4564
                     ‚îÇ                                   ‚îÇ  ‚îÇ     ‚îî‚îÄ 13.240 wrapper  torch/onnx/symbolic_helper.py:301
                     ‚îÇ                                   ‚îÇ  ‚îÇ        ‚îî‚îÄ 8.815 _rnn_packed  torch/onnx/symbolic_opset9.py:4536
                     ‚îÇ                                   ‚îÇ  ‚îÇ           ‚îî‚îÄ 8.725 _generic_rnn  torch/onnx/symbolic_opset9.py:4171
                     ‚îÇ                                   ‚îÇ  ‚îî‚îÄ 12.593 PyCapsule._jit_pass_onnx_graph_shape_type_inference  <built-in>:0
                     ‚îÇ                                   ‚îÇ        [2 frames hidden]  <built-in>
                     ‚îÇ                                   ‚îî‚îÄ 26.437 _create_jit_graph  torch/onnx/utils.py:916
                     ‚îÇ                                      ‚îî‚îÄ 26.333 _trace_and_get_graph_from_model  torch/onnx/utils.py:848
                     ‚îÇ                                         ‚îî‚îÄ 26.261 _get_trace_graph  torch/jit/_trace.py:1138
                     ‚îÇ                                            ‚îî‚îÄ 26.248 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ                                               ‚îî‚îÄ 26.246 forward  torch/jit/_trace.py:94
                     ‚îÇ                                                  ‚îî‚îÄ 24.035 wrapper  torch/jit/_trace.py:104
                     ‚îÇ                                                     ‚îú‚îÄ 15.227 _call_impl  torch/nn/modules/module.py:1184
                     ‚îÇ                                                     ‚îÇ  ‚îî‚îÄ 15.226 _slow_forward  torch/nn/modules/module.py:1164
                     ‚îÇ                                                     ‚îî‚îÄ 8.795 <genexpr>  torch/jit/_trace.py:114
                     ‚îú‚îÄ 271.394 wrapper  pytorch_test_common.py:51
                     ‚îÇ  ‚îú‚îÄ 29.974 test_index_put_loop  test_pytorch_onnx_onnxruntime.py:2351
                     ‚îÇ  ‚îÇ  ‚îî‚îÄ 29.930 run_test  onnx_test_common.py:82
                     ‚îÇ  ‚îÇ     ‚îî‚îÄ 29.913 _run_test  onnx_test_common.py:99
                     ‚îÇ  ‚îÇ        ‚îî‚îÄ 29.913 run_model_test  onnx_test_common.py:36
                     ‚îÇ  ‚îÇ           ‚îî‚îÄ 29.888 verify  torch/onnx/verification.py:605
                     ‚îÇ  ‚îÇ              ‚îî‚îÄ 24.322 _export  torch/onnx/utils.py:1380
                     ‚îÇ  ‚îÇ                 ‚îî‚îÄ 23.918 _model_to_graph  torch/onnx/utils.py:1040
                     ‚îÇ  ‚îÇ                    ‚îî‚îÄ 14.552 _optimize_graph  torch/onnx/utils.py:541
                     ‚îÇ  ‚îÇ                       ‚îî‚îÄ 9.139 _run_symbolic_function  torch/onnx/utils.py:1730
                     ‚îÇ  ‚îÇ                          ‚îî‚îÄ 9.027 prim_loop  torch/onnx/symbolic_opset9.py:6298
                     ‚îÇ  ‚îú‚îÄ 10.137 wrapper  pytorch_test_common.py:112
                     ‚îÇ  ‚îú‚îÄ 8.031 test_index_put_slice_index  test_pytorch_onnx_onnxruntime.py:2235
                     ‚îÇ  ‚îÇ  ‚îî‚îÄ 8.031 run_test  onnx_test_common.py:82
                     ‚îÇ  ‚îî‚îÄ 7.961 test_instancenorm3d_runningstats  test_pytorch_onnx_onnxruntime.py:3771
                     ‚îú‚îÄ 11.971 test_avgpool_3d_ceil  test_pytorch_onnx_onnxruntime.py:1420
                     ‚îÇ  ‚îî‚îÄ 9.299 _VariableFunctionsClass.randn  <built-in>:0
                     ‚îÇ        [2 frames hidden]  <built-in>
                     ‚îî‚îÄ 10.681 wrapper  pytorch_test_common.py:112
```
","2022-09-08","2024-01-17","496","priorized",NULL,"Justin Chu","titaiwangms","OTHERS"
41,"[BE] [c10d] [send] Improve error message on dist.send() with destination rank as itself","### üêõ Describe the bug

Calling `dist.send()` with the destination rank as its own rank doesn't make a lot of sense, but we should provide better error messages. It fails without very helpful error messages (GLOO will segfault and NCCL will have a library specific error). Let's improve the error messaging to specify this is not allowed to the user and also update the documentation.

## Example

```python
import torch
import torch.distributed as dist
import os

os.environ[""MASTER_ADDR""] = ""localhost""
os.environ[""MASTER_PORT""] = ""29500""
os.environ[""RANK""] = ""0""
os.environ[""WORLD_SIZE""] = ""1""
os.environ[""TORCH_SHOW_CPP_STACKTRACES""] = ""1""

def main():
    dist.init_process_group(""gloo"")
    print(""finished creating process group"")
    t = torch.tensor([1, 2, 3])
    dist.send(t, 0)

if __name__ == ""__main__"":
    main()
```

### Gloo error message

```
Segmentation fault (core dumped)
```

Debugging the core dump gives
```
Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
gloo::transport::tcp::UnboundBuffer::send (this=0x5555595cb2e0, dstRank=0, slot=0, offset=0, nbytes=24) at ../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:163
163¬†¬†¬†¬†¬†¬† context_->getPair(dstRank)->send(this, slot, offset, nbytes);
(gdb) bt
#0¬† gloo::transport::tcp::UnboundBuffer::send (this=0x5555595cb2e0, dstRank=0, slot=0, offset=0, nbytes=24) at ../third_party/gloo/gloo/transport/tcp/unbound_buffer.cc:163
#1¬† 0x00007fffb88b789e in c10d::ProcessGroupGloo::send (this=0x55555933fa60, tensors=..., dstRank=0, tag=0) at ../torch/csrc/distributed/c10d/ProcessGroupGloo.cpp:2618
#2¬† 0x00007fffb889bc58 in c10d::ops::send_cpu (tensors=..., process_group=..., dstRank=0, tag=0) at ../torch/csrc/distributed/c10d/OpsImpl.cpp:17
#3¬† 0x00007fffb889ddbe in c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (*)(c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long), c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10::guts::typelist::typelist<c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long> >::operator() (this=0x555557cf4a10, args#0=..., args#1=..., args#2=0, args#3=0) at ../aten/src/ATen/core/boxing/impl/WrapFunctionIntoRuntimeFunctor.h:18
#4¬† 0x00007fffb889e3a8 in c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (*)(c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long), c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10::guts::typelist::typelist<c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long> >, c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> > (c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long)>::call(c10::OperatorKernel*, c10::DispatchKeySet, c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long) (functor=0x555557cf4a10, args#0=..., args#1=..., args#2=0, args#3=0) at ../aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:433
#5¬† 0x00007fffb888fa2b in c10::callUnboxedKernelFunction<c10::intrusive_ptr<c10d::Work, c10::detail::intrusive_target_default_null_type<c10d::Work> >, c10::ArrayRef<at::Tensor>, c10::intrusive_ptr<c10d::ProcessGroup, c10::detail::intrusive_target_default_null_type<c10d::ProcessGroup> > const&, long, long> (
```

### Nccl error message
```
Traceback (most recent call last):
  File ""test_collective.py"", line 33, in <module>
    nccl_send()
  File ""test_collective.py"", line 27, in nccl_send
    dist.send(t, 0)
  File ""/private/home/howardhuang/.conda/envs/pytorch_experimentation/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 959, in send
    default_pg.send([tensor], dst, tag).wait()
RuntimeError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:46, invalid usage, NCCL version 2.10.3
ncclInvalidUsage: This usually reflects invalid usage of NCCL library (such as too many async ops, too many collectives at once, mixing streams in a group, etc).
Exception raised from ~AutoNcclGroup at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:46 (most recent call first):
```

### Versions

main branch

cc @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @kwen2501 @awgu @pietern @SciPioneer","2022-08-23","2022-11-15","84","priorized",NULL,"Howard Huang","Howard Huang","OTHERS"
42,"[FSDP] Incorrect CPU offload check always offload total_norm tensor to CPU in clip_grad_norm","### üêõ Describe the bug

This check: https://github.com/pytorch/pytorch/blob/a395f6e842f94fd1cbe6a334b1658f4dd31f5b8e/torch/distributed/fsdp/fully_sharded_data_parallel.py#L3648 is basically just a truthiness check, but the right way to check CPU offload is `self.cpu_offload.offload_params`, `self.cpu_offload` is always truthy.

As a result, `total_norm` tensor is always on CPU. I think this means clip_coef is always created on CPU even if GPU training, and it is moved to GPU which incurs an extra copy. 

### Versions

main

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501 @ezyang","2022-08-11","2023-05-31","293","priorized",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE"
43,"One dlpack to rule them all","### üêõ Describe the bug

One here https://github.com/pytorch/pytorch/blob/master/caffe2/python/dlpack.h
And another there https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/dlpack.h
Should we make one reference to another one?

### Versions

1.12/CI","2022-08-09","2024-05-21","651","priorized",NULL,"Nikita Shulga","rgommers","OTHERS"
44,"FSDP namedtuple support has issues","### üêõ Describe the bug

It seems that if we have namedtuple output from an FSDP wrapped unit, we can run into an issue where one of the elements of the namedtuple changes type to a generator. The following reproduces the issue:

```
import torch
import torch.distributed as dist
from collections import namedtuple

from torch.distributed.fsdp import FullyShardedDataParallel
from torch.distributed.fsdp import wrap
import torch.nn as nn
import os

dist.init_process_group(backend=""nccl"", world_size=1, rank=0)
print(""init"")

class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin = nn.Linear(1,1)

    def forward(self, x):
        return x

class Wrapper(nn.Module):
    def __init__(self, w):
        super().__init__()
        self.w = w

    def forward(self, inp):
        out = self.w(inp)
        print(out)
        print(out.image.data)
        return (out.projected_text_embeddings, out.projected_image_embeddings)


m = MyModule()
m = FullyShardedDataParallel(m)
model = Wrapper(m)
model=FullyShardedDataParallel(model)

t = torch.ones(1, device='cuda')
FLAVAOutput = namedtuple(
    ""FLAVAOutput"",
    [
        ""image"",
        ""image_masked"",
        ""text"",
        ""text_masked"",
        ""multimodal"",
        ""multimodal_masked"",
        ""projected_image_embeddings"",
        ""projected_text_embeddings"",
    ],
    defaults=(t,t,t,t,t,t,t,t),
)

inp = FLAVAOutput()
out = model(inp)
print(out)
```

the output is:

```
(pt) $ MASTER_ADDR=localhost MASTER_PORT=29500 python test.py
init
/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py:1077: UserWarning: Module is put on CPU and will thus have flattening and sharding run on CPU, which is less efficient than on GPU. We recommend passing in `device_id` argument which will enable FSDP to put module on GPU device, module must also be on GPU device to work with `sync_module_states=True` flag which requires GPU communication.
  warnings.warn(
/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/nn/parallel/scatter_gather.py:9: UserWarning: is_namedtuple is deprecated, please use the python checks instead
  warnings.warn(""is_namedtuple is deprecated, please use the python checks instead"")
FLAVAOutput(image=<generator object _apply_to_tensors.<locals>.apply.<locals>.<genexpr> at 0x7fd9f02253c0>, image_masked=tensor([1.], device='cuda:0'), text=tensor([1.], device='cuda:0'), text_masked=tensor([1.], device='cuda:0'), multimodal=tensor([1.], device='cuda:0'), multimodal_masked=tensor([1.], device='cuda:0'), projected_image_embeddings=tensor([1.], device='cuda:0'), projected_text_embeddings=tensor([1.], device='cuda:0'))
Traceback (most recent call last):
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/test.py"", line 55, in <module>
    out = model(inp)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/nn/modules/module.py"", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 2432, in forward
    outputs = self._fsdp_wrapped_module(*args, **kwargs)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/nn/modules/module.py"", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/distributed/fsdp/flatten_params_wrapper.py"", line 156, in forward
    return self.module(*inputs, **kwinputs)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/torch/nn/modules/module.py"", line 1186, in _call_impl
    return forward_call(*input, **kwargs)
  File ""/fsx/users/rvarm1/rvarm1/repos/pytorch/test.py"", line 29, in forward
    print(out.image.data)
AttributeError: 'generator' object has no attribute 'data'
```

### Versions

main

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-08-09","2022-10-05","57","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
45,"FSDP device_id + CPU offload can have issues","### üêõ Describe the bug

If we have the following setup

```
class MyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.a = nn.Linear(10, 10)
                self.b = nn.Linear(10, 10)

            def forward(self, x):
                return self.b(self.a(x))

        model = MyModel()

        fsdp = FSDP(
            model,
            auto_wrap_policy=always_wrap_policy,
            cpu_offload=CPUOffload(offload_params=True),
            device_id=torch.cuda.current_device()
        )
```

we hit the error:

```
RuntimeError: Module on rank 1 is given device_id argument cuda:1, but is on cpu.  Either move module before FSDP init or omit device_id argument.
```

This seems to be because the root FSDP unit does not manage any params, so when checking whether to move because it is given `device_id` argument, it accesses a FSDP submodule's FlatParam which is on CPU, and we throw an error: https://github.com/pytorch/pytorch/blob/5ca098fe3891f7151d73223c38e909aa9dd5c862/torch/distributed/fsdp/fully_sharded_data_parallel.py#L1054.

The proper fix should be to bypass this check if we end up with a flatparam.

Lightning integration has hit this issue.

### Versions

main

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-08-05","2022-08-16","11","high priority",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
46,"Variable's address changed when passing them through DDP model","### üêõ Describe the bug

I passed a BatchEncoding, which is the output of a RobertaTokenizerFast tokenizer, through a distributed data parallel model. I find the variable address is changed and its ```.encodings``` attribute is lost.
The following is a minimize implementation:
``` python
import argparse
from torch import nn
import torch
from icecream import ic
import os
import torch.distributed as dist

from transformers.models.roberta.tokenization_roberta_fast import RobertaTokenizerFast

def setup_for_distributed(is_master):
        """"""
        This function disables printing when not in master process
        """"""
        import builtins as __builtin__

        builtin_print = __builtin__.print

        def print(*args, **kwargs):
            force = kwargs.pop(""force"", False)
            if is_master or force:
                builtin_print(*args, **kwargs)

        __builtin__.print = print
def init_distributed_mode(args):
    """"""Initialize distributed training, if appropriate""""""
    if ""RANK"" in os.environ and ""WORLD_SIZE"" in os.environ:
        args.rank = int(os.environ[""RANK""])
        args.world_size = int(os.environ[""WORLD_SIZE""])
        args.gpu = int(os.environ[""LOCAL_RANK""])
    elif ""SLURM_PROCID"" in os.environ:
        args.rank = int(os.environ[""SLURM_PROCID""])
        args.gpu = args.rank % torch.cuda.device_count()
    else:
        print(""Not using distributed mode"")
        args.distributed = False
        return

    args.distributed = True

    torch.cuda.set_device(args.gpu)
    args.dist_backend = ""nccl""
    print(""| distributed init (rank {}): {}"".format(args.rank, args.dist_url), flush=True)

    dist.init_process_group(
        backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank
    )
    dist.barrier()
    setup_for_distributed(args.rank == 0)

def get_args_parser():
    parser = argparse.ArgumentParser(""Set transformer detector"", add_help=False)
    # Distributed training parameters
    parser.add_argument(""--world-size"", default=1, type=int, help=""number of distributed processes"")
    parser.add_argument(""--dist-url"", default=""env://"", help=""url used to set up distributed training"")
    return parser



class M(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.tokenizer = RobertaTokenizerFast.from_pretrained('ckpt/roberta-base')
        self.linear = nn.Linear(1,10)
    def forward(self,text=None,tokenized=None,xe=None):
        if tokenized:
            #tokenized._encodings = xe
            data = self.linear(tokenized.input_ids.float().unsqueeze(-1))
            return tokenized, data
        tokenized = self.tokenizer(text, padding=""longest"", return_tensors=""pt"").to('cuda')
        data = self.linear(tokenized.input_ids.float().unsqueeze(-1))
        return tokenized,data
        
if __name__=='__main__':
    args = get_args_parser().parse_args()

    init_distributed_mode(args)
    model = M()
    model.cuda()
    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
    tokenizer = RobertaTokenizerFast.from_pretrained('ckpt/roberta-base')
    
    text = ['I dont know','I know']

    tokenized = tokenizer(text, padding=""longest"", return_tensors=""pt"").to('cuda')
    ic(tokenized._encodings)
    ic(id(tokenized))
    tokenized,_ = model(tokenized = tokenized)
    ic(tokenized._encodings)
    ic(id(tokenized))
```
The stdout is:
```
ic| tokenized._encodings: [Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
                           Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
ic| id(tokenized): 140585220520352
ic| tokenized._encodings: None
ic| id(tokenized): 140585219892224
```
This bug appears when I use pytorch 1.11.0 with cuda 11.3. But when I change to pytorch 1.10.1 with cuda 10.2, the bug disappears. The output is as follow:
```
ic| tokenized._encodings: [Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
                           Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
ic| id(tokenized): 140490778204240
ic| tokenized._encodings: [Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),
                           Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]
ic| id(tokenized): 140490778204240
```

### Versions

## Env that has this bug
PyTorch version: 1.11.0
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0
Libc version: glibc-2.27

Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.19.91-009.ali4000.alios7.x86_64-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.0.194
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB

Nvidia driver version: 470.82.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==1.11.0
[pip3] torchaudio==0.11.0
[pip3] torchvision==0.12.0
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               11.3.1               h2bc3f7f_2  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.22.3           py38he7a7128_0  
[conda] numpy-base                1.22.3           py38hf524024_0  
[conda] pytorch                   1.11.0          py3.8_cuda11.3_cudnn8.2.0_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.11.0               py38_cu113    pytorch
[conda] torchvision               0.12.0               py38_cu113    pytorch
## Env that do not has this bug
PyTorch version: 1.10.1
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0
Libc version: glibc-2.27

Python version: 3.8.13 (default, Mar 28 2022, 11:38:47)  [GCC 7.5.0] (64-bit runtime)
Python platform: Linux-4.19.91-009.ali4000.alios7.x86_64-x86_64-with-glibc2.17
Is CUDA available: True
CUDA runtime version: 11.0.194
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB

Nvidia driver version: 470.82.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.1
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==1.10.1
[pip3] torchaudio==0.10.1
[pip3] torchvision==0.11.2
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.2.89              hfd86e86_1  
[conda] ffmpeg                    4.3                  hf484d3e_0    pytorch
[conda] mkl                       2021.4.0           h06a4308_640  
[conda] mkl-service               2.4.0            py38h7f8727e_0  
[conda] mkl_fft                   1.3.1            py38hd3c417c_0  
[conda] mkl_random                1.2.2            py38h51133e4_0  
[conda] numpy                     1.22.3           py38he7a7128_0  
[conda] numpy-base                1.22.3           py38hf524024_0  
[conda] pytorch                   1.10.1          py3.8_cuda10.2_cudnn7.6.5_0    pytorch
[conda] pytorch-mutex             1.0                        cuda    pytorch
[conda] torchaudio                0.10.1               py38_cu102    pytorch
[conda] torchvision               0.11.2               py38_cu102    pytorch

cc @ezyang @gchanan @zou3519 @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @H-Huang @kwen2501 @awgu @pietern @SciPioneer","2022-08-02","2022-11-05","95","high priority",NULL,"Âè∂Âä†Âçö","rohan-varma","SOFTWARE ARCHITECTURE"
47,"Split up `common_methods_invocations.py`?","`common_methods_invocations.py` has grown to 22K lines and over 1MB in file size. One implication of this is you can't open it in the GitHub UI or link to specific lines of code.

I propose creating an opinfos folder and where there are currently different categories, such as `UnaryUfuncInfo` or `ReductionOpInfo`, these could be their own file. ","2022-07-29","2024-07-17","719","priorized",NULL,"peterbell10","lezcano","OTHERS"
48,"[FSDP] `clip_grad_norm()` fails in `test_fsdp_core.py`","`clip_norm` and `norm_type` in `_test_identical_outputs()` (i.e. the testing subroutine for `test_fsdp_core.py`) do not propagate to `_train_for_several_steps()` (i.e. the training subroutine for `_test_identical_outputs()`). 
https://github.com/pytorch/pytorch/blob/5af48581b5098eefaaa8b7ec55a5fabc5027c8e5/torch/testing/_internal/common_fsdp.py#L543-L544
Fixing this yields to failing tests.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501 @ezyang","2022-07-05","2022-12-14","162","priorized",NULL,"Andrew Gu","awgu","OTHERS"
49,"[BE] Refactor FSDP Unit Tests","## Motivation & Goals
1. In H2, we plan to land new FSDP features such as non-recursive wrapping and multiple parameter group support within one `FlatParameter`. Since these features correspond to distinct code paths, the core unit tests need to parameterize over them. This means an increase in the time-to-signal (TTS) by up to 4x and may increase with further features (e.g. native XLA support).
**Goal 1: Cut down the existing TTS while maintaining feature coverage**.
2. The test files have become fragmented. Despite the existence of `common_fsdp.py`, test files often define their own models and training loops, leading to redundancy, and the methods that do exist in `common_fsdp.py` are not well-documented, leading to confusion.
**Goal 2: Refactor common models and boilerplate to `common_fsdp.py`.**
3. For the non-recursive wrapping code path, the model construction differs from the existing recursive wrapping code path. As a result, much of the existing tests cannot be directly adapted to test the non-recursive path by simply changing a single `FullyShardedDataParallel` constructor call.
**Goal 3: Refactor model construction to enable simpler testing for the non-recursive wrapping path.**

## Status Quo
On the AI AWS cluster with 2 A100 GPUs, the current TTS is approximately **4967.06 seconds = 82.78 minutes = 1.38 hours**[0]. PyTorch Dev Infra has asked to have our multi-GPU tests run in < 75 minutes in CI, which uses P60 GPUs.

The largest contributors to the TTS are `test_fsdp_core.py` (2191.89 seconds = 36.53 minutes) and `test_fsdp_mixed_precision.py` (1074.42 seconds  = 17.91 minutes), representing almost 2/3 of the total. Since these test the core FSDP runtime, newly-added code paths will target these tests.

[0]This is a point estimate from running each test file once and excludes recent changes to the test files (my stack was rebased on a commit from 6/21).

## Approach

I will proceed with a series of PRs. The order will be to address Goal 3 -> Goal 1 -> Goal 2.
- For Goal 3, I will introduce a common interface `FSDPTestModel`.
https://github.com/pytorch/pytorch/pull/80873
- For Goal 1, I will use `self.subTest()` with `dist.barrier()` to avoid the expensive process spawn and `dist.init_process_group()` for each parameterization.
https://github.com/pytorch/pytorch/pull/80908
https://github.com/pytorch/pytorch/pull/80915

https://blog.ganssle.io/articles/2020/04/subtests-in-python.html
> There are, however, occasionally situations where the subTest form factor offers some advantages even in parameterization. For example, if you have a number of tests you'd like to perform that have an expensive set-up function that builds or acquires an immutable resource that is used in common by all the subtests:
- For Goal 2, I will perform a deep comb through the existing test suite. 

## Related Issues
https://github.com/pytorch/pytorch/issues/80872
https://github.com/pytorch/pytorch/issues/78277
https://github.com/pytorch/pytorch/issues/67288

cc @zhaojuanmao @mrshenli @rohan-varma @ezyang","2022-07-05","2024-03-10","614","priorized",NULL,"Andrew Gu","awgu","OTHERS"
50,"Delete internal php code for viable/strict promotion","Since we have have migrated promoting to the viable/strict branch to an open-source GHA, we can now delete the internal php code that previously handled promoting from master to viable/strict on `pytorch/pytorch`. 

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2022-06-20","2022-07-22","32","priorized",NULL,"Sarah Wang","swang392","OTHERS"
51,"[BE][ZeRO] Enable multigpu unit tests","Following https://github.com/pytorch/pytorch/pull/77947, we should enable `ZeroRedundancyOptimizer` multigpu unit tests.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-06-13","2024-03-10","636","priorized",NULL,"Andrew Gu","Andrew Gu","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
52,"Add doc formatting check to lintrunner","From @mruberry: Docs are difficult to build locally (our documentation for how to do so is often out of date) and formatting errors in CI appear without line numbers (in at least some cases). Some recent example errors/warnings:

```
/opt/conda/lib/python3.7/site-packages/torch/__init__.py:docstring of torch.set_float32_matmul_precision:22: ERROR: Unexpected indentation.
/opt/conda/lib/python3.7/site-packages/torch/__init__.py:docstring of torch.set_float32_matmul_precision:23: WARNING: Block quote ends without a blank line; unexpected unindent.
```

If lintrunner could detect doc formatting issues it would making adding/updating docs much easier.","2022-06-13","2024-04-03","660","priorized",NULL,"Michael Suo","clee2000","PROJECT PATTERNS"
53,"Improve auto_wrap_policy documentation","### üìö The doc issue

A few improvements can be made to the doc of auto_wrap_policy:
- Add expected type of callable
- Clarify what the policy function should return, and how its used (i.e. what's done depends on `recurse` flag). 


### Suggest a potential alternative/fix

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-05-26","2022-05-31","5","low priority",NULL,"Rohan Varma","Rohan Varma","OTHERS"
54,"[FSDP][BE] Refactor unit tests to use a few canonical models","In the rush to push features, we have introduced a few test-file-specific models outside of `common_fsdp.py`. As a better engineering task, we should refactor the existing FSDP tests to use a few canonical models that exercise all of the relevant edge cases:
- 3+ levels of nesting (for asymmetric # of levels down versus up when recursing)
- Non-root FSDP with FSDP children and non-FSDP children in different orders (e.g. for three child modules: FSDP, non-FDSP, FSDP versus non-FSDP, non-FSDP, FSDP etc.)
-Ignored modules at every level
- Buffers
- Shared parameters (shared buffers?)
- FSDP instances with and without parameters

It should be the case that if we add a feature and it works for our set of canonical models, then we have strong confidence that the feature is robust.

Some of the boilerplate training code may also be refactored.


cc @zhaojuanmao @mrshenli @rohan-varma","2022-05-25","2022-10-01","129","priorized",NULL,"Andrew Gu","awgu","PROJECT PATTERNS"
55,"[primTorch] many primTorch test xfails are due to chalf","We should fix these issues or stop testing the references in chalf for now -- it's an outsized developer efficiency issue

cc @ezyang @anjali411 @dylanbespalko @mruberry @Lezcano @nikitaved @ngimel","2022-05-21","2022-05-22","1","priorized",NULL,"Mike Ruberry","kshitij12345","SOFTWARE ARCHITECTURE"
56,"TORCH_DISTRIBUTED_DEBUG should print the expected CollectiveFingerPrint","### üöÄ The feature, motivation and pitch

During collective mismatch detection, we print errors as follows:

```
RuntimeError: Detected mismatch between collectives on ranks. Rank 0 is running inconsistent collective: CollectiveFingerPrint(OpType=BROADCAST, TensorShape=[34112], TensorDtypes=Float, TensorDeviceTypes=TensorOptions(dtype=float (default), device=cuda, layout=Strided (default), requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt))
```

However, the problem here is that we don't actually print what is the expected CollectiveFingerPrint. Adding this information would make it much easier to debug issues.

### Alternatives

_No response_

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501","2022-05-20","2022-06-28","39","priorized",NULL,"Pritam Damania","H-Huang","OTHERS"
57,"Type code in torch/distributed/utils.py","### üöÄ The feature, motivation and pitch

https://github.com/pytorch/pytorch/pull/77187 is moving some additional code to `torch.distributed.utils`, we should add the appropriate type annotations for this code as well. 

### Alternatives

_No response_

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-05-10","2023-05-30","385","low priority",NULL,"Rohan Varma","Rohan Varma","OTHERS"
58,"Check all input parameters k and l","### üêõ Describe the bug

# Bug
file: TensorImpl.h
function: size_between_dim_ 
bug: the function only check parameter l, not check parameter k

# reproduce
```
// Product of all dims between k and l (not including dims[k] and dims[l])
inline int64_t size_between_dim_(int k, int l, IntArrayRef dims) {
  TORCH_CHECK((unsigned)l < dims.size());
  int64_t r = 1;
  if (k < l) {
    for (int i = k + 1; i < l; ++i) {
      r *= dims[i];
    }
  } else {
    for (int i = l + 1; i < k; ++i) {
      r *= dims[i];
    }
  }
  return r;
}
```

### Versions

Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A
ROCM used to build PyTorch: N/A

OS: macOS 10.15.7 (x86_64)
GCC version: (Homebrew GCC 10.2.0_4) 10.2.0
Clang version: 12.0.0 (clang-1200.0.32.29)
CMake version: version 3.20.1
Libc version: N/A

Python version: 3.9.7 (default, Sep 16 2021, 08:50:36)  [Clang 10.0.0 ] (64-bit runtime)
Python platform: macOS-10.15.7-x86_64-i386-64bit
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: N/A

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] torch==1.12.0a0+git7c2103a
[conda] numpy                     1.22.3                   pypi_0    pypi
[conda] torch                     1.12.0a0+git7c2103a          pypi_0    pypi","2022-05-02","2022-05-03","1","priorized",NULL,"caipengxiang","caipengxiang","PROJECT PATTERNS"
59,"Issues with typehints of F.multi_head_attention_forward","### üêõ Describe the bug

The typehints of `torch.nn.functional.multi_head_attention_forward` is wrong (or out-dated). This makes static type checkers (mypy/pyright) unhappy.

In pyi, there are three parameters that should be optional but are not: in_proj_weight, in_proj_bias, out_proj_bias.

```python
def multi_head_attention_forward(query: Tensor,
                                 key: Tensor,
                                 value: Tensor,
                                 embed_dim_to_check: int,
                                 num_heads: int,
                                 in_proj_weight: Tensor,
                                 in_proj_bias: Tensor,
                                 bias_k: Optional[Tensor],
                                 bias_v: Optional[Tensor],
                                 add_zero_attn: bool,
                                 dropout_p: float,
                                 out_proj_weight: Tensor,
                                 out_proj_bias: Tensor,
                                 training: bool = True,
                                 key_padding_mask: Optional[Tensor] = None,
                                 need_weights: bool = True,
                                 attn_mask: Optional[Tensor] = None,
                                 use_separate_proj_weight: bool = False,
                                 q_proj_weight: Optional[Tensor] = None,
                                 k_proj_weight: Optional[Tensor] = None,
                                 v_proj_weight: Optional[Tensor] = None,
                                 static_k: Optional[Tensor] = None,
                                 static_v: Optional[Tensor] = None
                                 ) -> Tuple[Tensor, Optional[Tensor]]: ...
```

### Versions

master branch

cc @albanD @mruberry @jbschlosser @walterddr @kshitij12345 @ezyang @malfet @rgommers @xuzhao9 @gramster","2022-04-21","2022-04-27","6","priorized",NULL,"Yuge Zhang","Yuge Zhang","SOFTWARE ARCHITECTURE"
60,"FSDP should verify / broadcast model similar to DDP","### üöÄ The feature, motivation and pitch

Similar to DDP, FSDP should broadcast model states for each wrapped module from rank 0 to the rest of the ranks during __init__ to ensure all model start off at the same state before training. This will help resolve hard to debug hang / accuracy issues.

### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-04-15","2022-06-03","49","high priority",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
61,"Standardize workflow job names","Currently we don't have any set convention for workflow job naming and so the names are not always consistent. This makes job searching and grouping difficult and harder to change in the future. We should enforce some sort of standardization and document it.

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2022-04-14","2022-10-28","197","priorized",NULL,"Jane (Yuan) Xu","ZainRizvi","OTHERS"
62,"checkpointing of DDP comms hook","### üöÄ The feature, motivation and pitch

Some DDP comms hook, like PowetSGD, are stateful and must be part of model checkpointing to enable trainer restarts.

Right now, this has to be manually done by users and, in some cases, requires workaround as the state objects are not serializatible. For example: https://discuss.pytorch.org/t/how-to-resume-with-powersgd-enabled-training/148747/

DDP should include those objects available on its state_dict to make restarts feasible.


### Alternatives

Make comm hook state objects serializable and let users deal with the extra plumbing

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-04-12","2022-06-29","78","high priority",NULL,"Rodrigo Kumpera","rohan-varma","OTHERS"
63,"[RFC] Consolidated and unified state_dict and load_state_dict hooks","### üöÄ The feature, motivation and pitch

Today, PyTorch offers a few private hooks for `state_dict` and `load_state_dict` that exist to allow users to run post and pre processing respectively when saving and loading model checkpoints.

Current state_dict post hook: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L1265
Current state_dict pre hook: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L1408

This proposal seeks to:
1) Propose 2 new hooks so that state_dict and load_state_dict both have pre and post hooks
2) Aim to consolidate all of the 4 hooks and make their semantics consistent
3) Provide a vision where all 4 new APIs are public and can be used by PyTorch users in a stable fashion.


#### Current State
1) `_register_state_dict_hook` allows registration of a `Callable[self, state_dict, prefix, local_metadata] -> Optional[Dict[str, Tensor]]` and is run after the state_dict of self (and all of its children) is processed. It can modify `state_dict` and optionally return a new one.
2) `_register_load_state_dict_pre_hook` allows registration of a `Callable[state_dict, prefix, local_metadatra, missing_keys, unexpected_keys, error_msgs]` or a `Callable[self, state_dict, prefix, local_metadatra, missing_keys, unexpected_keys, error_msgs] if with_module=True]`, and is run before loading state dict into self (and all of its children).

#### Proposed State

Revamped existing APIs:
1) `register_state_dict_post_hook` -> Same as (1) above, but we've renamed it to clearly indicate this is a post hook. This is a bit inconsistent with nn.Module register_hook semantics, but arguably more clear and will be consistent with state_dict hook APIs, which should be enough for consistency. This hook will behave in the same way it currently does.
2) `register_load_state_dict_pre_hook` -> Same as (2) above, but we will remove `with_module` argument and always provide the `self` module into the API. Users can simply ignore this module in their function if needed. EDIT (@janeyx99): We should also allow the hook to optionally return a new state_dict, if desired, as it will cover the use case where one may want to completely change up the state_dict. Relevant PR https://github.com/pytorch/pytorch/pull/105953 where we have this for the optimizer.

New APIs:

1) `register_state_dict_pre_hook: Callable[self, prefix] -> None` which is a state_dict() pre hook that will be run before the state_dict is computed for self (and all of its children). We do not pass in the `destination` argument is it is deprecated into this hook.
2) `register_load_state_dict_post_hook: Callable[self] -> None` which is a `load_state_dict()` post hook that runs after state dict is loaded into self module _and_ all of its children. Note that this is consistent with the rest of the APIs - hooks are called either before or after self computation and all children computation has taken place, never in between. Another proposal is that the hook can be a `Callable[self, NamedTuple] -> NamedTuple` where the `NamedTuple` contains the `missing_keys` and `unexpected_keys` that was returned by load_state_dict, and the user can do some optional processing / addition / removal and return a new `NamedTuple`. Although, we are not aware of any use case requiring that at the moment, so suggesting to keep it simple for now.
Similar to https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py#L1389, we should offer a `_register_load_state_dict_post_hook` that is called after `load_state_dict` for a particular `nn.Module` is completed.

The goal is to have this hook called even for modules that aren't at the top level during load_state_dict; and this hook should be called after self module is loaded as well as all of self's children.

In addition, similar to `load_state_dict`'s existing pre-hook, state_dict should also offer a pre-hook for any module-specific preprocessing that modules may want to run before calling into `state_dict`.  This can be invoked before the call to `self._state_dict_impl`. API to register this could be `_register_state_dict_pre_hook`. Unfortunately, the current state_dict hook is a post hook but is named `_register_state_dict_hook` which is a bit unclear as to whether it is a pre/post hook. Since the API is private, we could consider refactoring `_register_state_dict_hook` to `_register_state_dict_post_hook`.

#### Plan moving forward
Since these are all public facing APIs, we can add them in addition to the old existing hooks and keep the old ones for legacy reasons and add deprecated warnings.

#### Motivating use case:
- FullyShardedDataParallel in particular needs `register_load_state_dict_post_hook` and `register_load_state_dict_pre_hook` to enter/exit the full parameter summoning context. We don't want to do this in `load_state_dict` itself since `load_state_dict` is not overridable (it won't be called for submodules). Draft PR: 

cc @albanD @mruberry @jbschlosser @walterddr @mikaylagawarecki @kshitij12345 @saketh-are","2022-04-05","2024-06-10","797","priorized",NULL,"Rohan Varma","albanD","OTHERS"
64,"[shard] support ShardedTensor setters","### üöÄ The feature, motivation and pitch

in https://github.com/pytorch/pytorch/pull/74695 we switched ShardedTensor to use torch.Tensor as a subclass, but we currently only support getters, not setters, we might need to support them to allow user to alter some attributes of sharded tensor

### Alternatives

_No response_

### Additional context

_No response_","2022-04-05","2022-08-06","123","priorized",NULL,"Wanchao","Wanchao","OTHERS"
65,"`TORCH_DISTRIBUTED_DEBUG=DETAIL` raises runtime errors in ddp tests","### üêõ Describe the bug

While debugging I've exported a few env variables including `TORCH_DISTRIBUTED_DEBUG=DETAIL` and noticed that a lot of ddp tests started to fail suddenly and was able to narrow it down to the usage of `TORCH_DISTRIBUTED_DEBUG`.

Code to reproduce:
```python
TORCH_DISTRIBUTED_DEBUG=DETAIL python distributed/fsdp/test_fsdp_core.py -v
INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmplpi1b61s
INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmplpi1b61s/_remote_module_non_sriptable.py
test_backward_hooks_after_save (__main__.TestHooks) ... INFO:numba.cuda.cudadrv.driver:init
INFO:torch.testing._internal.common_distributed:Started process 0 with pid 197584
INFO:torch.testing._internal.common_distributed:Started process 1 with pid 197585
...
INFO:torch.testing._internal.common_distributed:Starting event listener thread for rank 0
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 601, in run_test
    getattr(self, test_name)()
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 486, in wrapper
    fn()
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 131, in wrapper
    return func(*args, **kwargs)
  File ""/opt/pytorch/pytorch/test/distributed/fsdp/test_fsdp_core.py"", line 265, in test_backward_hooks_after_save
    self._train_for_several_steps(model, num_steps=2, autocast=False)
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_fsdp.py"", line 430, in _train_for_several_steps
    model.module.run_backward(loss)
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_fsdp.py"", line 149, in run_backward
    loss.backward()
  File ""/opt/conda/lib/python3.8/site-packages/torch/_tensor.py"", line 399, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 1409, in _post_backward_hook
    dist._reduce_scatter_base(
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 2513, in _reduce_scatter_base
    work = group._reduce_scatter_base(output, input, opts)
RuntimeError: ProcessGroup nccldoes not support _reduce_scatter_base
Exception raised from _reduce_scatter_base at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroup.hpp:322 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6c (0x7f38b5a9a1dc in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xfa (0x7f38b5a77cd4 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0x3ae89a6 (0x7f38e4a009a6 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x9e0cff (0x7f38eabb2cff in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0x345399 (0x7f38ea517399 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #5: PyCFunction_Call + 0x6e (0x55bd9672ae7e in /opt/conda/bin/python)
frame #6: _PyObject_MakeTpCall + 0x501 (0x55bd96713631 in /opt/conda/bin/python)
frame #7: <unknown function> + 0x13bbfd (0x55bd9672abfd in /opt/conda/bin/python)
frame #8: _PyEval_EvalFrameDefault + 0x48dc (0x55bd9670effc in /opt/conda/bin/python)
frame #9: _PyEval_EvalCodeWithName + 0x2e1 (0x55bd96709461 in /opt/conda/bin/python)
frame #10: _PyFunction_Vectorcall + 0x18c (0x55bd9671b33c in /opt/conda/bin/python)
frame #11: _PyEval_EvalFrameDefault + 0x10e8 (0x55bd9670b808 in /opt/conda/bin/python)
frame #12: _PyEval_EvalCodeWithName + 0x2e1 (0x55bd96709461 in /opt/conda/bin/python)
frame #13: _PyFunction_Vectorcall + 0x18c (0x55bd9671b33c in /opt/conda/bin/python)
frame #14: PyObject_Call + 0x2d2 (0x55bd9672d172 in /opt/conda/bin/python)
frame #15: _PyEval_EvalFrameDefault + 0x2150 (0x55bd9670c870 in /opt/conda/bin/python)
frame #16: _PyEval_EvalCodeWithName + 0x9f6 (0x55bd96709b76 in /opt/conda/bin/python)
frame #17: _PyFunction_Vectorcall + 0x18c (0x55bd9671b33c in /opt/conda/bin/python)
frame #18: <unknown function> + 0x13ba3b (0x55bd9672aa3b in /opt/conda/bin/python)
frame #19: _PyObject_FastCallDict + 0x390 (0x55bd96712d70 in /opt/conda/bin/python)
frame #20: <unknown function> + 0x1fd1a9 (0x55bd967ec1a9 in /opt/conda/bin/python)
frame #21: _PyObject_MakeTpCall + 0x501 (0x55bd96713631 in /opt/conda/bin/python)
frame #22: <unknown function> + 0x12be0d (0x55bd9671ae0d in /opt/conda/bin/python)
frame #23: PyObject_CallFunctionObjArgs + 0xa1 (0x55bd9671aa41 in /opt/conda/bin/python)
frame #24: torch::autograd::PyFunctionPostHook::operator()(std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) + 0xcc (0x7f38ea80a70c in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #25: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x8ea (0x7f38e4113a7a in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x5f0 (0x7f38e4115400 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x9f (0x7f38e410c85f in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #28: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x67 (0x7f38ea7fd527 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #29: <unknown function> + 0xd6de4 (0x7f38ec865de4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #30: <unknown function> + 0x8609 (0x7f3921854609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)
frame #31: clone + 0x43 (0x7f3921613163 in /usr/lib/x86_64-linux-gnu/libc.so.6)

 exiting process 7 with exit code: 10
ERROR:torch.testing._internal.common_distributed:Caught exception: 
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 601, in run_test
    getattr(self, test_name)()
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 486, in wrapper
    fn()
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_distributed.py"", line 131, in wrapper
    return func(*args, **kwargs)
  File ""/opt/pytorch/pytorch/test/distributed/fsdp/test_fsdp_core.py"", line 265, in test_backward_hooks_after_save
    self._train_for_several_steps(model, num_steps=2, autocast=False)
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_fsdp.py"", line 430, in _train_for_several_steps
    model.module.run_backward(loss)
  File ""/opt/conda/lib/python3.8/site-packages/torch/testing/_internal/common_fsdp.py"", line 149, in run_backward
    loss.backward()
  File ""/opt/conda/lib/python3.8/site-packages/torch/_tensor.py"", line 399, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File ""/opt/conda/lib/python3.8/site-packages/torch/autograd/grad_mode.py"", line 27, in decorate_context
    return func(*args, **kwargs)
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py"", line 1409, in _post_backward_hook
    dist._reduce_scatter_base(
  File ""/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 2513, in _reduce_scatter_base
    work = group._reduce_scatter_base(output, input, opts)
RuntimeError: ProcessGroup nccldoes not support _reduce_scatter_base
Exception raised from _reduce_scatter_base at /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroup.hpp:322 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x6c (0x7f6334eae1dc in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: c10::detail::torchCheckFail(char const*, char const*, unsigned int, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0xfa (0x7f6334e8bcd4 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #2: <unknown function> + 0x3ae89a6 (0x7f6363e149a6 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x9e0cff (0x7f6369fc6cff in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #4: <unknown function> + 0x345399 (0x7f636992b399 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #5: PyCFunction_Call + 0x6e (0x55932727ce7e in /opt/conda/bin/python)
frame #6: _PyObject_MakeTpCall + 0x501 (0x559327265631 in /opt/conda/bin/python)
frame #7: <unknown function> + 0x13bbfd (0x55932727cbfd in /opt/conda/bin/python)
frame #8: _PyEval_EvalFrameDefault + 0x48dc (0x559327260ffc in /opt/conda/bin/python)
frame #9: _PyEval_EvalCodeWithName + 0x2e1 (0x55932725b461 in /opt/conda/bin/python)
frame #10: _PyFunction_Vectorcall + 0x18c (0x55932726d33c in /opt/conda/bin/python)
frame #11: _PyEval_EvalFrameDefault + 0x10e8 (0x55932725d808 in /opt/conda/bin/python)
frame #12: _PyEval_EvalCodeWithName + 0x2e1 (0x55932725b461 in /opt/conda/bin/python)
frame #13: _PyFunction_Vectorcall + 0x18c (0x55932726d33c in /opt/conda/bin/python)
frame #14: PyObject_Call + 0x2d2 (0x55932727f172 in /opt/conda/bin/python)
frame #15: _PyEval_EvalFrameDefault + 0x2150 (0x55932725e870 in /opt/conda/bin/python)
frame #16: _PyEval_EvalCodeWithName + 0x9f6 (0x55932725bb76 in /opt/conda/bin/python)
frame #17: _PyFunction_Vectorcall + 0x18c (0x55932726d33c in /opt/conda/bin/python)
frame #18: <unknown function> + 0x13ba3b (0x55932727ca3b in /opt/conda/bin/python)
frame #19: _PyObject_FastCallDict + 0x390 (0x559327264d70 in /opt/conda/bin/python)
frame #20: <unknown function> + 0x1fd1a9 (0x55932733e1a9 in /opt/conda/bin/python)
frame #21: _PyObject_MakeTpCall + 0x501 (0x559327265631 in /opt/conda/bin/python)
frame #22: <unknown function> + 0x12be0d (0x55932726ce0d in /opt/conda/bin/python)
frame #23: PyObject_CallFunctionObjArgs + 0xa1 (0x55932726ca41 in /opt/conda/bin/python)
frame #24: torch::autograd::PyFunctionPostHook::operator()(std::vector<at::Tensor, std::allocator<at::Tensor> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&) + 0xcc (0x7f6369c1e70c in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #25: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x8ea (0x7f6363527a7a in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #26: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x5f0 (0x7f6363529400 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #27: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x9f (0x7f636352085f in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #28: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x67 (0x7f6369c11527 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #29: <unknown function> + 0xd6de4 (0x7f636bc79de4 in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)
frame #30: <unknown function> + 0x8609 (0x7f63a0c68609 in /usr/lib/x86_64-linux-gnu/libpthread.so.0)
frame #31: clone + 0x43 (0x7f63a0a27163 in /usr/lib/x86_64-linux-gnu/libc.so.6)

 exiting process 5 with exit code: 10
...
```

Unsure if this env variable should *not* be used while executing tests and if this might thus be an expected failure.
In this case, we might want to raise another error message to avoid debugging it.

### Versions

PyTorch version: 1.12.0a0+bd13bc6
Is debug build: False
CUDA used to build PyTorch: 11.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.4 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.3
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.4.0-89-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.6.124
GPU models and configuration: 
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB
GPU 4: NVIDIA A100-SXM4-40GB
GPU 5: NVIDIA A100-SXM4-40GB
GPU 6: NVIDIA A100-SXM4-40GB
GPU 7: NVIDIA A100-SXM4-40GB

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.4.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.4.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] numpy==1.22.3
[pip3] pytorch-quantization==2.1.2
[pip3] torch==1.12.0a0+bd13bc6
[pip3] torch-tensorrt==1.1.0a0
[pip3] torchtext==0.13.0a0
[pip3] torchvision==0.13.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.22.3           py38h05e7239_0    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torch                     1.12.0a0+bd13bc6          pypi_0    pypi
[conda] torch-tensorrt            1.1.0a0                  pypi_0    pypi
[conda] torchtext                 0.13.0a0                 pypi_0    pypi
[conda] torchvision               0.13.0a0                 pypi_0    pypi


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @mruberry","2022-03-31","2022-10-05","188","priorized",NULL,"ptrblck","ptrblck","OTHERS"
66,".gitignore core.* regex needs refinement","### üêõ Describe the bug

The `core.*` regex https://github.com/pytorch/pytorch/blob/51e50a2ddb9844e0abaf6028fe5e4246448dffff/.gitignore#L313 includes files named `core.py`
[1] https://github.com/pytorch/pytorch/blob/v1.10.0/torch/fx/experimental/unification/core.py
[2] https://github.com/pytorch/pytorch/blob/v1.10.0/torch/fx/experimental/unification/multipledispatch/core.py

This leads to problematic behavior, for example when you are trying to add a new `core.py` file, it's easy to miss force-add.

The suggestion is to refine the regex to avoid including files like `core.py`, `core.cpp` etc.
I would do it myself, but unfortunately I don't know what is the format of the core dump names that the regex is meant to ignore.

In case it's obsolete consider removing it altogether.

### Versions

n/a

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2022-03-29","2022-04-10","12","priorized",NULL,"Sergei Vorobev","albanD","PROJECT PATTERNS"
67,"[FSDP] full_state_dict: FSDP details can exist in key names","### üêõ Describe the bug

The goal of FSDP's full state_dict implementation is to return a state_dict as if the model were exactly as local, but this is not always the case. For an example, add this test:

```
@skip_if_lt_x_gpu(2)
    @parametrize(""double_nest"", [True])
    def test_state_dict_skip_module(self, double_nest):
        torch.cuda.set_device(self.rank)
        def _create_module():
            with enable_wrap(wrapper_cls=FSDP):
                module = SkipModel(double_nest=double_nest)
                # Full name of linear_skip param tensors in SkipModel, as would be
                # stored in checkpoint.
                linear_skip_tensor_names = [
                    k for k in dict(module.named_parameters()).keys()
                    if 'linear_skip' in k
                ]
                # skip SkipModule
                linear_skip = getattr(module, 'linear_skip')
                delattr(module, 'linear_skip')
                # Wrap FSDP
                fsdp = wrap(module)
                # reattach
                setattr(module, 'linear_skip', linear_skip)
                return fsdp, linear_skip_tensor_names

        fsdp, linear_skip_tensor_names = _create_module()
        # Run a forward pass
        inp = torch.randn((1, 10), device=torch.cuda.current_device())
        loss = fsdp(inp)
        loss.sum().backward()
        if self.rank == 0:
            print(fsdp)
        with fsdp.summon_full_params():
            param_names = dict(fsdp.named_parameters()).keys()
            if self.rank == 0:
                print(f""names {param_names}"")

        state_dict = fsdp.state_dict()
        if self.rank == 0:
            print(f""sd_keys {state_dict.keys()}"")
            for name, tensor in state_dict.items():
                has_been_cloned = getattr(tensor, '_has_been_cloned', False)
                if name in linear_skip_tensor_names:
                    self.assertFalse(has_been_cloned)
                else:
                    self.assertTrue(has_been_cloned)

        # Create new identical module
        new_fsdp, _ = _create_module()
        new_fsdp.load_state_dict(deepcopy(state_dict))
        for (p1, p2) in zip(fsdp.parameters(), new_fsdp.parameters()):
            self.assertEqual(p1, p2)
```

to `test_fsdp_state_dict`, with this model def:

```
	
class SkipModule(Module):
    def __init__(self):
        super().__init__()
        self.lin = Linear(10, 10, bias=False)

    def forward(self, x):
        return self.lin(x)

class NestedLinear(Module):
    def __init__(self, fsdp_wrap):
        super().__init__()
        if fsdp_wrap:
            self.nested_linear = wrap(Linear(10, 10, bias=False).cuda())
        else:
            self.nested_linear = Linear(10, 10, bias=False).cuda()

    def forward(self, x):
        return self.nested_linear(x)


class SkipModel(Module):
    def __init__(self, double_nest):
        super().__init__()
        self.linear = nn.Linear(10, 10, bias=False).cuda()
        self.linear_skip = SkipModule().cuda()
        self.nested_linear = wrap(NestedLinear(fsdp_wrap=double_nest))

    def forward(self, x):
        x = self.linear(x)
        x = self.linear_skip(x)
        x = self.nested_linear(x)
        return x
```

the state_dict keys are: 

```
sd_keys odict_keys(['linear.weight', 'nested_linear._fpw_module.nested_linear.weight', 'linear_skip.lin.weight'])
```

although the second key should not have _fpw_module.



### Versions

main

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-27","2022-04-12","16","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
68,"[Reliability Improvement] Record the step # in the optimizer state of PostLocalSGDOptimizer","### üöÄ The feature, motivation and pitch

**Context:**
[PostLocalSGDOptimizer](https://github.com/pytorch/pytorch/blob/8b0847811501cd452b131b6153f28c254a3ee44f/torch/distributed/optim/post_localSGD_optimizer.py#L5) can have a non-trivial warm-up stage. E.g., a training may take 30 epochs, and the warmup stage may take 3-5 epochs.

**Issue:**
If the training process is not very reliable and may fail a few times, then the training needs to resume from checkpoints. However, in the current setup the built-in [step counter](https://github.com/pytorch/pytorch/blob/8b8fac91bfca6cadf86d402622f31e8e2dcdc429/torch/distributed/algorithms/model_averaging/averagers.py#L22) in `ModelAverager` will be reset after each restart. As a result, the warm-up stage will run multiple times, which will lead to less speedup.

**Proposed Solution:**
To be able to skip the warm-up stage after resuming from a checkpiont, the step # across epochs must recorded and stored in the checkpoint as well. A good option is the optimizer state.

### Alternatives

If PyTorch Ignite or PyTorch Lightning is used, the global step # will be automatically stored in the checkpoint, and such feature can also be easily supported by modifying the application code.

However, it will be nice to support this feature in PyTorch directly, especially if an optimizer state is already available in this case.

cc: @rohan-varma 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-22","2022-06-09","79","low priority",NULL,"Yi Wang","wayi1","PROJECT PATTERNS"
69,"[FSDP] summon_full_params + named_buffers clean full path","### üöÄ The feature, motivation and pitch

- Similar to https://github.com/pytorch/pytorch/issues/73890, we should clean buffer names of FSDP wrapped module details in named_buffers before returning them to the user. 
- Elect to do this only in summon_full_params for consistency in the behavior between named_buffers and named_parameters
- This might also have implications for full_state_dict() checkpoint. We should validate whether the buffer name appears as expected for checkpointing use cases and fix as needed.

### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-21","2022-04-12","22","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
70,"[BE] Update TCPStore pybinded definition to use c10::optional world_size","### üêõ Describe the bug

The TCPStore initialization supports optional world size argument so we should also update the pybind definition to support this as well.

https://github.com/pytorch/pytorch/blob/f0db315efeef27a2d8760cec4573bcbe40854b01/torch/csrc/distributed/c10d/TCPStore.cpp#L964

This also means updating the tests and examples that we have to use None instead of -1 when demonstrating non-fixed world_size.

We should change [TCPStore docs](https://pytorch.org/docs/master/distributed.html#torch.distributed.TCPStore) so that users will use None for world_size as it is cleaner than having the magic number -1. We will keep support for -1 to be backwards compatible.

Pybind defintion for TCPStore:
https://github.com/pytorch/pytorch/blob/dc4f12d9cc15476c545e3a1bb7a74e23d5b0ddf5/torch/csrc/distributed/c10d/init.cpp#L883-L885

## Testing
Tests:
https://github.com/pytorch/pytorch/blob/dc4f12d9cc15476c545e3a1bb7a74e23d5b0ddf5/test/distributed/test_store.py#L158

Cpp Tests:
https://github.com/pytorch/pytorch/blob/master/test/cpp/c10d/TCPStoreTest.cpp

Test instructions:
https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md#running-unit-tests

### Versions

torch 1.12

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-21","2022-05-12","52","priorized",NULL,"Howard Huang","Howard Huang","PROJECT PATTERNS"
71,"Make our functions have correct __module__ information / way to get human readable name for func in __torch_function__","### üêõ Describe the bug

A common thing to want to do when writing a `__torch_function__` override is to get a human readable name of the func you were passed. Unfortunately, the obvious logic using `__module__` and `__name__` does not work because many functions do not correctly have `__module__` information attached to them.

As a result, there are multiple workarounds for this problem:

* FX https://github.com/pytorch/pytorch/blob/3d6317cbe07a97191991552bebf5a7983fc88910/torch/fx/node.py#L64
* torchdynamo https://github.com/facebookresearch/torchdynamo/blob/00d2af529dc2c98ee201b9503e26bece468d07ea/torchdynamo/allowed_functions.py#L46

Either fix `__module__` so it can be relied upon, or reify the internal hacks into an API that people can use (put it in torch.overrides)

### Versions

master

cc @hameerabbasi @rgommers @peterbell10","2022-03-21","2023-08-02","499","priorized",NULL,"Edward Z. Yang","albanD","PROJECT PATTERNS"
72,"[BE] Consolidate duplicated rendezvous code","### üêõ Describe the bug

As we introduce dynamic group in #73372, the two rendezvous paths have some duplicated code. 

https://github.com/pytorch/pytorch/blob/f0db315efeef27a2d8760cec4573bcbe40854b01/torch/distributed/rendezvous.py#L112-L121

https://github.com/pytorch/pytorch/blob/f0db315efeef27a2d8760cec4573bcbe40854b01/torch/distributed/rendezvous.py#L85-L94

### Versions

 #73372

### Note

It will be easier to consolidate the duplicated rendezvous code once rank is not required for initialization of dynamic RPC members.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-20","2022-07-22","124","low priority",NULL,"mrshenli","mrshenli","OTHERS"
73,"[FSDP] Investigate summon_full_params with CPU offloading","### üêõ Describe the bug

When writeback=True, the shards may not point to CPU as expected. See this comment: https://github.com/pytorch/pytorch/pull/73904#discussion_r823025082. We should investigate, fix accordingly, and add tests.

### Versions

main

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-14","2022-06-16","94","high priority",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
74,"[FSDP] Generalize `summon_full_params()` to non-FSDP root module","We should generalize the method `summon_full_params()` to a static method to accommodate being called on a root module that is not an FSDP instance. This avoids having to search for FSDP root instances within the root module and aggregating manually.

For example, right now if we want to compare the parameters of `root_module` with a fully local version of the model `local_model`, then we may have to do something like:
```
params = []
for module in root_module.children():
    context = module.summon_full_params() if isinstance(module, FSDP) else suppress()
    with context:
        params.extend(list(p.detach().clone() for p in module.parameters())
# Now we can correctly call `zip(params, local_model.parameters())`
```
We should place this kind of logic behind our API. The generalization should be similar to the above only using `yield` instead of explicitly constructing a `list` and not `detach().clone()`-ing since the memory should be safe to access while inside the context.

cc @zhaojuanmao @mrshenli @rohan-varma","2022-03-14","2022-04-12","29","priorized",NULL,"Andrew Gu","rohan-varma","OTHERS"
75,"[FSDP][Docs][BE] Document FSDP behavior of parameters(), named_parameters()","### üìö The doc issue

As mentioned in a few different issues such as https://github.com/pytorch/pytorch/issues/73891 and https://github.com/pytorch/pytorch/issues/73890, FSDP modules may behave differently than what the user expects both within and outside of `summon_full_parameter` context. If we aim to support these APIs in beta release (i.e. expect users to access them), we should override them to add docs on how they behave as appopriate.



### Suggest a potential alternative/fix

_No response_

cc @brianjo @mruberry @zhaojuanmao @mrshenli @rohan-varma","2022-03-07","2023-06-08","458","priorized",NULL,"Rohan Varma","Rohan Varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
76,"ci: Check if docker daemon is running and start it if it isn't","Some workflows currently fail with the following message: ([example](https://github.com/pytorch/pytorch/runs/5450990477?check_suite_focus=true))
<img width=""1197"" alt=""Screen Shot 2022-03-07 at 10 15 59 AM"" src=""https://user-images.githubusercontent.com/1700823/157093611-01147ee3-7b80-4951-8c62-d2ec9f390ca8.png"">

We should add a check / step to our EC2 linux setup common scripts see if the docker daemon is currently running using `systemctl is-active --quiet docker` and then use `sudo systemctl start docker` if it isn't currently running.

Change can probably be added as a separate step here:

https://github.com/pytorch/pytorch/blob/a3d099ea18043d64d873dd83f344f95765a875d3/.github/templates/common.yml.j2#L97-L125

To be done prior to ECR login so that ECR login should work as expected (which relies on the docker daemon).

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2022-03-07","2022-03-14","7","priorized",NULL,"Eli Uriegas","janeyx99","PROJECT PATTERNS"
77,"[FSDP] Hide ConfigAutoWrap","### üöÄ The feature, motivation and pitch

torch.distributed.fsdp.wrap.ConfigAutoWrap is public but this shouldn't be a public API, users can use enable_wrap, wrap, and auto_wrap_policy APIs. 

### Alternatives

_No response_

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-03-03","2022-03-04","1","low priority",NULL,"Rohan Varma","Rohan Varma","OTHERS"
78,"Check mismatched number of parameters in DDP _verify_params_across_processes","### üöÄ The feature, motivation and pitch

Some use cases may encounter errors like mismatched number of parameters in DDP, _verify_params_across_processes should check this error before checking shapes and sizes of parameters

### Alternatives

_No response_

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-02-28","2022-03-15","15","high priority",NULL,"Yanli Zhao","Yanli Zhao","OTHERS"
79,"Protect the branch name `main`","### üöÄ The feature, motivation and pitch

Someone who has the permissions needs to protect the branch name `main` so we can start the process of migrating the default branch from master to main. We don't want to have any accidental pushes to `main` while in the migration so it needs the same protections as master currently has. See #71806 for the progress of the transition.

### Alternatives

N/A

### Additional context

_No response_","2022-02-23","2022-02-25","2","low priority",NULL,"Brian Muse","Brian Muse","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
80,"Debug optimizer overlap tests with NCCL_ASYNC_ERROR_HANDLING","### üêõ Describe the bug

These tests seem to fail when NCCL_ASYNC_ERROR_HANDLING is enabled. Originally discovered in an unrelated PR: https://github.com/pytorch/pytorch/runs/5291461671?check_suite_focus=true. 

### Versions

main

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-02-22","2022-11-05","256","high priority",NULL,"Rohan Varma","Rohan Varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
81,"CI: Remove code that differentiates between PR and trunk","### üöÄ The feature, motivation and pitch

The recent CIFlow trigger change moves many jobs to push triggers, which breaks our functionality that separates logic dependent on whether the CI running is in PR CI or trunk CI. A lot of this functionality is confusing and not particularly useful, so we should eliminate these instances.

### Alternatives

We fix the logic by changing the ref value in the push trigger

### Additional context

_No response_

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2022-02-17","2022-10-28","253","priorized",NULL,"Jane (Yuan) Xu","ZainRizvi","OTHERS"
82,"[DDP][FSDP][BE] Refactor `no_sync()` context","Motivated by this [comment](https://github.com/pytorch/pytorch/pull/72446#discussion_r802807811), we should consider refactoring [`DistributedDataParallel.no_sync()`](https://github.com/pytorch/pytorch/blob/0972db5b7d1f4fdf3a9bf1841b108455acfa7359/torch/nn/parallel/distributed.py#L904-L924) and [`FullyShardedDataParallel.no_sync()`](https://github.com/pytorch/pytorch/pull/72446) since they follow a similar structure.
- Save the old `require_backward_grad_sync`
- Set the flag(s) to `False`
- `try: yield`
- Restore the old `require_backward_grad_sync` in the `finally`


cc @zhaojuanmao @mrshenli @rohan-varma","2022-02-09","2022-10-01","234","priorized",NULL,"Andrew Gu","Andrew Gu","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
83,"Remove/refactor BucketReplica in DDP reducer","### üöÄ The feature, motivation and pitch

BucketReplica is now redundant since SPMD (single-process multi device) mode is deprecated, and we don't need to maintain this list: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.hpp#L426 and index into it as there is always only one replica per bucket. 

We can either:
1) Keep BucketReplica which is sort of a container for gradient tensors and other useful variables, maintain only one instance of it per bucket, and rename it to something better indicative of what it is
2) Remove BucketReplica and move all its member variables to `Bucket` so that there is only one place where all data related to gradient buckets exists.

### Details

1. Previously, DDP would let users replicate the model across GPUs in a single process similar to DataParallel, but this had performance limitations and we deprecated this approach. Most code related to this, and all public-facing code, has since been removed. 
2. The above notion of `BucketReplica`s is a bit of vestiget left over from this time. As a result we only now need one `BucketReplica` and don't have to maintain the list as mentioned above. 
3. `reducer` https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp is essentially DDP's C++ backend which is responsible for managing and bucketing gradients, reducing them across all workers, and synchronizing with the backwards pass appropriately.

### Tests
- This is not a user facing change so no new tests should be needed. 
- Please ensure all tests in https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/distributed/distributed_test.py and https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d_nccl.py (and test_c10d*) pass.

### Alternatives

_No response_

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-02-08","2022-02-28","20","priorized",NULL,"Rohan Varma","awgu","PROJECT PATTERNS"
84,"Remove Caffe2","Caffe2 has been deprecated for some time, and the little Caffe2 code that PyTorch is using can be refactored into PyTorch proper.

If there are still Caffe2 users this might require setting up Caffe2 in its own repository with its own tests. ","2022-02-08","2024-10-24","989","low priority",NULL,"Mike Ruberry","Mike Ruberry","OTHERS"
85,"[FSDP][BE] Remove multiple param logic in FSDP code","### üöÄ The feature, motivation and pitch

Currently we only shard based on flat_param, eventually once it is decided that FSDP will not support non-flat param based training, we should remove/simplify all code such as https://github.com/pytorch/pytorch/blob/master/torch/distributed/fsdp/fully_sharded_data_parallel.py#L275 that assumes that there might be multiple parameters. 

We can probably do this in the future once we've investigated all various use cases such as finetuning and are confident that we won't need non flat param approach, simplifying the code base by quite a bit. 

### Alternatives

_No response_

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-02-08","2022-10-16","250","priorized",NULL,"Rohan Varma","awgu","OTHERS"
86,"DDP debug mode: Throw when static_graph and unused parameter sizes are different across ranks","### üöÄ The feature, motivation and pitch

static_graph training is not meant for the case where unused parameters are variable across ranks, however we've seen a lot of use cases where static_graph is enabled and unused parameter sizes are different across ranks, which leads to hard to debug errors. 

To help with this, In DETAIL debug mode, we can allgather unused parameter sizes across ranks and verify that they are equal under static graph training. Some additional design consideration may be needed as to where to add this detection and if we need to run this check every iteration or only the first few training iterations. 

### Alternatives

Looking at log output can confirm whether unused parameter sizes are different, however this takes additional time and requires DDP specific context / expertise and will increase the troubleshooting time.

### Additional context

_No response_

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-02-07","2022-02-17","10","low priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
87,"Better Engineering: Create test_dlpack","Currently dlpack tests are in test_torch.py, a catch-all for core PyTorch components. It would be organizationally useful to separate them to their own test suite. 

cc @mruberry","2022-01-20","2024-05-21","852","priorized",NULL,"Mike Ruberry","rgommers","PROJECT PATTERNS"
88,"Move `AT_ERROR` to `TORCH_CHECK(false`","### üöÄ The feature, motivation and pitch

I just learnt we're deprecating `AT_ERROR` in favour of `TORCH_CHECK(false`. I don't know the reasons behind this, but it'd be good to simply regex our way to victory here.

### Alternatives

Not deprecate `AT_ERROR`? In all fairness, I prefer `AT_ERROR` as it's more concise both in intentions and in keystrokes.","2022-01-13","2022-01-13","0","low priority",NULL,"Mario Lezcano Casado","ngimel","OTHERS"
89,"TORCH_DISTRIBUTED_DEBUG not effective","### üêõ Describe the bug

I'M testING TORCH_DISTRIBUTED_DEBUG according to this documents: https://pytorch.org/docs/master/distributed.html#debugging-torch-distributed-applications

```
import os

import torch
import torch.distributed as dist
import torch.multiprocessing as mp


class TwoLinLayerNet(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.a = torch.nn.Linear(10, 10, bias=False)
        self.b = torch.nn.Linear(10, 1, bias=False)

    def forward(self, x):
        a = self.a(x)
        b = self.b(x)
        return (a, b)


def worker(rank):
    dist.init_process_group(""nccl"", rank=rank, world_size=2)
    torch.cuda.set_device(rank)
    print(""init model"")
    model = TwoLinLayerNet().cuda()
    print(""init ddp"")
    ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])

    inp = torch.randn(10, 10).cuda()
    print(""train"")

    for _ in range(20):
        output = ddp_model(inp)
        loss = output[0] + output[1]
        loss.sum().backward()


if __name__ == ""__main__"":
    os.environ[""MASTER_ADDR""] = ""localhost""
    os.environ[""MASTER_PORT""] = ""29501""
    os.environ[
        ""TORCH_DISTRIBUTED_DEBUG""
    ] = ""DETAIL""  # set to DETAIL for runtime logging.
    mp.spawn(worker, nprocs=2, args=())
```

The code above is from this document and is identical

but the output has no ddp debug information

```
init model
init model
init ddp
init ddp
i39a12200:2427:2427 [0] NCCL INFO Bootstrap : Using bond0:11.164.100.100<0>
i39a12200:2427:2427 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

i39a12200:2427:2427 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
i39a12200:2427:2427 [0] NCCL INFO NET/Socket : Using [0]bond0:11.164.100.100<0>
i39a12200:2427:2427 [0] NCCL INFO Using network Socket
NCCL version 2.10.3+cuda11.3
i39a12200:2428:2428 [1] NCCL INFO Bootstrap : Using bond0:11.164.100.100<0>
i39a12200:2428:2428 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation

i39a12200:2428:2428 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
i39a12200:2428:2428 [1] NCCL INFO NET/Socket : Using [0]bond0:11.164.100.100<0>
i39a12200:2428:2428 [1] NCCL INFO Using network Socket
i39a12200:2427:2577 [0] NCCL INFO NCCL_MAX_NCHANNELS set by environment to 2.
i39a12200:2428:2578 [1] NCCL INFO NCCL_MAX_NCHANNELS set by environment to 2.
i39a12200:2427:2577 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 2.
i39a12200:2427:2577 [0] NCCL INFO Channel 00/02 :    0   1
i39a12200:2428:2578 [1] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 2.
i39a12200:2428:2578 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
i39a12200:2427:2577 [0] NCCL INFO Channel 01/02 :    0   1
i39a12200:2428:2578 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffffffff,ffffffff,ffffffff
i39a12200:2427:2577 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
i39a12200:2427:2577 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff,ffffffff
i39a12200:2428:2578 [1] NCCL INFO Channel 00 : 1[57000] -> 0[52000] via P2P/IPC/read
i39a12200:2427:2577 [0] NCCL INFO Channel 00 : 0[52000] -> 1[57000] via P2P/IPC/read
i39a12200:2428:2578 [1] NCCL INFO Channel 01 : 1[57000] -> 0[52000] via P2P/IPC/read
i39a12200:2427:2577 [0] NCCL INFO Channel 01 : 0[52000] -> 1[57000] via P2P/IPC/read
i39a12200:2428:2578 [1] NCCL INFO Connected all rings
i39a12200:2428:2578 [1] NCCL INFO Connected all trees
i39a12200:2428:2578 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
i39a12200:2428:2578 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
i39a12200:2427:2577 [0] NCCL INFO Connected all rings
i39a12200:2427:2577 [0] NCCL INFO Connected all trees
i39a12200:2427:2577 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
i39a12200:2427:2577 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
i39a12200:2428:2578 [1] NCCL INFO comm 0x7fdb5c002f70 rank 1 nranks 2 cudaDev 1 busId 57000 - Init COMPLETE
i39a12200:2427:2577 [0] NCCL INFO comm 0x7fbfdc002f70 rank 0 nranks 2 cudaDev 0 busId 52000 - Init COMPLETE
i39a12200:2427:2427 [0] NCCL INFO Launch mode Parallel
train
train
```

Do I need to successfully configure USE_GLOG at compile time or specify a CAFFE2_LOG_THRESHOLD of no less than INFO in order to print these messages? If so, I suggest refine the documentation here.

The PyTorch build used: 

```
pip3 install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
```


### Versions

```
PyTorch version: 1.10.1+cu113
Is debug build: False
CUDA used to build PyTorch: 11.3
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.19.1
Libc version: glibc-2.9

Python version: 3.6.12 |Anaconda, Inc.| (default, Sep  8 2020, 23:10:56)  [GCC 7.3.0] (64-bit runtime)
Python platform: Linux-3.10.0-327.ali2019.alios7.x86_64-x86_64-with-debian-buster-sid
Is CUDA available: True
CUDA runtime version: 11.3.109
GPU models and configuration:
GPU 0: NVIDIA A100-SXM4-40GB
GPU 1: NVIDIA A100-SXM4-40GB
GPU 2: NVIDIA A100-SXM4-40GB
GPU 3: NVIDIA A100-SXM4-40GB
GPU 4: NVIDIA A100-SXM4-40GB
GPU 5: NVIDIA A100-SXM4-40GB
GPU 6: NVIDIA A100-SXM4-40GB
GPU 7: NVIDIA A100-SXM4-40GB

Nvidia driver version: 470.82.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.0
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.16.6
[pip3] torch==1.10.1+cu113
[pip3] torch-cluster==1.5.9
[pip3] torch-geometric==2.0.3
[pip3] torch-scatter==2.0.9
[pip3] torch-sparse==0.6.12
[pip3] torch-spline-conv==1.2.1
[pip3] torch-xla==1.10
[pip3] torchaudio==0.10.1+cu113
[pip3] torchtext==0.11.1
[pip3] torchvision==0.11.2+cu113
[conda] magma-cuda113             2.5.2                         1    pytorch
[conda] mkl                       2021.4.0           h06a4308_640    defaults
[conda] mkl-include               2022.0.1           h8d4b97c_803    conda-forge
[conda] numpy                     1.16.6                   pypi_0    pypi
[conda] torch                     1.10.1+cu113             pypi_0    pypi
[conda] torch-cluster             1.5.9                    pypi_0    pypi
[conda] torch-geometric           2.0.3                    pypi_0    pypi
[conda] torch-scatter             2.0.9                    pypi_0    pypi
[conda] torch-sparse              0.6.12                   pypi_0    pypi
[conda] torch-spline-conv         1.2.1                    pypi_0    pypi
[conda] torch-xla                 1.10                     pypi_0    pypi
[conda] torchaudio                0.10.1+cu113             pypi_0    pypi
[conda] torchtext                 0.11.1                   pypi_0    pypi
[conda] torchvision               0.11.2+cu113             pypi_0    pypi
```

cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2022-01-05","2022-05-03","118","high priority",NULL,"cicirori","rohan-varma","OTHERS"
90,"Improve CI timeout debuggability","### üöÄ The feature, motivation and pitch

PyTorch CI experiences timeouts somewhat regularly (ASAN, multigpu). Whenever a job times out in CI, it is incredibly hard to debug. The current workflow is something like:
1. Notice that a job is timing out
2. Wait for it to time out (4.5hr on GHA), as rerunning isn't available until every job of a workflow is complete.
2. Rerun it so you could SSH into it (if it were on a PR. If it was on trunk, SSHing is not available).
3. Wait for it to build
4. Now SSH into the test job and wait for it to reach the right place in the code for you to investigate--if it even times out again.
5. If not, repeat this process.

What would be amazing if there were a skeleton SSH key available that would allow key people to get into and investigate any runner whenever so that the investigation could be done on the spot.

### Alternatives

Other options:
1. Make all runners ssh'able by default, which has its own security/resource downsides

### Additional context

_No response_

cc @ezyang @gchanan @zou3519 @bdhirsh @seemethere @malfet @pytorch/pytorch-dev-infra","2022-01-04","2022-01-10","6","priorized",NULL,"Jane (Yuan) Xu","janeyx99","OTHERS"
91,"Use fmt::format instead of string concat","### Changes

A few string concatenations in `agent_utils.cpp` can be replaced with [`fmt::format`](https://github.com/fmtlib/fmt).

https://github.com/pytorch/pytorch/blob/6f9844693f331663f0fbdce38511772e60f4119d/torch/csrc/distributed/rpc/agent_utils.cpp#L51-L56

### Validation
Any tests that call `rpc.shutdown()` with a graceful shutdown will be sufficient to test the change.

Example testing command:
`pytest test/distributed/rpc/test_tensorpipe_agent.py -vs -k test_shutdown_followed_by_rpc`

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-12-15","2021-12-28","13","low priority",NULL,"Howard Huang","H-Huang","OTHERS"
92,"Obliviate repeat_test_for_types usage in pytorch","### üöÄ The feature, motivation and pitch

Currently there are still a few tests in the codebase (mostly in test_nn.py) that use repeat_test_for_types to repeat a test for different data types. There is now a better way to do this with OpInfos which is now used across most of pytorch. We should migrate these tests away from using repeat_test_for_types and remove repeat_test_for_types in common_utils.py. 

### Alternatives

_No response_

### Additional context

The current test cases that use this decorator still are below.
In test_nn.py
* test_Conv2d_deterministic_cudnn
* test_Conv2d_large_workspace
* test_ConvTranspose2d_large_output_padding
* test_Conv2d_depthwise_naive_groups_cuda
* test_Conv3d_depthwise_naive_groups_cuda
* test_noncontig_conv_grad_cuda
* test_batchnorm_large_batch
* test_conv_double_backward_cuda

In test_data_parallel.py
* test_data_parallel_module
* test_data_parallel_module_kwargs_only
* test_data_parallel_module_kwargs_only_empty_list
* test_data_parallel_module_kwargs_only_empty_dict
* test_data_parallel_module_kwargs_only_empty_tuple

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @albanD @mruberry @jbschlosser @walterddr @kshitij12345","2021-12-13","2022-01-10","28","low priority",NULL,"Jane (Yuan) Xu","mruberry","OTHERS"
93,"Update elementwise binary tests","In https://github.com/pytorch/pytorch/pull/69439 a significant number of new elementwise binary tests were added, and there are still a few updates to be made:

- [ ] elementwise binary OpInfos needs to be labeled as such
- [ ] test_type_promotion needs to be enabled on ROCm
- [ ] skips should be updated to xfails where possible
- [ ] pre-existing tests that are replaced with the new OpInfo-based tests should be removed
- [ ] additional elementwise binary tests where each input has the same shape but a different discontiguity should be investigated

cc @mruberry","2021-12-06","2022-04-04","119","priorized",NULL,"Mike Ruberry","jithunnair-amd","OTHERS"
94,"Replace instances of batchCheckErrors with new at::_linalg_check_errors","## üöÄ Feature

Since we've added a new at::_linalg_check_errors function, we should be able to replace all instances of 
```
if (self.dim() > 2) {
  batchCheckErrors(infos, ""svd_cpu"");
} else {
  singleCheckErrors(infos.item<int64_t>(), ""svd_cpu"");
}
```
with it

## Motivation

Deduplicate code

## Context

Follow-up to https://github.com/pytorch/pytorch/pull/69437


cc @jianyuh @nikitaved @pearu @mruberry @walterddr @IvanYashchuk @xwang233 @Lezcano","2021-12-06","2022-02-08","64","priorized",NULL,"Richard Zou","lezcano","PROJECT PATTERNS"
95,"[PYTORCH]: log error","there may be a log error in  pytorch/aten/src/THC/THCGenerateByteTypes.h, in line2,  #error ""You must define THC_GENERIC_FILE before including THGenerateByteTypes.h"", the ""THGenerateByteTypes.h"" should be ""THCGenerateByteTypes.h"".
bty, this file is a general file to generate files of different scalar_t, so i think, the name is better to be THGenerateTypes.h. 
","2021-12-06","2022-03-04","88","priorized",NULL,"zgplvyou","abhamedewar","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
96,"torch.return_types: simplify implementation","## üöÄ Feature

Follow up to this thread: https://github.com/pytorch/pytorch/pull/66614#discussion_r745505413

The implementation of torch.return_types uses a static map. This unfortunately duplicates the memory needed for torch.return_types (but the total memory needed is not that much). We should try to get rid of the map by turning the entries in the map into globals that get set during initialization and used directly.

## Motivation

Decrease the amount of memory used by PyTorch; also Better Engineering

## Pitch

We should try to get rid of the map by turning the entries in the map into globals that get set during initialization and used directly.

## Alternatives

Use a constexpr map. This seems like overkill

## Additional context

n/a


cc @ezyang @bhosmer @smessmer @ljk53 @bdhirsh","2021-12-01","2023-09-05","643","priorized",NULL,"Richard Zou","Richard Zou","OTHERS"
97,"Run distributed tests with TORCH_CPP_SHOW_STACKTRACES=1","## üöÄ Feature

This environment variable prints out the C++ stacktrace when an error with TORCH_CHECK(false, ...) is raised which is helpful for quickly root causing where the issue is coming from.

See for example the logs in https://github.com/pytorch/pytorch/runs/4302443560?check_suite_focus=true. It is hard to find where in C++ code the error comes from, but enabling TORCH_CPP_SHOW_STACKTRACES=1 would quickly point it to here in FileStore: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/FileStore.cpp#L342 as well as the tensorpipe agent callsite: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L468 which will help the speed of debugging.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-30","2021-12-08","8","low priority",NULL,"Rohan Varma","Rohan Varma","OTHERS"
98,"DDP only syncs parameters used in most recent pass when `find_unused_parameters` is True.","## üêõ Bug

I'm working with a training loop with gradient accumulation, where several forwards/backwards passes run for each call to the optimizer step. To reduce communication overhead, DDP syncs are disabled for all but the final gradient accumulation step before the optimizer step. I'm also using an optimized model which randomly disables various parts of itself on each pass. Critically, the parts of the model that are disabled can vary between gradient accumulation steps. The problem I'm observing is that the DDP sync ran on that final grad accum step appears to only be syncing the parameters used on that step. Parameters unused on the final step but used on previous steps are never synced.

## To Reproduce

Steps to reproduce the behavior:

```python
import contextlib
import os

import torch
import torch.distributed
import torch.multiprocessing
import torch.nn as nn

GRADIENT_ACCUMULATION_STEPS = 2


class SimpleConditionalModel(nn.Module):
    # uses nn1 layer on the first pass; nn2 layer on the second pass

    def __init__(self):
        super().__init__()

        self.nn1 = nn.Linear(1, 1)
        self.nn2 = nn.Linear(1, 1)

        self.loss = nn.MSELoss()

        self.state = 0

    def forward(self, input):
        if self.state == 0:
            self.state = 1
            return self.nn1(input)
        else:
            self.state = 0
            return self.nn2(input)


def test_model(rank):
    os.environ[""RANK""] = str(rank)
    os.environ[""MASTER_ADDR""] = ""127.0.0.1""
    os.environ[""MASTER_PORT""] = ""29000""

    torch.distributed.init_process_group(backend='gloo', world_size=2)
    model = SimpleConditionalModel()
    ddp = torch.nn.parallel.DistributedDataParallel(model, find_unused_parameters=True)

    for microbatch_idx in range(GRADIENT_ACCUMULATION_STEPS):
        context = contextlib.nullcontext
        if microbatch_idx < GRADIENT_ACCUMULATION_STEPS - 1:
            context = ddp.no_sync

        with context():
            input = torch.rand((1, ))
            output = ddp.forward(input)
            target = torch.rand((1, ))

            loss = model.loss(output, target)
            loss.backward()

    print(f'Rank {rank} gradients: {[p.grad for p in model.parameters()]}')


if __name__ == '__main__':
    torch.multiprocessing.spawn(test_model, nprocs=2)
```

Output:
```
Rank 0 gradients: [tensor([[-1.1942]]), tensor([-1.4291]), tensor([[-0.4950]]), tensor([-3.3037])]
Rank 1 gradients: [tensor([[-0.3461]]), tensor([-0.4722]), tensor([[-0.4950]]), tensor([-3.3037])]
```
The first two gradients, corresponding to the `nn1` layer, are unsynced.

## Expected behavior

DDP should be able to track used/unused parameters across multiple forwards/backwards passes.

## Environment
 - PyTorch Version: 1.9.0
 - OS: Linux
 - How you installed PyTorch: `pip`
 - Python version: 3.8.0


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-29","2024-05-09","892","low priority",NULL,"Jamie Bloxham","TNTWEN","OTHERS"
99,"distributed/algorithms/ddp_comm_hooks/test_ddp_hooks and distributed/algorithms/quantization/test_quantization don't run in OSS CI","## üêõ Bug

""distributed/algorithms/ddp_comm_hooks/test_ddp_hooks"", ""distributed/algorithms/quantization/test_quantization"" tests do not run in CI, i.e. they are not contained in `DISTRIBUTED_TESTS` list here: https://github.com/pytorch/pytorch/blob/master/test/run_test.py#L302

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-29","2022-06-30","213","low priority",NULL,"Rohan Varma","Rohan Varma","SOFTWARE ARCHITECTURE"
100,"[DDP] Static graph should print out unused parameters when they are detected. ","## üöÄ Feature

Recent debugging of a DDP issue/hang with static graph resulted in some confusion around why static graph is picking up some parameters as unused on a rank vs not. During the first iteration, static graph will memorize how often hooks are called on a parameter, so if this value is zero, we should log these parameter names for each rank for better debugability.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-23","2022-07-28","247","low priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
101,"Cannot use DDP with NCCL backend on A100 GPUs","## üêõ Bug

Hi, I am unable to use DDP with the NCCL backend when using a machine with 8x A100 GPU's. I am currently running the latest pytorch 1.10 release and have tried multiple versions of CUDA: 11.3, 11.4 and 11.5. In all cases the pattern is the same: I can use DDP when using the gloo backend, but with nccl the code freezes during `dist.init_process_group`. The script to reproduce is the one provided by pytorch.

## To Reproduce

Steps to reproduce the behavior:

1. Run this script on a machine with A100 GPUs: https://github.com/pytorch/examples/blob/master/distributed/ddp/main.py

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Environment

 - PyTorch Version (e.g., 1.0): 1.10
 - OS (e.g., Linux): Linux
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Build command you used (if compiling from source):
 - Python version: 3.8
 - CUDA/cuDNN version: 11.3
 - GPU models and configuration: A100
 - Any other relevant information:

## Additional context

<!-- Add any other context about the problem here. -->


cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-22","2022-04-08","137","high priority",NULL,"Jeremy Wohlwend","monajalal","OTHERS"
102,"retry_on_connect_failures decorator appears to no longer work","## üêõ Bug

Seems like since https://github.com/pytorch/pytorch/pull/68226, `retry_on_connect_failures` test decorator stopped working as it does exact match on the string ""Address already in use"".

This is causing failures such as: https://github.com/pytorch/pytorch/runs/4242952936?check_suite_focus=true

Should be a pretty easy fix to make the decorator match on the updated string.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-17","2021-11-19","2","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
103,"[c10d] ProcessGroupNCCL work inconsistent with ProcessGroupGloo","The `work.result()` of ProcessGroupNCCL and ProcessGroupGloo are inconsistent, for example, `all_gather(..).wait()` will return different values for these two process group, this is because in ProcessGroupNCCL, we flatten the output first, then result assigned based on the flattened output, instead of the original output. See https://github.com/pytorch/pytorch/blob/515d9fb2a99586e62cfb941cfc51e86e7d58c1f4/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp#L1275

We need to fix ProcessGroupNCCL to attach the correct output

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-16","2022-06-03","199","high priority",NULL,"Wanchao","rohan-varma","OTHERS"
104,"DISABLED test_forward_overlap (__main__.TestForwardOverlapWorldSizeTwo)","## üêõ Bug

`test_forward_overlap (__main__.TestForwardOverlapWorldSizeTwo)` FSDP test is flaky, see: 
https://github.com/pytorch/pytorch/runs/4217545932?check_suite_focus=true

Relevant snippet: 

```
======================================================================
ERROR [21.066s]: test_forward_overlap (__main__.TestForwardOverlapWorldSizeTwo)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 458, in wrapper
    self._join_processes(fn)
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 677, in _join_processes
    self._check_return_codes(elapsed_time)
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 722, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 0 exited with error code 10 and exception:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 576, in run_test
    getattr(self, test_name)()
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 460, in wrapper
    fn()
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 112, in wrapper
    return func(*args, **kwargs)
  File ""/var/lib/jenkins/workspace/test/distributed/fsdp/test_fsdp_overlap.py"", line 237, in test_forward_overlap
    self._dist_train()
  File ""/var/lib/jenkins/workspace/test/distributed/fsdp/test_fsdp_overlap.py"", line 211, in _dist_train
    self.assertTrue(s * 10 < l)
  File ""/opt/conda/lib/python3.6/unittest/case.py"", line 682, in assertTrue
    raise self.failureException(msg)
AssertionError: False is not true
```

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-15","2022-04-08","144","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
105,"[FSDP] Implement a standalone _recursive_wrap that is not dependent on ConfigAutoWrap","## üöÄ Feature

https://github.com/pytorch/pytorch/pull/68155 is adding preliminary support for auto wrapping in FSDP API, but needs some follow up work. In particular, it relies on `ConfigAutoWrap.recursive_wrap` to reuse existing checked in code, but we should clean this up and only maintain the needed portion of `wrap.py`: https://github.com/pytorch/pytorch/blob/master/torch/distributed/_fsdp/wrap.py and remove the dependency on `ConfigAutoWrap`. 

cc @zhaojuanmao @mrshenli @rohan-varma","2021-11-12","2022-01-05","54","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
106,"Alleviate sccache connectivity issues on CI","See on 11/2:

[PyTorch] Add IValue::toTupleRef() (#65504) win-vs2019-cuda11.3-py3 on trunk: https://github.com/pytorch/pytorch/runs/4083329308?check_suite_focus=true

and also in binaries
https://app.circleci.com/pipelines/github/pytorch/pytorch/403881/workflows/9fb5af0d-a370-4170-ba55-fab0095f2f73/jobs/16649825

```
FAILED: third_party/fbgemm/CMakeFiles/fbgemm_generic.dir/src/EmbeddingSpMDMNBit.cc.obj 
C:\w\b\windows\tmp_bin\sccache-cl.exe  /nologo /TP -DTH_BLAS_MKL -DWIN32_LEAN_AND_MEAN -D_OPENMP_NOFORCE_MANIFEST -I..\third_party\cpuinfo\include -I..\third_party\fbgemm\third_party\asmjit\src -I..\third_party\fbgemm\include -I..\third_party\fbgemm -I..\cmake\..\third_party\benchmark\include -I..\cmake\..\third_party\googletest\googlemock\include -I..\cmake\..\third_party\googletest\googletest\include -I..\third_party\protobuf\src -IC:\w\b\windows\mkl\include -I..\third_party\XNNPACK\include /DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/w/b/windows/mkl/include /wd4244 /wd4267 /wd4305 /wd4309 /MD /O2 /Ob2 /DNDEBUG /w /bigobj -std:c++14 /showIncludes /Fothird_party\fbgemm\CMakeFiles\fbgemm_generic.dir\src\EmbeddingSpMDMNBit.cc.obj /Fdthird_party\fbgemm\CMakeFiles\fbgemm_generic.dir\ /FS -c ..\third_party\fbgemm\src\EmbeddingSpMDMNBit.cc
error: failed to execute compile
caused by: error reading compile response from server
caused by: Failed to read response header
caused by: failed to fill whole buffer
```

Some previous investigation done by @suo suggests investigating mozilla/sccache#256. Suggested fixes in that issue are 1) reducing parallelism, and 2) increasing the timeout 

cc @peterjc123 @mszhanyi @skyline75489 @nbcsm @seemethere @malfet @pytorch/pytorch-dev-infra","2021-11-02","2022-03-01","119","priorized",NULL,"Jane (Yuan) Xu","mszhanyi","PROJECT PATTERNS"
107,"Add some profiling ranges to FSDP","## üöÄ Feature

FSDP has many extra steps in forward, backward to all gather parameters, scatter gradients, run CPU -> GPU copies for CPU offload, etc. For easier to understand profiling, it would be useful to add ranges on some of these events so that we know the high-level context/operations of what some low level operators correspond to.

For example, the following stream sync in backwards by itself might be hard to diagnose - 
![image](https://user-images.githubusercontent.com/8039770/139954574-fd6b909a-4b33-48af-af73-1a59e4936466.png)

Although this sync is caused by synchronizing the current stream which we need to do when offloading gradients, so it would be good to label the code section which does this as so.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-11-02","2022-06-04","214","low priority",NULL,"Rohan Varma","Rohan Varma","OTHERS"
108,"Preserve core dump as artifacts","It would make the process of debugging easier for issues such as https://github.com/pytorch/pytorch/issues/67646

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2021-11-02","2022-03-14","132","priorized",NULL,"Jane (Yuan) Xu","janeyx99","OTHERS"
109,"Sharing file for init_process_group and init_rpc can potentially cause issues due to destructor of file","## üêõ Bug

This issue was likely the reason for the flaky tests for #66488, #66951 which are now fixed.

When using the same file in file initialization for init_process_group and init_rpc, destruction of process_group will trigger the destructor of FileStore and remove the file from disk https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/FileStore.cpp#L277-L288. This can cause issues since the same file is also used by RPC.

## Basic Example

```python
import torch.distributed as dist
import torch.distributed.rpc as rpc
import tempfile
import os.path

file = tempfile.NamedTemporaryFile(delete=False)
file_name = file.name
print(f""Using file: {file_name}, File Exists: {os.path.isfile(file_name)}"")

# Init RPC using file
rpc_backend_options = rpc.TensorPipeRpcBackendOptions()
rpc_backend_options.init_method = f""file://{file_name}""
rpc.init_rpc(""worker"", rank=0, world_size=1, rpc_backend_options=rpc_backend_options)

# Init PG using file
dist.init_process_group(""gloo"", rank=0, world_size=1, init_method=f""file://{file_name}"")

# Destroying PG also removes file
dist.destroy_process_group()
print(f""File Exists: {os.path.isfile(file_name)}"")
```

Output:
```
Using file: /tmp/tmplxkgrz_r, File Exists: True
File Exists: False
```

## Solutions 

(UPDATED to add context for bootcamp and incorporate Rohan's suggestion)

On initialization FileStore can [`add()`](https://pytorch.org/docs/master/distributed.html#torch.distributed.Store.add) an internal key for the number of FileStore instances that are currently using that file.

https://github.com/pytorch/pytorch/blob/d71b8e1a8db19facad9b4fdc580cc795b760ba99/torch/csrc/distributed/c10d/FileStore.cpp#L268-L274

On destruction, it should inspect this key and only delete the file if there are no more instances referencing it.

https://github.com/pytorch/pytorch/blob/d71b8e1a8db19facad9b4fdc580cc795b760ba99/torch/csrc/distributed/c10d/FileStore.cpp#L276-L301

## Validation

The script in the beginning of the description should return that the file still exists after `destroy_process_group()` because the FileStore instantiation during `init_rpc()` is still valid.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-29","2022-10-07","343","low priority",NULL,"Howard Huang","KinglittleQ","SOFTWARE ARCHITECTURE"
110,"Remove TH/THC Storage functions for unused dtypes","Followup from #47442

Now that we're only using TH/THC ByteStorage internally, it should be relatively simple to remove all the generated TH/THC Storage functions for every dtype except for Byte.

cc @ezyang @bhosmer @smessmer @ljk53 @bdhirsh","2021-10-28","2021-11-01","4","priorized",NULL,"kurtamohler","kurtamohler","OTHERS"
111,"[FSDP][BE] Rename some methods/streams","## üöÄ Feature
Some fsdp methods/attributes can be renamed to more clearly reflect what they do:

- post_backward_hook is not really called post backward, it is called after a grad is ready, the backwards can still be going on. Possibly rename it to _grad_ready_hook. Thus wait_for_post_backward can be renamed appropriately as well.
- wait_for_previous_optim_step actually does a pretty generic sync with the current stream so it is more general, can be renamed aptly as well. 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-26","2023-01-12","443","low priority",NULL,"Rohan Varma","awgu","OTHERS"
112,"[Distributed Tests] Run a single distributed test in a standalone process","## üöÄ Feature

There are a lot of [distributed flaky tests](https://github.com/pytorch/pytorch/issues?q=is%3Aopen+is%3Aissue+label%3A%22oncall%3A+distributed%22+label%3A%22module%3A+flaky-tests%22+) and we would like to make them easier for developers to reproduce and ultimately drive down the # of flaky tests. 

One major issue is reproducibility of these tests. For example, in our [current test setup](https://github.com/pytorch/pytorch/blob/master/test/run_test.py): we run the entire test file at once, but developers usually run a single flaky test to reproduce. Unifying the way the test is run in OSS and how a developer might run them would make the reproducibility a lot easier and contribute to a reduction in flaky tests. 

cc @ezyang @gchanan @zou3519 @bdhirsh @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @mruberry","2021-10-21","2021-12-22","62","high priority",NULL,"Rohan Varma","mruberry","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
113,"Test classes should extend common_utils.TestCase vs vanilla TestCase","There are many cases where unittest.TestCase is extended today instead of our specialized common_utils.TestCase in our code base, like in many distributed tests, fx tests, and onnx tests:

https://github.com/pytorch/pytorch/search?q=Unittest.TestCase

Sometimes, this is intentional, but often times, I suspect it is accidental or due to oversight/unawareness. These test cases miss out on the customizations we place in https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_utils.py#L1307, which include:
- Not enjoying the functionality of pre-written **wrappers**
- The CI not being able to check and skip if they are a **slow test** 
- The CI not being able to check and skip if they are **disabled**

To enforce this, we can check in run_tests() if the test is a vanilla unittest.TestCase  and error when that is the case. This would capture the case where the test writer is trying to use all the features in common_utils.py but neglected to extend the right TestCase. This would also allow test classes that intend to use the vanilla TestCase to do so without being yelled at. 



cc @seemethere @malfet @pytorch/pytorch-dev-infra","2021-10-19","2022-01-25","98","priorized",NULL,"Jane (Yuan) Xu","janeyx99","OTHERS"
114,"Add documentation for how to work with PyTorch in Windows SSH","Our Windows machines require all dependencies to be installed before you could do anything with PyTorch (like run tests).

We should document how someone could get to a stage where they can work with PyTorch, or provide a script to automate this process.

Moreover, our Windows scripts need cleaning up in general, but that's more tracked with https://github.com/pytorch/pytorch/issues/65718

cc @brianjo @mruberry","2021-10-19","2022-02-28","132","priorized",NULL,"Jane (Yuan) Xu","janeyx99","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
115,"Merge `ciflow-should-run` job into the first build job","Every time PR is opened more-and-more `ciflow-should-run` jobs are reported in signal box.
But this job exists there just to check the condition, which can totally by done by the first job in the workflow

cc @seemethere @malfet @pytorch/pytorch-dev-infra","2021-10-15","2021-12-22","68","priorized",NULL,"Nikita Shulga","malfet","OTHERS"
116,"RFC: Create unified CI experience for pytorch and domain libraries","## Introduction
This is a project to **consolidate** and **standardize** CI process across pytorch, vision, text and audio to allow a uniform CI experience. The current status: each pytorch repository runs its own CI process with no shared standards and the builds of domain libs can be broken due to dependency on `pytorch`. 

### Standardization
- use standard specifications of CI workflows for domain libraries
- develop tooling to support/automate the generation of the standards

### Consolidation 
- create a central repo to host the common build rules that can be shared across repositories
- build e2e CI process across dependent repositories as [integration test to early detect errors](https://github.com/pytorch/builder/issues/833)

## Prototyping
### Scope
We started with consolidating linux binary builds for CircleCI and intended to cover Github Action. At the time of writing this proposal all domain libraries are 100% on CircleCI.
### Design
Use `pytorch/builder` as the central repository to:
- host and test changes to common build scripts
- trigger CI of pytorch and domain libraries.
- host tools for CI template generation for domain libraries

### Main Tasks
- [x] https://github.com/pytorch/builder/issues/866
  - [x] https://github.com/pytorch/builder/pull/860
  - [x] https://github.com/pytorch/builder/pull/861
- [ ] https://github.com/pytorch/builder/issues/865 
  - [ ] https://github.com/pytorch/builder/pull/874
  - [ ] https://github.com/pytorch/builder/pull/877
- [x] https://github.com/pytorch/builder/issues/880 
 


cc @ezyang @malfet @zhouzhuojie @janeyx99 @seemethere @orionr @pytorch/pytorch-dev-infra","2021-10-14","2024-05-08","937","priorized",NULL,"Lan Gong","atalman","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
117,"DISABLED test_collectives (__main__.PythonProcessGroupTest)","https://github.com/pytorch/pytorch/runs/3859954275

```
======================================================================
ERROR [1.014s]: test_collectives (__main__.PythonProcessGroupTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 423, in wrapper
    self._join_processes(fn)
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 642, in _join_processes
    self._check_return_codes(elapsed_time)
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 687, in _check_return_codes
    raise RuntimeError(error)
RuntimeError: Process 1 exited with error code 10 and exception:
Traceback (most recent call last):
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 541, in run_test
    getattr(self, test_name)()
  File ""/opt/conda/lib/python3.6/site-packages/torch/testing/_internal/common_distributed.py"", line 425, in wrapper
    fn()
  File ""/var/lib/jenkins/workspace/test/distributed/test_c10d_common.py"", line 774, in test_collectives
    dist.init_process_group(""dummy"", rank=self.rank, world_size=self.world_size)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 608, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 231, in _store_based_barrier
    worker_count = store.add(store_key, 0)
RuntimeError: Broken pipe

```

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-11","2021-10-26","15","high priority",NULL,"mrshenli","mruberry","OTHERS"
118,"test_nccl_timeout should not rely on time.sleep()","## üêõ Bug

In general `time.sleep` in unittests are prone to breakages and flakiness because we are relying on the passage of time rather than a certain event being triggered to test some behavior. `test_nccl_timeout` currently has this issue, this task is to refactor the test such that it does not rely on `time.sleep`. 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-08","2021-12-09","62","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
119,"Add _reduce_scatter_base and _allgather_base to processGroupWrapper","_reduce_scatter_base and _allgather_base are used in FSDP code base, it is good to add those APIs into processGroupWrapper as well, so that these two collectives can be debugged in DETAIL mode

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-08","2022-06-29","264","priorized",NULL,"Yanli Zhao","Yanli Zhao","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
120,"Clarify hook support in DDP","## üìö Documentation

Main discussion in https://github.com/pytorch/pytorch/issues/35191, but basically DDP should now properly support forward and backward hooks now that single process multiple device mode is deprecated (when we had replication across GPUs in one process hooks would not fire properly). 

To ensure this case is fixed, let's add comprehensive unittesting of forward and backward hooks, as well as remove the misleading language from the documentation. 


cc @ezyang @gchanan @zou3519 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-06","2022-04-07","183","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
121,"Track models with SyncBN in DDP","## üöÄ Feature
We are interested in understanding and improving performance of models with SyncBN, so we would like to detect if models have SyncBN and log that to DDP logging.

This task is to add a field ""has_sync_bn"" to DDP logging data and ensure it is correctly populated for models with sync batch norm. Model is generally instantiated such as - 

```
model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[gpu_index],
            output_device=gpu_index,
            gradient_as_bucket_view=True,
        )
```

so we can log this in the constructor where `set_construction_data_and_log` is called - https://github.com/pytorch/pytorch/blob/master/torch/nn/parallel/distributed.py#L672

To detect whether a model actually has sync BN or not, we can probably iterate through modules recursively and run `isinstance` checks for SyncBN. 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang","2021-10-06","2021-10-19","13","low priority",NULL,"Rohan Varma","Rohan Varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
122,"TestZeroRedundancyOptimizerDistributed.test_multiple_groups is failing intermittently","See https://github.com/pytorch/pytorch/runs/3778916355, relevant snippet:

```
ERROR: TestZeroRedundancyOptimizerDistributed.test_multiple_groups
Traceback (most recent call last):
  File ""C:\actions-runner\_work\pytorch\pytorch\build\win_tmp\build\torch\testing\_internal\common_distributed.py"", line 423, in wrapper
    self._join_processes(fn)
  File ""C:\actions-runner\_work\pytorch\pytorch\build\win_tmp\build\torch\testing\_internal\common_distributed.py"", line 642, in _join_processes
    self._check_return_codes(elapsed_time)
  File ""C:\actions-runner\_work\pytorch\pytorch\build\win_tmp\build\torch\testing\_internal\common_distributed.py"", line 692, in _check_return_codes
    raise RuntimeError(
RuntimeError: Process 0 terminated or timed out after 100.03042507171631 seconds
```

Test did not fail on preceding or subsequent run. 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @mruberry","2021-10-02","2022-03-16","165","high priority",NULL,"Mike Ruberry","mruberry","PROJECT PATTERNS"
123,"New utils.checkpoint rollout.","Thanks to the work done on SavedVariable hooks, we can know have a new implementation of checkpoint that does not require re-entrant autograd and so make it more composable with systems that don't support re-entrant autograd (most notably DDP).

We have a full fledge prototype and testing in https://github.com/pytorch/pytorch/pull/62964 that implement the core logic and ensures it works as expected.

The API we want to introduce here is to have a `use_reentrant: bool` argument to checkpoint and checkpoint_sequential that control which version of checkpoint is used.

Due to BC concerns we won't be able to just change the default to the new version so we should (one PT version per bullet point):
- Add the new boolean flag that default to True (current behavior) and deprecate not passing the value for it
- Remove the default value for the flag
- Add the a default value of False for the flag

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @albanD @gqchen @pearu @nikitaved @soulitzer @Lezcano @Varal7","2021-09-23","2022-01-10","109","high priority",NULL,"albanD","albanD","OTHERS"
124,"[Beta] Avoid register hooks to AccumulateGrad in every FSDP iteration","DDP only registers the grad hook in ctor once, but FSDP registers and removes the hook in every iteration. Explore if it is possible to stablize FSDP hook registration logic to only do it once. 

https://github.com/pytorch/pytorch/blob/af20f2e7b0dc086e825dd621f58885bda18f0f6a/torch/distributed/_fsdp/fully_sharded_data_parallel.py#L1187-L1197

https://github.com/pytorch/pytorch/blob/af20f2e7b0dc086e825dd621f58885bda18f0f6a/torch/distributed/_fsdp/fully_sharded_data_parallel.py#L1365-L1372

cc @zhaojuanmao @mrshenli @rohan-varma","2021-09-17","2023-06-07","628","priorized",NULL,"mrshenli","awgu","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
125,"[quant] Add support for Embedding/EmbeddingBag quantization via dynamic quant APIs","Currently for eager mode quantization the user needs to follow the static quantization APIs in order to quantize embeddings. 
An example of how the user needs to do this is below (combine static + dynamic quant APIs)

```
class EmbeddingWithLinear(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = torch.nn.Embedding(num_embeddings=10, embedding_dim=12)
        self.fc = torch.nn.Linear(5, 5)

    def forward(self, indices, linear_in):
        return self.emb(indices), self.fc(linear_in)

qconfig_dict = {'fc' : default_dynamic_qconfig}
model = EmbeddingWithLinear()
quantize_dynamic(model, qconfig_dict, inplace=True)

model.emb.qconfig = float_qparams_weight_only_qconfig
prepare(model, inplace=True)
convert(model, inplace=True)

```
Which generates the following model
```
EmbeddingWithLinear(
  (emb): QuantizedEmbedding(num_embeddings=10, embedding_dim=12, dtype=torch.quint8, qscheme=torch.per_channel_affine_float_qparams)
  (fc): DynamicQuantizedLinear(in_features=5, out_features=5, dtype=torch.qint8, qscheme=torch.per_tensor_affine)
)
```

Technically embeddings can be supported in both APIs, since it is weight only quantization.


cc @jerryzh168 @jianyuh @raghuramank100 @jamesr66a @vkuzo","2021-09-16","2021-10-08","22","low priority",NULL,"supriyar","supriyar","OTHERS"
126,"Make gradcheck settings kwarg only ","See this comment: https://github.com/pytorch/pytorch/pull/65040#discussion_r709333873

cc @ezyang @gchanan","2021-09-16","2021-09-21","5","priorized",NULL,"Jeffrey Wan","soulitzer","OTHERS"
127,"[Efficiency] Avoid H2D copy and stream sync in clip_grad_norm_","https://github.com/pytorch/pytorch/pull/63881#discussion_r709551959

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @cbalioglu @gcramer23","2021-09-15","2023-06-08","631","priorized",NULL,"Yanli Zhao","awgu","PROJECT PATTERNS"
128,"[Comment] calc_grad_norm_  always compute the norm in full precision and avoid duplication code in clip_grad_norm_ of torch core","https://github.com/pytorch/pytorch/pull/63881#discussion_r709549042

https://github.com/pytorch/pytorch/pull/63881#discussion_r709553560

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @cbalioglu @gcramer23","2021-09-15","2023-11-07","783","priorized",NULL,"Yanli Zhao","Yanli Zhao","OTHERS"
129,"[proposal] Let's unify the various gather/scatter ops.","### Proposal: Let's make all of the `gather`-type ops (`index.Tensor`, `index_select`, `gather`) composite, and have them all call into the same underlying operator (`gather_general`). This will 1. reduce effort of transformations/backends that need to handle every operator, 2. reduce inefficiencies from having users use the wrong operator, and 3. allow us to focus on optimizing a single operator.

### Context

Motivated by needing to write batching rules for `index.Tensor`, I realized that there are often multiple ways of doing the same operation for the scatter ops. For example, let's say we want to `index_select` along the first dimension. Here are 3 ways of writing it:

```
def index_select1(x, y):
    return torch.index_select(x, 0, y)
   
def index_select2(x, y):
    dim = 0
    for ii in range(0, len(x.shape)):
        if ii != dim:
            y = y.unsqueeze(ii)
    expanse = list(x.shape)
    expanse[dim] = -1
    return torch.gather(x, dim, y.expand(expanse))
    
def index_select3(x, y):
    return x[y] # calls index.Tensor under the hood
```

Now, which one is the fastest? This ... is a very annoying question. The answer is ... it depends on your rank, your shape, and your device.

Let's define the function we're benchmarking as `torch.index_select(A, 0, B)`, and let's say we're going over the shape range `[10, 100]`, with a rank of 2. That means we're going to benchmark 8 results: `A: [10, 10], B: [10]`, `A: [10, 10], B: [100]`, `A: [10, 100], B: [10]`, and so on. Then, we're going to count how often implementation 1 (`torch.index_select`), implementation 2 (`torch.gather`), and implementation 3 (`indexing`) is the fastest.

Let's start off by going over the shape range `[10, 100, 1000]` with a rank of 3 on the CPU. We see that of the 81 results, `index_select` was the fastest 22 times, `gather` was never the fastest, and `index` was the fastest 59 times, with an average margin of 36% (i.e. 1.36 seconds vs 1.0 seconds). Ok, that's somewhat reasonable, but perhaps `index_select` should be calling into `index` in some cases?

Now, let's check the same shapes/ranks, but on GPU.

```
shapes: [10, 100, 1000], rank: 3
index_select: 28
gather: 30
index: 23
average margin: 15%
```

Oh... now we see that `gather` has thrown its hat into the ring, and is sometimes the fastest. But... if we try limiting only to fairly big shapes, we see...

```
shapes: [100, 1000], rank: 3
index_select: 0
gather: 14
index: 2
average margin: 13%
```
Ok, so now for fairly big shapes on CUDA, `gather` seems to be faster than everything else.

So, we have a situation where for the same operation, there are multiple ways of writing it, and it's non-obvious which one will be the fastest. Judging from these results, it seems very possible that for large enough GPU indexing, we `torch.index_select` should just call into `gather` instead of running it itself.

This hurts both 1. people writing transformations, 2. users writing code, and 3. PyTorch developers

For people writing transformations (like vmap), I don't know whether I should call into `gather`, `index_select`, or `index.Tensor` and I'm worried that choosing the wrong one will hurt performance.

For users, they need to worry about the same thing. Moreover, most of them probably have no idea how these differ in performance, and thus might be leaving performance on the table by choosing the wrong option.

For PyTorch developers, what this situation means is that any optimizations/improvements they make to these ops have limited applicability, since users might be using a completely different code path.

I propose that we make all of these ops (`gather`, `index_select`, and `index.Tensor`) composite, and have them all call into the same unified implementation. It's not totally clear to me what the required semantics are, but XLA's gather may be a good place to start: https://www.tensorflow.org/xla/operation_semantics#gather

I've mainly talked about gather ops here, but I suspect our scatter ops are in a similar situation.","2021-09-13","2024-08-20","1072","priorized",NULL,"Horace He","ngimel","OTHERS"
130,"Convert unexpected test suite warnings into errors","This would help prevent unexpected warnings from being thrown by native PyTorch functions, which are surprising and disruptive to users.

cc @mruberry","2021-09-08","2021-09-08","0","priorized",NULL,"Mike Ruberry","ngimel","OTHERS"
131,"GHA failures require scanning the log","When a test fails on a GHA job engineers have to scan through the log to find the failure. This can be difficult and time consuming. It'd be nicer if failures were collected (like they were on CircleCI). 

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @seemethere @malfet @lg20987 @pytorch/pytorch-dev-infra what's the right label for this?

cc @ngimel and @suo who I think also struggled with this recently","2021-09-07","2021-09-15","8","high priority",NULL,"Mike Ruberry","seemethere","PROJECT PATTERNS"
132,"Replace usages of c10::str with fmt::format in distributed","## üêõ Bug

As per the discussion in https://github.com/pytorch/pytorch/pull/64241, we would prefer to use `fmt` library (specifically `fmt::format`) to format strings/error messages instead of c10::str. However the `fmt` library is currently only linked to libtorch_python sources, see the PR that added it: https://github.com/pytorch/pytorch/pull/64241. 

To use `fmt::format` we would probably need to link fmt against libtorch, and then we can update the callsites introduced in `ProcessGroupNCCL` and `NCCLUtils`  in the above PR.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @gcramer23","2021-09-07","2023-10-20","773","low priority",NULL,"Rohan Varma","andreigh","PROJECT PATTERNS"
133,"Disambiguate JOB_BASE_NAME, BUILD_ENVIRONMENT in our scripts","Currently we have the following concepts in our CI scripts/test structure:

* The idea of a job prefix, used when trying to consolidate test time stats between build and test jobs. Relevant code https://github.com/pytorch/pytorch/blob/f2c47cf4dbbdd0cafc1bd2118121c6eda3947f3f/tools/testing/test_selections.py#L25-L35 

* The idea of a job name, which is the most specific one could go with test names, used here https://github.com/pytorch/pytorch/blob/f2c47cf4dbbdd0cafc1bd2118121c6eda3947f3f/tools/stats/print_test_stats.py#L782-L793

* The idea of a build environment, which is used everywhere in our build and test scripts in .jenkins/pytorch.

To address those concepts, we use the following environment variables:
* BUILD_ENVIRONMENT: this is sometimes used as ALL three of the concepts above
* JOB_BASE_NAME: this attempts to be a job name but is not very specific on GHA and _is_ specific on Circle because we set JOB_BASE_NAME to CIRCLE_JOB. This variable is also used to derive BUILD_ENVIRONMENT like the first code example above.

All this is not fun to deal with. We should clean it up. A good idea is to revamp what we mean by BUILD_ENVIRONMENT and JOB_BASE_NAME. 

Example proposal:

* BUILD_ENVIRONMENT: should represent the environment in which pytorch was built! This could also function as a great job prefix.
* JOB_NAME: should represent the most specific description of a job. This would include test config and shard number and all that good stuff. 
* We may want other variables to represent other ideas, and I am not opposed to more variables, but we should clarify the existing ones and stick to our definitions of them. 


cc @ezyang @seemethere @malfet @walterddr @lg20987 @pytorch/pytorch-dev-infra","2021-08-27","2022-02-28","185","priorized",NULL,"Jane (Yuan) Xu","seemethere","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
134,"Document how to disable python tests on CI through issues","## üìö Documentation

We should document the use of issues to disable tests in a public wiki.


cc @ezyang @seemethere @malfet @walterddr @lg20987 @pytorch/pytorch-dev-infra","2021-08-27","2021-10-11","45","priorized",NULL,"Jane (Yuan) Xu","janeyx99","SOFTWARE ARCHITECTURE"
135,"check_errors argument does nothing in torch._lu_with_info","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->
`check_errors` argument for `torch._lu_with_info` does nothing since https://github.com/pytorch/pytorch/pull/28608 is merged. See https://github.com/pytorch/pytorch/pull/28608/files#r339365219. This function is used in `torch.lu` and `torch.linalg.det` implementations.

Since `check_errors` doesn't do anything, it should be removed.

## To Reproduce

```python
In [1]: import torch
In [2]: a = torch.ones(3, 3)

In [3]: torch._lu_with_info(a, pivot=True, check_errors=False)
Out[3]: 
torch.return_types._lu_with_info(
LU=tensor([[1., 1., 1.],
        [1., 0., 0.],
        [1., 0., 0.]]),
pivots=tensor([1, 2, 3], dtype=torch.int32),
info=tensor(2, dtype=torch.int32))

In [4]: torch._lu_with_info(a, pivot=True, check_errors=True) # info is positive should raise an error, but doesn't
Out[4]: 
torch.return_types._lu_with_info(
LU=tensor([[1., 1., 1.],
        [1., 0., 0.],
        [1., 0., 0.]]),
pivots=tensor([1, 2, 3], dtype=torch.int32),
info=tensor(2, dtype=torch.int32))
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->



cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano","2021-08-26","2021-11-27","93","priorized",NULL,"Ivan Yashchuk","Ivan Yashchuk","OTHERS"
136,"test_python_dispatch fails under pytest","## üêõ Bug
```
--- Logging error ---
Traceback (most recent call last):
  File ""/scratch/rzou/pt/workspace-env/lib/python3.7/logging/__init__.py"", line 1025, in emit
    msg = self.format(record)
  File ""/scratch/rzou/pt/workspace-env/lib/python3.7/logging/__init__.py"", line 869, in format
    return fmt.format(record)
  File ""/scratch/rzou/pt/workspace-env/lib/python3.7/site-packages/_pytest/logging.py"", line 65, in format
    return super().format(record)
  File ""/scratch/rzou/pt/workspace-env/lib/python3.7/logging/__init__.py"", line 608, in format
    record.message = record.getMessage()
  File ""/scratch/rzou/pt/workspace-env/lib/python3.7/logging/__init__.py"", line 369, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File ""/raid/rzou/pt/workspace-env/bin/pytest"", line 11, in <module>
```

## To Reproduce

`pytest test/test_python_dispatch.py -v`

","2021-07-30","2021-09-12","44","priorized",NULL,"Richard Zou","thanujkumar1999","OTHERS"
137,"[rfc] Consolidate pytorch linter scripts","Our linter scripts are scattered across numerous ad-hoc scripts. A large part of our linter logic is also encoded inside of `.github/workflows/lint.yml`. These features make it difficult to re-use logic, thereby making it both bug-prone and cumbersome to express more complex linter workflows. Further, ad-hoc scripts do not enable a good user experience, making it harder for users to detect and debug flakiness.

Here are some examples of the mentioned issues:
- File filtering logic
    - https://github.com/pytorch/pytorch/blob/master/tools/actions_local_runner.py#L165-L172
    - https://github.com/pytorch/pytorch/blob/master/tools/linter/clang_tidy/run.py#L376-L460
    - https://github.com/pytorch/pytorch/blob/master/tools/linter/clang_format_all.py#L33-L44
    - https://github.com/pytorch/pytorch/blob/master/.github/workflows/lint.yml
    - https://github.com/pytorch/pytorch/blob/master/tools/linter/clang_format_all.py#L33-L44
- Running shell commands
    - https://github.com/pytorch/pytorch/blob/master/tools/linter/clang_tidy/run.py#L173-L195
    - https://github.com/pytorch/pytorch/blob/master/tools/linter/clang_format_all.py#L57-L59
    - https://github.com/pytorch/pytorch/blob/master/tools/actions_local_runner.py#L105-L130
- Error reporting
    - https://github.com/pytorch/pytorch/blob/master/.github/workflows/lint.yml#L117-L123
    - https://github.com/pytorch/pytorch/blob/master/.github/workflows/lint.yml#L391-L399
- Multiple ways to run lints
    - `make <lint name>`
    - `python3 tools/linter/<linter name>`
    - Commit hooks
    - `python3 tools/actions_local_runner.py <lint name>`
- Dependency requirements are scattered

## Proposal
I'm proposing that we consolidate our linter scripts into a single framework (written in Python) that supports common operations like file filtering, running lints on changes, running shell commands, and reliable error reporting. The lints will be exposed through a dispatcher script that will invoke the appropriate linter and report back results.

Once we have this, we should migrate all of the logic from within `.github/workflows/lint.yml` to this new framework, thereby making python scripts the source of ground truth. The new `.github/workflows/lint.yml` would simply invoke the dispatcher script and report back errors (if any)

### Goals
- Support only one way to run linters
- Make it easier to run lints locally
- Provide an ergonomic way to run lints
- Enable code re-use and linter extensibility

This is how I envision the usage to be:
```bash
alias lint=""python3 tools/linter/lint.py""

# Run all lints on the codebase
lint

# Run all lints only on changes
lint --changed-only

# Run a single lint on the codebase
lint mypy

# Run a single lint configured with options
lint clang-tidy --paths torch/csrc --diff-file pr.diff --verbose
```

Once we have this dispatcher script, we can use it to create a `setup.py lint` target, which is advantageous as PyTorch devs use `setup.py` to build their code.

## Implementation

I have a WIP PR (#62393) that implements the aforementioned infrastructure.

It provides a `Lint` base class. Each linter is implemented in a separate module. The module must expose a class that extends the `Lint` base class. Each linter declares the options it accepts (this includes things like file filters, verbose output, extra arguments). The dispatcher script with take the declaration, instantiate the linter, and run it. 

Related issue: #62370

Future vision:
- Possibly release our framework as a separate package
- How can we support existing linter workflows?

@pytorch/pytorch-dev-infra 


cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-07-30","2022-05-04","278","priorized",NULL,"Elton Leander Pinto","suo","OTHERS"
138,"`torch.linalg.svd` does unnecessary conjugate transposes that cancel each other","## üêõ Better engineering

<!-- A clear and concise description of what the bug is. -->
`_svd_helper` is used both for `torch.svd` and `torch.linalg.svd`. The main difference between the two svd versions is that the third return (the V matrix) is conjugate-transposed or not.
`svd_helper` computes V^H, then conjugate transposes it to get V = (V^H)^H, then `linalg.svd` conjugate transposes it again to get Vh = ((V^H)^H)^H. That's too much of back and forth operation for one return.

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->
The result should be computed as efficiently as possible. PyTorch shouldn't do so many identity conversions internally.

## Additional context

<!-- Add any other context about the problem here. -->
PR that introduced `torch.linalg.svd`: https://github.com/pytorch/pytorch/pull/45562.


cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @xwang233 @Lezcano","2021-07-22","2022-02-08","201","priorized",NULL,"Ivan Yashchuk","lezcano","SOFTWARE ARCHITECTURE, PATTERNS AND ARCHITECTURAL STYLES"
139,"torch.chunk docs seem underspecified","## üìö Documentation

[torch.chunk](https://pytorch.org/docs/stable/generated/torch.chunk.html)'s documentation doesn't fully specify how the number and sizes of chunks produced are chosen.

Particularly, it's not clear what happens in cases where the size of the given dimension is not divisible by the number of chunks requested. One of the parameters to the function is ""chunks (int) ‚Äì number of chunks to return,"" but torch.chunk may not actually return that many chunks. For example,

```
>>> torch.chunk(torch.tensor([1, 2, 3, 4, 5]), chunks=4)
(tensor([1, 2]), tensor([3, 4]), tensor([5]))
```

The page could also benefit from some usage examples.

cc @brianjo @mruberry","2021-06-14","2021-08-13","60","priorized",NULL,"Saketh Are","mruberry","SOFTWARE ARCHITECTURE"
140,"torch.get/set_rng_state only handles cpu","This behavior is kinda expected since we also have `torch.cuda.get/set_rng_state` but api `torch.manual_seed`(defined in the same file https://github.com/pytorch/pytorch/blob/master/torch/random.py#L22)  handles both cpu & cuda. 
We should at least improve our documentation for `torch.get/set_rng_state` to tell users it only supports cpu, and if they want cuda as well they need to use `manual_seed`. 
```
[15]
import torch
s0 = torch.get_rng_state()
torch.set_rng_state(s0)
torch.randn(5, device='cpu')

tensor([-0.3407,  0.1835,  0.6271,  0.5801, -1.2006])
[17]
torch.set_rng_state(s0)
torch.randn(5, device='cpu')


tensor([-0.3407,  0.1835,  0.6271,  0.5801, -1.2006])
[21]
torch.set_rng_state(s0)
torch.randn(5, device='cuda')



tensor([-0.2259, -0.1659,  1.6437,  0.2440, -1.2208], device='cuda:0')
[20]
torch.set_rng_state(s0)
torch.randn(5, device='cuda')
tensor([-0.3669, -1.0622, -0.6299,  1.8582,  0.5537], device='cuda:0')
```
cc: @roosephu who reported the issue to me. 

cc @brianjo @mruberry @pbelevich","2021-06-14","2021-06-22","8","priorized",NULL,"Ailing Zhang","bdhirsh","OTHERS"
141,"Figure out why the clang-tidy job in lint.yml failed to login ghcr.io occasionally","Example

![image](https://user-images.githubusercontent.com/658840/121723988-0f940580-ca9c-11eb-9c8d-1a15a8460516.png)




cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @Varal7 @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-06-11","2021-06-23","12","high priority",NULL,"Rex Zhou","zhouzhuojie","OTHERS"
142,"[deploy] Restrict Tensors to only belong to one Python interpreter","Currently, torchdeploy supports associating a single TensorImpl with multiple Python interpreters. There are some downsides to allowing this:

1. If a TensorImpl is mutated across multiple threads, locking is necessary. However, torchdeploy doesn't provide any  mechanisms for ensuring that locking actually happens. This means that this sharing is quite dangerous: normal, ordinarily safe Python code running in torchdeploy can potentially data race and crash the process. Although TorchScript has a similar problem (it permits running multiple TS interpreters in parallel with shared data), we believe torchdeploy is more likely to be affected by this problem because you *really are just writing Python* and people are going to quite reasonably expect to be able to do whatever they want without locking.
2. Associating a single TensorImpl with multiple PyObjects requires us to maintain a separate hash map per interpreter mapping TensorImpl to PyObject, because there isn't enough space on TensorImpl to store all of the PyObjects a TensorImpl may be associated with. This is inefficient (because whenever Tensor crosses to Python we have to do a hash table lookup)

This proposal is to force tensors to only belong to a single Python interpreter for now, so that (1) you cannot share a tensor across multiple interpreters, and (2) we can safely save the PyObject pointer in TensorImpl. This is ""for now"" as we can always decide to relax this restriction later (making cases that previously errored stop erroring) if we decide to change our mind.

The primary implementation idea for how we do this is that Tensors start off unassociated with any Python interpreter. The first time the cross into the PyObject, we atomically assign them to some interpreter; the Tensor is now permanently associated with this interpreter and will raise an error if you try to transfer cross into a PyObject on a different interpreter. This means we store both a pointer to a PyObject, as well as an ID identifying what interpreter it is associated with. On 64-bit systems, these can be packed into a single word, if we specify the interpreter ID to be a 16-bit integer.","2021-05-06","2021-05-20","14","priorized",NULL,"Edward Z. Yang","Edward Z. Yang","OTHERS"
143,"test_mkldnn.py have some Linux specific checks in default codepath","See 
https://github.com/pytorch/pytorch/blob/095c328d9fb86c348fbd100c3ee995a0698ac9a9/test/test_mkldnn.py#L30-L37

Which causes following nice error messages when running for example on Mac:
```
Apr 30 15:25:29   test_max_pool2d_bf16 (__main__.TestMkldnn) ... grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:29 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
Apr 30 15:25:30 grep: /proc/cpuinfo: No such file or directory
```

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-04-30","2021-05-03","3","low priority",NULL,"Nikita Shulga","imaginary-person","OTHERS"
144,"Avoid unnecessary creation of tensors in SparseCsrTensorImpl::resize_and_clear_","As in title. The issue raised in https://github.com/pytorch/pytorch/pull/50937#discussion_r603536011

Related code:
https://github.com/pytorch/pytorch/blob/76214bb4641fd75fe4ab0e87b05f90462de85fdc/aten/src/ATen/SparseCsrTensorImpl.cpp#L64-L70


cc @aocsa @nikitaved @pearu @mruberry","2021-04-22","2021-05-31","39","priorized",NULL,"Pearu Peterson","aocsa","PROJECT PATTERNS"
145,"Use `rg` for quick checks","The quick checks in `lint.yml` use `git grep` to search for things like newlines, trailing spaces, etc. 

https://github.com/pytorch/pytorch/blob/a4626348bc85b6484d647ceb07298b3516e2a7b3/.github/workflows/lint.yml#L48

We should change this to install [rg](https://github.com/BurntSushi/ripgrep) via `sudo apt install -y ripgrep` and use that instead. It's ever so slightly more ergonomic to use and provides about a 3x speedup over `git grep`

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-04-20","2021-08-02","104","priorized",NULL,"driazati","driazati","OTHERS"
146,"Re-order tests based on files in the PR","We already have some (unused) logic to bring certain tests to the front of the list in `run_test.py`:

https://github.com/pytorch/pytorch/blob/a4626348bc85b6484d647ceb07298b3516e2a7b3/test/run_test.py#L829-L831

We should use something like this to re-order tests based on the PR in order to decrease testing turn-around time. A simplistic version could query git for changed test files and bring those to the front. Chances are if someone edited a test file and there is a broken test on their PR, it is probably going to be in that file. By running it first, CI will terminate / deliver signal to the developer faster. This could be way more granular / smarter (i.e. use code coverage data to determine which code affects which tests rather than just looking at file names), but we could still probably get CI runtime wins by starting with the simple version.

I'm imagining some code in `run_test.py` that looks for CircleCI's environment variables for the base / head SHA, `git diff --name-only`'s them, and prioritizes the intersection of the passed in test list + the test files from the diff.

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-04-20","2022-03-15","329","priorized",NULL,"driazati","janeyx99","SOFTWARE ARCHITECTURE"
147,"[BE] optimize hard-coded branch name in util scripts that might not work on release branch","There are a bunch of hard-coded master branch name in utility scripts used for CI. 

Here are the ones that potentially will be used in CI
* [.jenkins/pytorch/common_utils.sh](https://github.com/pytorch/pytorch/blob/a4626348bc85b6484d647ceb07298b3516e2a7b3/.jenkins/pytorch/common_utils.sh#L52-L58)
* [test/run_test.py](https://github.com/pytorch/pytorch/blob/a4626348bc85b6484d647ceb07298b3516e2a7b3/test/run_test.py#L411-L413)
* [tools/export_slow_tests.py](https://github.com/pytorch/pytorch/blob/a4626348bc85b6484d647ceb07298b3516e2a7b3/tools/export_slow_tests.py#L16)

These might be broken if run on release branch.



cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-04-20","2022-03-04","318","priorized",NULL,"Rong Rong","seemethere","OTHERS"
148,"Remove distutils","Distutils is [deprecated via PEP 632](https://www.python.org/dev/peps/pep-0632/#:~:text=In%20Python%203.10%20and%203.11,platforms%20will%20not%20be%20added.) in Python 3.10+ and will be removed in 3.12. There's no reason for us to keep using it, so we should get rid of all usages so it doesn't bite us later when 3.10 rolls around.

Usages:

```
$ ag --ignore third_party distutils
caffe2/CMakeLists.txt
1707:      from distutils import sysconfig
1719:      from distutils import sysconfig

cmake/Dependencies.cmake
938:    # distutils.sysconfig, if it's installed, is more accurate than sysconfig,
940:    pycmd_no_exit(_py_inc _exitcode ""from distutils import sysconfig; print(sysconfig.get_python_inc())"")
943:      message(STATUS ""Setting Python's include dir to ${_py_inc} from distutils.sysconfig"")

scripts/build_android.sh
64:  CMAKE_ARGS+=(""-DCMAKE_PREFIX_PATH=$($PYTHON -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')"")

scripts/build_mobile.sh
18:CMAKE_ARGS+=(""-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')"")

scripts/get_python_cmake_flags.py
18:from distutils import sysconfig

scripts/build_ios.sh
16:  CMAKE_ARGS+=(""-DCMAKE_PREFIX_PATH=$(python -c 'from distutils.sysconfig import get_python_lib; print(get_python_lib())')"")

test/test_spectral_ops.py
16:from distutils.version import LooseVersion

tools/setup_helpers/cmake.py
10:import distutils.sysconfig
11:from distutils.version import LooseVersion
233:            'CMAKE_PREFIX_PATH': distutils.sysconfig.get_python_lib()
319:                      PYTHON_INCLUDE_DIR=distutils.sysconfig.get_python_inc(),

tools/build_pytorch_libs.py
10:    from distutils._msvccompiler import _get_vc_env

tools/generate_torch_version.py
5:from distutils.util import strtobool

torch/package/_stdlib.py
79:    ""distutils"",
298:    ""distutils"",
516:    ""distutils"",
732:    ""distutils"",

torch/testing/_internal/common_cuda.py
9:from distutils.version import LooseVersion

torch/testing/_internal/common_methods_invocations.py
37:from distutils.version import LooseVersion

torch/utils/tensorboard/__init__.py
2:from distutils.version import LooseVersion

torch/utils/cpp_extension.py
338:    Fallbacks to the standard distutils backend if Ninja is not available.
368:                   '{}. Falling back to using the slow distutils backend.')
479:            # of distutils.UnixCCompiler). See the following link.
480:            # https://github.com/python/cpython/blob/f03a8f8d5001963ad5b5b28dbd95497e9cc15596/Lib/distutils/ccompiler.py#L564-L567
484:            # we update our python version (which is when distutils can be
697:        # https://github.com/python/cpython/blob/dc0284ee8f7a270b6005467f26d8e5773d76e959/Lib/distutils/ccompiler.py#L511
737:        if IS_WINDOWS and 'VSCMD_ARG_TGT_ARCH' in os.environ and 'DISTUTILS_USE_SDK' not in os.environ:
738:            msg = ('It seems that the VC environment is activated but DISTUTILS_USE_SDK is not set.'
740:                   'Please set `DISTUTILS_USE_SDK=1` and try again.')
1638:        from distutils.util import get_platform
1639:        from distutils._msvccompiler import _get_vc_env

README.md
316:set DISTUTILS_USE_SDK=1

mypy.ini
190:[mypy-distutils.*]

setup.py
24:#     the C/C++ compiler to use (NB: the CXX flag has no effect for distutils
25:#     compiles, because distutils always uses CC to compile, even for C++
200:from distutils import core
201:from distutils.core import Distribution
202:from distutils.errors import DistutilsArgError
205:import distutils.command.clean
206:import distutils.command.sdist
207:import distutils.sysconfig
270:rel_site_packages = distutils.sysconfig.get_python_lib(prefix='')
272:full_site_packages = distutils.sysconfig.get_python_lib()
276:        distutils.sysconfig.get_config_var(""prefix""),
277:        distutils.sysconfig.get_config_var(""VERSION""))
283:            distutils.sysconfig.get_config_var(""VERSION""))
286:        distutils.sysconfig.get_config_var(""LIBDIR""),
287:        distutils.sysconfig.get_config_var(""INSTSONAME""))
288:cmake_python_include_dir = distutils.sysconfig.get_python_inc()
494:        system_c_flags = str(distutils.sysconfig.get_config_var('CFLAGS'))
496:            os.environ['CC'] = str(distutils.sysconfig.get_config_var('CC'))
562:        distutils.command.build_ext.build_ext.build_extensions(self)
565:        outputs = distutils.command.build_ext.build_ext.get_outputs(self)
647:class clean(distutils.command.clean.clean):
671:class sdist(distutils.command.sdist.sdist):
870:    except DistutilsArgError as msg:
```

cc @malfet @seemethere @walterddr @pytorch/pytorch-dev-infra ","2021-04-20","2021-04-29","9","priorized",NULL,"driazati","driazati","OTHERS"
149,"Simplify error message for failed int comparison","If you use assertEqual to compare two integers you get a message like

```
AssertionError: False is not true : Scalars failed to compare as equal! Comparing -6 and 0 gives a difference of 6, but the allowed difference with rtol=0 and atol=0 is only 0!
```

The stuff about rtol and atol is is needlessly obscure and obscures the fact that, e.g., the integers in question are exit codes, for example. One easy fix is to suppress rtol/atol sentence if those are exactly zero; I'd also suggest we shouldn't report ""difference"" for integer comparison.","2021-04-19","2021-06-29","71","priorized",NULL,"Edward Z. Yang","Edward Z. Yang","PROJECT PATTERNS"
150,"Compile PyTorch core with -Werror","PyTorch codebase should be reasonable warning free, which is not the case for now.
This is something that is very hard to achieve in one change, so perhaps a gradual approach can be taken:

- [ ] Add all source files to libtorch_compiler_with_Wnoerror_flag
- [ ] Modify CMake system to add -Werror to the CFLAGS unless file is in the list
- [ ] Modify build system to ignore warnings in 3rd party libraries
- [ ] Update default Wno-lists
This would allow us to stabilize the situation and eventually remove files from this list one by one

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra","2021-04-14","2022-02-28","320","priorized",NULL,"Nikita Shulga","seemethere","PROJECT PATTERNS"
151,"C++17 for PyTorch","We are planning to migrate the PyTorch codebase to C++17, but are currently blocked by CUDA. This issue summarizes a discussion I had with @malfet and @ngimel about this.

CUDA 11 is the first CUDA version to support C++17, so we'd have to drop support for CUDA 10, but there are good reasons for us to keep CUDA 10 around still:
1. A PyTorch build with CUDA 10 takes around 800MB, while a PyTorch build with CUDA 11 takes 1.9GB. This is a lot of data to put in a wheel. The size difference is both due to CUDA 11 libraries generally being larger, and also due to CUDA 11 wheels supporting 2 additional architectures, sm_80 and sm_86.
2. PyTorch wheels and conda packages with CUDA 11 are slower than CUDA 10 for the architectures they both support (sm_75) because [fast kernels are dropped due to static linking](https://github.com/pytorch/pytorch/issues/47908). Additionally, our CUDA 11 wheels are slower than dynamically linked CUDA 11 builds on Ampere (sm86) because of the dropped kernels, but that‚Äôs a separate issue that needs to be solved. 


These issues would have to be fixed in CUDA, there's not much we can do on the PyTorch side.

#### Workarounds we considered (kudos to @malfet and @ngimel for the ideas)
- Only build .cpp files with C++17 while building .cu files in C++14 mode.
  - Since .cu files includes a lot of our core headers, this means we'd still be restricted to C++14 in many parts of the codebase. Furthermore, if you accidentally use a C++17 feature right now and do a local CPU build, in the current world you get a local compiler error while in the dual C++14/C++17 world, it would only fail on CI. This would decrease developer efficiency and productivity.
- Stay with CUDA 10 but invoke nvcc with `-std=c++14 '-Xcompiler -std=c++17'`
  - This could work for some C++17 features, namely ones that don't change C++ syntax, but C++17 does introduce a couple of features that change the C++ syntax. This would likely trip up nvcc when it's trying to parse the C++ code. Even if it worked today, this could be a fragile setup and break in the future.
- Use a Frankenstein CUDA where most components are from CUDA 10, but nvcc and cudafe are from CUDA 11.
  - This would add a significant maintenance burden. Also, it would likely only work in Facebook where we fully control the build. It would leave people behind that build in open source with the public available CUDA distribution, unless they use the Frankenstein toolkit too, and we don't want to be in the business of distributing that. And leaving oss users behind is not an option.
- Split the wheel
  - Instead of distributing PyTorch in one wheel, we could have users download two wheels, e.g. one for PyTorch and one for CUDA, or maybe one for PyTorch CPU and one for PyTorch CUDA kernels + CUDA library. This could alleviate the binary size problem for some users, namely those who don't use CUDA, but GPU users would still have to download both. Distributing CUDA separately could work well for conda, but could pose issues with pip.
- Link CUDA dynamically
  - At least some of the perf issues in CUDA 11 seem to be due to cuDNN not using fast kernels if linked statically. By linking dynamically, we could fix those perf problems. However, that would make the binary size problem even worse since dynamically linking CUDA would prevent the linker from stripping away unused parts of it. Also, we wouldn't be able to link it with `-fvisibility=hidden`, so it could cause symbol conflicts with other libraries.
- Change the way we integrate CPU/CUDA, i.e. stop using triple chevrons `cuda_kernel<<<1, 1>>>(arguments, argument, argument)`
  - If we migrate our CUDA code away from triple chevrons towards `cuLaunchKernel`, then we would be able to avoid nvcc for code outside of CUDA kernels, which would allow us to write it in C++17. However, this would be a nontrivial engineering effort. Also, kineto isn't correctly recording kernels launched with `cuLaunchKernel`, so that would have to be fixed too.

cc @malfet @seemethere @walterddr @ngimel","2021-04-14","2022-12-07","602","priorized",NULL,"Sebastian Messmer","rgommers","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
152,"Preserve PyObject even when it is dead from Python side","In https://github.com/pytorch/pytorch/issues/22884 I observed that if the Python object representing a Tensor gets deallocated, it promptly goes dead and we create a new Python object to replace it next time it passes from C++ to Python. This is problematic for tensors that represent Python subclasses and/or have custom `__dict__` entries, because all of that information solely lives in Python and is lost when this occurs.

I have devised a scheme to solve this problem, based off of @colesbury's POC for keeping two PyObjects live so long as one of them lives https://github.com/colesbury/refcount/blob/master/refcount.c The key idea is that it is possible to ""rescue"" a Python object from being deallocated, even if its refcount goes to zero (this is easy to do safely in Python because of the GIL). So here's the idea:

* Ordinarily, PyObject holds an owning reference to TensorImpl
* When the PyObject would die, we now check if the TensorImpl is uniquely owned by the PyObject's owning reference. If so, the TensorImpl is dead too and we can destruct everything as normal. HOWEVER, if the TensorImpl is still live, we now flip ownership: we toggle a boolean on TensorImpl indicating that it has an owning reference to the PyObject, and we bump the PyObject's reference count to make sure it doesn't go dead. Call this a **zombie PyObject**
* If a TensorImpl dies while holding an owning reference to a PyObject, it is responsible for also deallocating the PyObject. This will be done using a virtual call in to the libpython.
* If we try to take an owning reference to a zombie PyObject, we flip the ownership back again. So we toggle the TensorImpl to be non-owning again, skip the Python incref and directly return the PyObject (no longer zombie)

The key invariant of this scheme is that it is not necessary to have resurrect a zombie TensorImpl; the invariant is that a TensorImpl only ever goes dead if there are truly no references (no direct references to the TensorImpl, and no owning reference from a non-zombie PyObject).

To make testing out this scheme less risky, we can opt into this scheme only for subclasses of Tensor. However, I think that if there is no major performance impact we should eventually turn this on globally.","2021-04-09","2021-06-03","55","priorized",NULL,"Edward Z. Yang","albanD","OTHERS"
153,"Misleading/Inconsistent Exceptions Raised by `nn.ModuleList.forward`","## üêõ Bug

`nn.ModuleList` does not have meaningful semantics for `forward`, yet calling forward does not raise informative exceptions. I have seen this be confusing for many students in a course we are teaching. For a call to `forward` with non-empty arguments, instead of the apparently intended `NotImplementedException` or, alternatively, the default Python `AttributeError` (""ModuleList object has no attribute 'forward'"") that one might expect, we instead get a `TypeError: forward() takes 1 positional argument but <n> were given`.


## To Reproduce

Steps to reproduce the behavior:

1. `nn.ModuleList().forward()` # results in expected `NotImplementedException` being raised
2. `nn.ModuleList().forward(torch.zeros(1))` # results in `TypeError: forward() takes 1 positional argument but 2 were given`

## Expected behavior

`nn.ModuleList().forward(*args)` should raise `NotImplementedException` for any argument list to make it clear that `forward` is unsupported/unimplemented functionality for `nn.ModuleList` (and not merely a call with wrong arguments).

The issues is in the current implementation of the `forward` method, which raises `NotImplementedException` but takes no arguments (except `self`), resulting in the above issues.

## Environment
The issue is independent of environment/runtime. It is present in current master and a quick look suggests that it is present at least since version 1.4.
","2021-04-07","2021-04-21","14","priorized",NULL,"Lukas Prediger","jbschlosser","OTHERS"
154,"c10d: GlooStore and Store APIs are inconsistent","## GlooStore and Store APIs are inconsistent

ProcessGroupGloo wraps Store in a GlooStore, which has the following wait API signature (sets a `std::vector<char>`):
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/ProcessGroupGloo.cpp#L175

However, Store has the following signature (sets a `std::vector<uint8_t>`):
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/Store.hpp#L29

This results in having to write different code when using GlooStore/Store, but ideally user wouldn't even need to be aware of the underlying store impl and the code would be simplified if both had the same interface. We would also be able to remove some code that is responsible for converting `std::vector<char>` -> `std::vector<uint8_t>` and vice versa.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu","2021-04-05","2021-04-29","24","priorized",NULL,"Rohan Varma","rohan-varma","PROJECT PATTERNS"
155,"Move distributed tests to single CI configuration","## üöÄ Feature
Move all the distributed tests (tests in the `distributed` folder onto a single configuration on CI.

## Motivation
The distributed tests are currently running on many different CI configurations (basically in all the ones with the name `-cuda-` in it). This does not have to be so. By moving them into one configuration, we can reduce CI cost considerably.

## Additional context

The way to do this would be to tinker around with the files in the .circleci folder. This change may be similar to the special configurations we have for noarch_tests or slow_tests.

cc @ezyang @seemethere @malfet @walterddr @pytorch/pytorch-dev-infra @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu","2021-04-01","2022-03-02","335","priorized",NULL,"Jane (Yuan) Xu","janeyx99","OTHERS"
156,"Use dispatcher to split up quantized operations","There are a bunch of operators which are defined like this:

```
std::tuple<Tensor, Tensor> max(const Tensor& self, int64_t dim, bool keepdim) {
  Tensor max_indices = at::empty({0}, self.options().dtype(kLong));
  if (self.is_quantized()) {
    Tensor max = at::empty({0}, self.options().dtype(toUnderlying(self.scalar_type())));
    at::native::max_out(max, max_indices, self.int_repr(), dim, keepdim);
    // TODO: qscheme
    return std::tuple<Tensor, Tensor>(at::_make_per_tensor_quantized_tensor(max, self.q_scale(), self.q_zero_point()), max_indices);
  } else {
    Tensor max = at::empty({0}, self.options());
    return at::native::max_out(max, max_indices, self, dim, keepdim);
  }   
}   
```

It would be better for the quantized portion of the conditional to be put in its own kernel, and then have it dispatched via QuantizedCPU.

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a @vkuzo","2021-03-17","2022-02-25","345","priorized",NULL,"Edward Z. Yang","dzdang","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
157,"Add tests for store.wait()","There are no test cases for [store.wait()](https://pytorch.org/docs/master/distributed.html#torch.distributed.Store.wait). We should add them for TCPStore, FileStore, and HashStore in https://github.com/pytorch/pytorch/blob/master/test/distributed/test_store.py

## How to get started

0.1. (optional) get conda and create conda env
1. install dependences - https://github.com/pytorch/pytorch#install-dependencies
2. get the source - https://github.com/pytorch/pytorch#get-the-pytorch-source
3. install - https://github.com/pytorch/pytorch#install-pytorch
4. Once torch is installed with no errors. Test `python -c ""import torch; print(torch.__version__)""` to see you have the correct version, it should look something like: 2.0.0a0+git54020ba
5. Create your own python script and try to call the APIs you are testing. Note if you make only python changes to pytorch, you do NOT need to rebuild.

## How to run a test

1. `pip install pytest`
2. Use this test as an example `test_address_already_in_use` (https://github.com/pytorch/pytorch/blob/master/test/distributed/test_store.py#L205)
3. `pytest test/distributed/test_store.py -vsk test_address_already_in_use`
4. Check out https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md#running-unit-tests for more details

cc @gravityknife

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @SciPioneer @H-Huang @kwen2501 @agolynski @mrzzd @cbalioglu","2021-03-11","2023-05-01","781","priorized",NULL,"Howard Huang","Howard Huang","OTHERS"
158,"tensorinv and other linear algebra operators should not do a universal catch all block","In the implementation of tensorinv we have:

```
  // If the reshaped self is not invertible catch this error
  Tensor result;
  try {
    result = at::inverse(self.reshape({prod_ind_end, prod_ind_end}));
  } catch (...) {
    TORCH_CHECK(false, ""Failed to invert the input tensor, because it is singular."");
  }
```

This is bad code practice. For example, the inner operation could raise an exception because it ran out of memory, and you would still report it as singular. More concretely, this resulted in wrong behavior at https://github.com/pytorch/pytorch/pull/53682/files/29f176c952d5dd5614c933398fe3ed92de690a9e#diff-7e17421f32124016eb8de04dc2f445da5786a28355e1addc72b305466f590180 We shouldn't do a catch all. Ideally, there would be some interface to at::inverse that would let it explicitly signal, e.g., via boolean, that input was singular.

Related https://github.com/pytorch/pytorch/issues/47608

cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr @IvanYashchuk @glaringlee ","2021-03-10","2021-08-18","161","priorized",NULL,"Edward Z. Yang","IvanYashchuk","OTHERS"
159,"Remove silly ""Buy new RAM!"" sentence in error message when out of CPU memory","See 

https://github.com/pytorch/pytorch/blob/f1f9b049d83fc20256a244d62b2eca6548a02fd5/c10/core/CPUAllocator.cpp#L75

The phrase ""Buy new RAM!"" is silly. It's not even good advice (wouldn't it be ""Buy more RAM!""?). And purchasing memory is not always an option for PyTorch users. We should just remove this unhelpful and possibly offensive phrase.","2021-03-08","2021-03-09","1","priorized",NULL,"Mike Ruberry","mthrok","OTHERS"
160,"Make not implemented autograd formulas raise NotImplementedError","Make the codegen raise nice NotImplementedError for formulas that are not implemented by leveraging work done in https://github.com/pytorch/pytorch/pull/53377

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer","2021-03-05","2021-06-08","95","priorized",NULL,"albanD","albanD","OTHERS"
161,"Stop using legacy torch.Tensor constructor","There are still a number of torch.Tensor call sites in our codebase:

```
torch/_torch_docs.py:    >>> torch.nextafter(torch.Tensor([1, 2]), torch.Tensor([2, 1])) == torch.Tensor([eps + 1, 2 - eps])
torch/distributions/kumaraswamy.py:        >>> m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))
torch/linalg/__init__.py:                           pass `(torch.Tensor(), out_S, torch.Tensor())`
torch/nn/intrinsic/qat/modules/conv_fused.py:            self.bias = Parameter(torch.Tensor(out_channels))
torch/nn/modules/activation.py:            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
torch/nn/modules/activation.py:            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
torch/nn/modules/activation.py:            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
torch/nn/modules/activation.py:        self.weight = Parameter(torch.Tensor(num_parameters).fill_(init))
torch/nn/modules/batchnorm.py:            self.weight = Parameter(torch.Tensor(num_features))
torch/nn/modules/batchnorm.py:            self.bias = Parameter(torch.Tensor(num_features))
torch/nn/modules/conv.py:            self.weight = Parameter(torch.Tensor(
torch/nn/modules/conv.py:            self.weight = Parameter(torch.Tensor(
torch/nn/modules/conv.py:            self.bias = Parameter(torch.Tensor(out_channels))
torch/nn/modules/linear.py:        self.weight = Parameter(torch.Tensor(out_features, in1_features, in2_features))
torch/nn/modules/linear.py:            self.bias = Parameter(torch.Tensor(out_features))
torch/nn/modules/normalization.py:            self.weight = Parameter(torch.Tensor(*self.normalized_shape))
torch/nn/modules/normalization.py:            self.bias = Parameter(torch.Tensor(*self.normalized_shape))
torch/nn/modules/normalization.py:            self.weight = Parameter(torch.Tensor(num_channels))
torch/nn/modules/normalization.py:            self.bias = Parameter(torch.Tensor(num_channels))
torch/nn/modules/rnn.py:                w_ih = Parameter(torch.Tensor(gate_size, layer_input_size))
torch/nn/modules/rnn.py:                w_hh = Parameter(torch.Tensor(gate_size, real_hidden_size))
torch/nn/modules/rnn.py:                b_ih = Parameter(torch.Tensor(gate_size))
torch/nn/modules/rnn.py:                b_hh = Parameter(torch.Tensor(gate_size))
torch/nn/modules/rnn.py:                    w_hr = Parameter(torch.Tensor(proj_size, hidden_size))
torch/nn/modules/rnn.py:        self.weight_ih = Parameter(torch.Tensor(num_chunks * hidden_size, input_size))
torch/nn/modules/rnn.py:        self.weight_hh = Parameter(torch.Tensor(num_chunks * hidden_size, hidden_size))
torch/nn/modules/rnn.py:            self.bias_ih = Parameter(torch.Tensor(num_chunks * hidden_size))
torch/nn/modules/rnn.py:            self.bias_hh = Parameter(torch.Tensor(num_chunks * hidden_size))
torch/nn/modules/sparse.py:            self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))
torch/nn/modules/sparse.py:            self.weight = Parameter(torch.Tensor(num_embeddings, embedding_dim))
torch/nn/parameter.py:            data = torch.Tensor()
torch/nn/parameter.py:        data = torch.Tensor()
torch/nn/parameter.py:        data = torch.Tensor()
torch/nn/utils/prune.py:                nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])
torch/onnx/symbolic_opset9.py:    return g.op('Div', log(g, self), g.op('Constant', value_t=torch.Tensor([_ln2])))
torch/quantization/fx/quantize.py:                torch.Tensor(input_quantized_idxs)
torch/quantization/fx/quantize.py:            model._standalone_module_output_quantized_idxs = torch.Tensor(output_quantized_idxs)
torch/quantization/ns/numeric_suite_core_apis_fx.py:#       'values': [torch.Tensor(...), ...],
torch/testing/_internal/common_nn.py:    t = Variable(torch.Tensor(15).uniform_().mul(10).floor().long())
torch/testing/_internal/common_nn.py:    t = Variable(torch.Tensor(15).uniform_().mul(10).floor().long())
torch/testing/_internal/common_nn.py:    t = Variable(torch.Tensor(15).uniform_().mul(10).floor().long())
torch/testing/_internal/common_nn.py:    t = Variable(torch.Tensor(15).uniform_().mul(10).floor().long())
torch/testing/_internal/common_nn.py:    t = Variable(torch.Tensor(15).uniform_().mul(10).floor().long())
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10 + 1).floor().long() - 1,
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/common_nn.py:        target_fn=lambda: torch.Tensor(15).uniform_().mul(10).floor().long(),
torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py:            _weight=torch.Tensor([init_em] * num_embeddings),
torch/testing/_internal/distributed/ddp_under_dist_autograd_test.py:                    training_examples.dense_features[idx, :] = torch.Tensor((x, y))
```

These should all get updated to use `torch.empty` or `torch.tensor` as appropriate. This is a simple bug good for beginner.

cc @gchanan @mruberry","2021-03-02","2021-03-22","20","priorized",NULL,"Edward Z. Yang","ysiraichi","OTHERS"
162,"Add compare_set for HashStore and FileStore","### Improvement
Add functionality for `compare_set` to [HashStore](https://pytorch.org/docs/master/distributed.html#torch.distributed.FileStore) and [FileStore](https://pytorch.org/docs/master/distributed.html#torch.distributed.FileStore) to have achieve parity with TCPStore.

### Background
We [recently added a method to TCPStore](https://github.com/pytorch/pytorch/commit/97e35858ece696a0627b58c5945eed98bd47a41a) for `compare_set(key, current_value, new_value)`. The logic for it is as follows:

if key doesn't exist: return current_value
if get(key) == current_value: update key to new_value and return new_value
if get(key) != current_value: do not update key, return get(key)

This should also be added to the other two store types, HashStore and FileStore.

**Implementation caveat**: Since `store` is used by multiple processes, implementing compareSet by internally calling `get` followed by `set` will not suffice due to potential race conditions. It is fine to use the logic of the two functions, but they must be combined so that compareSet is an atomic operation.

Pybind definition:
https://github.com/pytorch/pytorch/blob/b3bf08e67f5cf29cbfd197e3c9e9a2f45e0b5c9d/torch/csrc/distributed/c10d/init.cpp#L424-L461

Relevant files:
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/HashStore.cpp
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/FileStore.cpp
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/TCPStore.cpp

### Testing

Testing should be done for both python and C++ APIs

#### Python:
https://github.com/pytorch/pytorch/blob/master/test/distributed/test_c10d.py

Run with `python test/distributed/test_c10d.py TCPStoreTest.test_compare_set`(it should be under HashStoreTest and FileStoreTest)

#### C++:
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/test/HashStoreTest.cpp
https://github.com/pytorch/pytorch/blob/master/torch/lib/c10d/test/FileStoreTest.cpp

Run with `build/bin/HashStoreTest` and `build/bin/FileStoreTest`","2021-03-01","2021-03-12","11","priorized",NULL,"Howard Huang","tmliew","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
163,"Replace deprecated AT_ERROR with `TORCH_CHECK(false,`  in `c10`","According to 
https://github.com/pytorch/pytorch/blob/64847c7f0b3559c6edc40f001619b80c7dc68ef7/c10/util/Exception.h#L479
all usages of AT_ERROR(""msg"") should be replace with `TORCH_CHECK(false, ""msg"")`
There are currently 29 instances of AT_ERROR being used in c10 codebase:
```
$ grep AT_ERROR c10 -R|grep -v Exception.h|wc -l
29
```","2021-02-23","2021-03-22","27","priorized",NULL,"Nikita Shulga","mruberry","OTHERS"
164,"RPC hangs in callback when there is an error","## üêõ Bug

When an RPC future is completed with an error, `fut.wait()` should throw the error on the client so that the user can handle the exception.

Although, this doesn't appear to work correctly when `fut.wait()` on an RPC that has an error is called with an RPC callback. This can lead to a hang in the callback, which can cause, among other things, subsequent chained futures to not be completed. 

What is very strange is that there is a workaround: wrapping the call to `fut.wait()` in the cb with a try/catch. This is because I'm assuming that `fut.wait()` throws as expected and then we handle the exception. Although, the exception should be thrown even if we don't handle it as I understand.

## To Reproduce

Patch following test:
```
@dist_init
    def test_rpc_err_in_cb(self):
        if self.rank != 0:
            return
        # this will cause the RPC to be marked with an error due to timeout. 
        fut = rpc.rpc_async(worker_name(self.rank + 1), slow_add, args=(torch.tensor(1), torch.tensor(1)), timeout=0.01)
        # fut.wait()
        done = False
        def cb(fut):
            print(f""Fut result is {fut.wait()}"")
            nonlocal done
            done = True

        fut.add_done_callback(cb)
        while not done:
            pass
```

## Expected behavior

Ideally `fut.wait()` should throw the error in cb and we should get a crash, as we do when we don't await in cb.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @anjali411 @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @mrzzd @agolynski @SciPioneer @H-Huang @cbalioglu","2021-02-11","2021-03-31","48","high priority",NULL,"Rohan Varma","rohan-varma","OTHERS"
165,"`ATen/cpu/vec256/` should be a header only library for all but quantized types","Consider the following example:
```
#include <iostream>
#include <ATen/cpu/vec256/vec256.h>
int main(void) {
  std::cout << ""Hello World"" << std::endl;
}
```
First of all, this program couldn't be compiled without linking to c10:
```
$ g++ -Iaten/src/ -I. -Ibuild -Ibuild/aten/src  vec256_test.cpp
/usr/bin/ld: /tmp/cc0F69nh.o: in function `c10::UndefinedTensorImpl::singleton()':
vec256_test.cpp:(.text._ZN3c1019UndefinedTensorImpl9singletonEv[_ZN3c1019UndefinedTensorImpl9singletonEv]+0x5): undefined reference to `c10::UndefinedTensorImpl::_singleton'
/usr/bin/ld: /tmp/cc0F69nh.o: in function `c10::IValue::toDouble() const':
vec256_test.cpp:(.text._ZNK3c106IValue8toDoubleEv[_ZNK3c106IValue8toDoubleEv]+0x85): undefined reference to `c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/usr/bin/ld: /tmp/cc0F69nh.o: in function `c10::IValue::toInt() const':
vec256_test.cpp:(.text._ZNK3c106IValue5toIntEv[_ZNK3c106IValue5toIntEv]+0x85): undefined reference to `c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/usr/bin/ld: /tmp/cc0F69nh.o: in function `c10::IValue::toTensor() const &':
vec256_test.cpp:(.text._ZNKR3c106IValue8toTensorEv[_ZNKR3c106IValue8toTensorEv]+0xbf): undefined reference to `c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/usr/bin/ld: /tmp/cc0F69nh.o: in function `c10::intrusive_ptr<c10::TensorImpl, c10::UndefinedTensorImpl>::retain_()':
vec256_test.cpp:(.text._ZN3c1013intrusive_ptrINS_10TensorImplENS_19UndefinedTensorImplEE7retain_Ev[_ZN3c1013intrusive_ptrINS_10TensorImplENS_19UndefinedTensorImplEE7retain_Ev]+0xb6): undefined reference to `c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
collect2: error: ld returned 1 exit status
```
And when linked with c10 produces a 182Kb executable, compared to 24Kb executable without `#include <vec256.h>`","2021-01-15","2021-01-25","10","priorized",NULL,"Nikita Shulga","albanD","OTHERS"
166,"torch.nn.parallel.scatter_gather.gather cannot handle namedtuple as output","## üêõ Bug

`torch.nn.parallel.scatter_gather.gather` can't gather outputs of type `namedtuple`.

## To Reproduce

Steps to reproduce the behavior:

```
import torch
import collections

MyTuple = collections.namedtuple('MyTuple', ['a', 'b', 'c'])
out1 = MyTuple(torch.tensor(1), torch.tensor(2), torch.tensor(3))
out2 = MyTuple(torch.tensor(4), torch.tensor(5), torch.tensor(6))
outputs = [out1, out2]

from torch.nn.parallel.scatter_gather import gather
gather(outputs, 0)
```

Error message:
Copied from execution on Google Colab.
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-139-9cc42055abf8> in <module>()
      8 
      9 from torch.nn.parallel.scatter_gather import gather
---> 10 gather(outputs, 0)

1 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/scatter_gather.py in gather_map(outputs)
     61             return type(out)(((k, gather_map([d[k] for d in outputs]))
     62                               for k in out))
---> 63         return type(out)(map(gather_map, zip(*outputs)))
     64 
     65     # Recursive function calls like this create reference cycles.

TypeError: __new__() missing 2 required positional arguments: 'b' and 'c'
```

## Expected behavior

A successful gather should return an object of type `MyTuple` with the following values:
```
MyTuple(a=tensor([1, 4]), b=tensor([2, 5]), c=tensor([3, 6]))
```

## Environment
I tested on my server as well as google colab.

Google Colab:
```
Collecting environment information...
PyTorch version: 1.7.0+cu101
Is debug build: True
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: 6.0.0-1ubuntu2 (tags/RELEASE_600/final)
CMake version: version 3.12.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: 10.1.243
GPU models and configuration: Could not collect
Nvidia driver version: Could not collect
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.5
[pip3] torch==1.7.0+cu101
[pip3] torchsummary==1.5.1
[pip3] torchtext==0.3.1
[pip3] torchvision==0.8.1+cu101
[conda] Could not collect
```

My server:
```
Collecting environment information...
PyTorch version: 1.8.0.dev20210113+cu101
Is debug build: False
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.4 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.10.2

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 10.0.130
GPU models and configuration:
GPU 0: Quadro RTX 5000
GPU 1: Quadro RTX 5000
GPU 2: Quadro RTX 5000
GPU 3: Quadro RTX 5000
GPU 4: Quadro RTX 5000
GPU 5: Quadro RTX 5000
GPU 6: Quadro RTX 5000
GPU 7: Quadro RTX 5000

Nvidia driver version: 440.44
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.8.0.dev20210113+cu101
[pip3] torchaudio==0.8.0.dev20210113
[pip3] torchvision==0.9.0.dev20210113+cu101
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2020.2                      256
[conda] mkl-service               2.3.0            py38he904b0f_0
[conda] mkl_fft                   1.2.0            py38h23d657b_0
[conda] mkl_random                1.1.1            py38h0573a6f_0
[conda] numpy                     1.18.5                   pypi_0    pypi
[conda] torch                     1.8.0.dev20210113+cu101          pypi_0    pypi
[conda] torchaudio                0.8.0.dev20210113          pypi_0    pypi
[conda] torchvision               0.9.0.dev20210113+cu101          pypi_0    pypi
```

## Additional context

I think it's related to [#41327](https://github.com/pytorch/pytorch/issues/41327), but probably more generai as `namedtuple` is buit-in python class? I personally feel that since `scatter_gather.py` already has the `is_namedtuple` function, this should be relatively easy to fix. For example, just add something like below (not thoroughly tested though):
```
def gather_map(outputs):
    out = outputs[0]
    if isinstance(out, torch.Tensor):
        return Gather.apply(target_device, dim, *outputs)
    if out is None:
        return None
    if isinstance(out, dict):
        if not all((len(out) == len(d) for d in outputs)):
            raise ValueError('All dicts must have the same number of keys')
        return type(out)(((k, gather_map([d[k] for d in outputs]))
                          for k in out))
    if is_namedtuple(out):
      return type(out)._make(map(gather_map, zip(*outputs)))
    return type(out)(map(gather_map, zip(*outputs)))
```

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd","2021-01-13","2021-02-11","29","low priority",NULL,"Mingda Zhang","adizhol","OTHERS"
167,"[BE] found some more tests still using unittest.main()","## üöÄ Feature
This is a follow up on #50448, some more python tests are still using unittest.main()

Not sure if they are intended since they are not using common_utils but here is the list.
test/custom_backend/*
test/custom_operator/*
test/mobile/*
test/onnx/*
modules/detectron/*
scripts/release_notes

in addition there are many more tests in /caffe2/ folder which I assume we will not address at the moment.


cc @mruberry @VitalyFedyunin @walterddr","2021-01-13","2021-01-25","12","priorized",NULL,"Rong Rong","walterddr","SOFTWARE ARCHITECTURE"
168,"Some test modules are using unittest.main() instead of common_utils.run_tests()","## üöÄ Feature

Some test modules are using `unittest.main()` instead of `torch.testing._internal.common_utils.run_tests()`. Thus, the arg parser in `common_utils` are not running for these tests, which makes the whole test suite somewhat inconsistent.

https://github.com/pytorch/pytorch/blob/71766d89ea55bb8d4e180bac1c21bd95a00d54d5/torch/testing/_internal/common_utils.py#L149-L162

```
test_expecttest
test_mobile_optimizer
test_show_pickle
test_namedtuple_return_api 
test_jit_disabled 
test_overrides 
test_tensorexpr 
test_package 
```

We encountered this in a problem while running tests

```
python test/run_test.py --verbose -- --save-xml -v
```

`--save-xml` should be passed to each test module, but some test modules not using `common_utils.run_tests()` don't have these parser and will just throw an `error: unrecognized arguments: --save-xml`

cc @mruberry @VitalyFedyunin @walterddr @ptrblck ","2021-01-12","2021-01-13","1","priorized",NULL,"Wang, Xiao","mruberry","PROJECT PATTERNS"
169,"View CreationMeta are not propagated when chaining views","As per title.
Example failing code that generates potentially wrong gradients:

```python
import torch

a = torch.rand(10, requires_grad=True).clone()

# View for which inplace are invalid
b = a.unbind(0)
# Chain of view where the last one is valid for inplace
c = b[0].view_as(b[0])

# This one should fail but does not
c.mul_(2)
# This one properly fails
b.mul_(2)
```

In general, the CreationMeta should be inherited from the previous view to ensure invalid chain of views are properly forbidden.

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @soulitzer","2020-12-24","2021-01-27","34","priorized",NULL,"albanD","ezyang","OTHERS"
170,"torch.where should mention Bool rather than Byte when given wrong dtype mask","## üêõ Bug

![image](https://user-images.githubusercontent.com/5674597/102943344-bb617e80-4485-11eb-8dcc-2124bb6a7314.png)


 - PyTorch Version (e.g., 1.0): 1.7.0
","2020-12-22","2021-05-10","139","priorized",NULL,"Tongzhou Wang","ngimel","OTHERS"
171,"Migrate `masked_fill` from TH to ATen (CUDA)","Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.","2020-12-17","2021-02-10","55","priorized",NULL,"anjali411","anjali411","SOFTWARE ARCHITECTURE"
172,"Migrate `masked_scatter` from TH to ATen (CUDA)","Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.

cc @ezyang @gchanan @zou3519 @bdhirsh","2020-12-17","2021-01-27","41","high priority",NULL,"anjali411","anjali411","SOFTWARE ARCHITECTURE"
173,"Migrate `masked_scatter` from TH to ATen (CPU) ","Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.

cc @ezyang @gchanan @zou3519 @bdhirsh","2020-12-17","2021-01-22","36","high priority",NULL,"anjali411","anjali411","SOFTWARE ARCHITECTURE"
174,"NewModuleTest: merge `check_jacobian` with `check_gradgrad`","## üöÄ Feature

Currently, in our autogenerated torch.nn test suite, NewModuleTest does the following:
- In `check_jacobian`, we compute the analytical and numerical jacobian and compare them.
- For `check_gradgrad`, we run gradcheck and gradgradcheck. gradcheck computes the analytical and numeric jacobian and compares them.

This seems redundant, and in fact, we have a TODO to remove the redundancy between all of these:

https://github.com/pytorch/pytorch/blob/5912316cf7cd8892c76d9610803f11fbeb880ea6/torch/testing/_internal/common_nn.py#L5043

## Alternatives

Deprecate NewModuleTest, design something saner. 


cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved @mruberry","2020-12-15","2020-12-22","7","priorized",NULL,"Richard Zou","albanD","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
175,"Roll-up: remaining TH functions","Not to be confused with THC/THNN/THCUNN, there the list is bigger. 
Based on native_functions.yaml

- [x] _th_put
- [x] _th_index_fill
- [x] _th_renorm
- [x] _th_trace
- [x] _th_nonzero
- [x] _th_gels
- [x] _th_potri #50269
- [x] _th_geqrf #56249
- [x] _th_orgqr #50502
- [x] _th_ormqr #57315
- [x] _multinomial_alias_setup
- [x] _th_histc
- [x] _th_var
- [x] _th_std

Most of them are linear algebra, should be ported.`_multinomial_alias_setup` is likely not used. We should get rid of them, we can do it!



cc @jianyuh @nikitaved @pearu @mruberry @heitorschueroff @walterddr","2020-12-15","2021-06-24","191","priorized",NULL,"Natalia Gimelshein","nikitaved","OTHERS"
176,"View tracking for autograd should not save optional std::function","In the view tracking for autograd in `torch/csrc/variable.h`, we save the optional view function as an `c10::optional<std::function<Variable(const Variable&)>>` but the `std::function` class already has an empty state.

Unless there are performance reasons (see comment https://github.com/pytorch/pytorch/pull/49097#discussion_r541099546), the optional can be removed here.

cc @ezyang @albanD @zou3519 @gqchen @pearu @nikitaved","2020-12-11","2021-01-15","35","priorized",NULL,"albanD","albanD","OTHERS"
177,"Profiling distributed NCCL collectives deadlocks when profiler run with use_cuda=True","## üêõ Bug

We recently enabled profiling of distributed collectives with this PR: https://github.com/pytorch/pytorch/pull/46471. However, when we run the profiler with `use_cuda=True` and the NCCL backend for distributed collective operations, there is a deadlock and the test eventually fails with a timeout. 

The deadlock is caused by a `cudaEventCreate` (called in the profiler here: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/profiler_cuda.cpp#L38) on a device, which attempts to initialize a CUDA context on the device, that causes a wait for all NCCL kernels (and other operations) to exit. However, a separate process has launched a NCCL kernel on the device, and the NCCL kernel is blocked (specifically in `ncclBarrierEnqueueWait`: https://github.com/NVIDIA/nccl/blob/920dbe5b359fe5817b8ba874476ca4ba2dc5f1ef/src/group.cc#L322) waiting for the other rank to call it as well (which it never does). This deadlock can be seen in these stacktraces:
https://gist.github.com/rohan-varma/45f407a1fd7015d1a6d4f5d797ad4734

At a high level, the sequence that results in the deadlock is the following (can be seen in the stacktraces as well):
- Process 1 calls `enableProfilerLegacy()`, which in turn creates warm up and start events on each device: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/profiler_legacy.cpp#L518. Eventually, Process 1 exits this and enters the NCCL kernel launched by the collective.
- The NCCL kernels behave sort of like a barrier, the NCCL kernel cannot exit until all other ranks have called into the kernel as well. Thus, Process 1 blocks until Process 0 would enter its NCCL kernel
Process 0 calls `enableProfilerLegacy()`, which in turn creates events on each device similar to above. Eventually, `cudaCreateEvent` tries to create a cuda context on the device, which blocks until all operations on the device have finished. Since we do this for all devices, that means that device 1 would cause Process 0 to block since it would wait for the NCCL kernel on device 1 to finish. But it will never finish, since it is blocked waiting for Process 0 to call into it.

The main challenges in fixing the deadlock are:
- It's not sufficient to just synchronize every cuda device after `enableProfilerLegacy`. This is because In the above example, Process 1 could finish synchronizing all devices before Process 0 attempts to create the cuda events in `enableProfilerlegacy`. 
- Removing the `onEachDevice()` logic from profiler would result in CUDA events not being created for other devices in the profiler, which would likely break use cases for single-process that use multiple devices, since CUDA operations on those devices would no longer correctly be recorded.
- Since we need some way to sync processes after `enableProfilerLegacy`, we could have ProcessGroupNCCL do a one-time write to the store ensuring the profiler has been initialized properly, when we detect that we are profiling with CUDA. Although, this would incur a (one-time) wait in the distributed collective, which is not desirable from a perf perspective. 


## To Reproduce
- Re-enable distributed profiling tests by patching https://github.com/pytorch/pytorch/pull/48946.
- In `call_dist_op` in `distributed_test`, pass in `use_cuda=True` to the profiler. 
- Run a test which exercises the profiling path, such as `test_all_reduce_sum_cuda_async`. 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd","2020-12-07","2021-05-05","149","priorized",NULL,"Rohan Varma","ngimel","SOFTWARE ARCHITECTURE"
178,"Enable RRef proxy helpers to work with ScriptModule","## üöÄ Feature
We are adding support for transferring ScriptModule over the wire in RPC, see https://github.com/pytorch/pytorch/pull/48293. However, this does not currently work with RRef helper, due to the check on this line: https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/rref_proxy.py#L18.

There is a workaround, however, which is to implement the helper without the check in user code:
```
def run(rref, func_name, args, kwargs):
    return getattr(rref.local_value(), func_name)(*args, **kwargs)

def rref_isinstance(rref, cls_to_check):
    return isinstance(rref.local_value(), cls_to_check)

# Ensure rref is a scriptModule.
ret = rpc.rpc_sync(remote_script_module.owner(), rref_isinstance, args=(remote_script_module, torch.jit.ScriptModule))
assert ret
# Run remote forward pass
remote_forward_output = rpc.rpc_sync(remote_script_module.owner(), run, args=(remote_script_module, ""forward"", (), {}))
```

Proposal is instead of checking for the exact type, such as `type(x) is A` which is what we are doing right now, we check the equivalent of `isinstance(x, A)` for the rref.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @gmagogsfm","2020-11-20","2020-12-04","14","priorized",NULL,"Rohan Varma","gmagogsfm","OTHERS"
179,"Tests to ensure cuda dlls are loaded","## üöÄ Feature
No tests for basic cnn and rnn in windows.
Our current smoke tests don't cover the issues like [Could not load library cudnn_ops_infer64_8.dll](https://github.com/pytorch/pytorch/issues/46312/).
Tests are needed to cover this similar issue.

## Motivation
Since cuda11, some cnn and rnn ops in cuda are moved in different dlls. 
Tests are needed to ensure that dlls are loaded correctly.
Futhermore, it can also cover the basic CNN and RNN functions




cc @peterjc123 @maxluk @nbcsm @guyang3532 @gunandrose4u @smartcat2010 @mszhanyi","2020-11-05","2020-11-06","1","priorized",NULL,"Yi Zhang","Yi Zhang","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
180,"Simplify RPC Message Request/Response type check","As we add more and more message types, the request/response type check function needs to go through more and more comparisons. I wonder if we should organize bits to simplify it. E.g., we can have the lowest 8 bits representing the message type, and then the next 8 bits serve as flags. And have a `kIsRequestMask=0x10` mask. In this case `SCRIPT_CALL` will be `kIsRequestMask | 0`, etc. Then the `isRequest` function can simply be `return kIsRequestMask & type_;`.


https://github.com/pytorch/pytorch/blob/cb4b6336baac14173ec5659896fe3e7f06c2f5b2/torch/csrc/distributed/rpc/message.cpp#L78-L113

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse @agolynski","2020-10-30","2020-11-19","20","priorized",NULL,"mrshenli","H-Huang","OTHERS"
181,"Replace list(map(lambda constructs by list comprehension","During debugging I've seen many constructs like `list(map(lambda i: torch.device('cuda:' + str(i)), gpus))`

I'd propose to replace those by ""regular"" list comprehensions. So the above is equivalent to `[torch.device('cuda:' + str(i)) for i in gpus]`

This replacement can be automated via regexp replacement for (likely) almost all cases.

Reasoning: The nesting of parentheses and the inline lambda make it harder to read than the pythonic approach using list comprehensions. It is also likely that the latter is faster as the intent is clearer to the runtime and being widely used it is likely well  optimized","2020-10-15","2020-10-19","4","priorized",NULL,"Alexander Grund","Flamefire","OTHERS"
182,"Warnings during compiling: floating-point value does not fit in required integral type","Hi, I've moved to Cuda 11.0 and I am getting warnings below while compiling. I just can't remember exactly, but I don't think I saw these with 10.2. I don't know if it related, but `run_test.py` is also failing when it comes to Distribution related tests. I can post test's log when the build is complete, if it is helpful. Thank you.


```
[3950/5005] Building NVCC (Device) object caffe2/CMakeFile...ve/cuda/torch_cuda_generated_DistributionRandomKernel.cu.o
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(60): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=double, V=uint64_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(64): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=double, V=uint64_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(60): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=double, V=uint32_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(64): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=double, V=uint32_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(60): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=float, V=uint64_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(64): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=float, V=uint64_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(60): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=float, V=uint32_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/core/TransformationHelper.h(64): warning: floating-point value does not fit in required integral type
          detected during:
            instantiation of ""T at::transformation::uniform_int<T,V>(V) [with T=float, V=uint32_t]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionTemplates.h(352): here
            instantiation of ""void at::native::templates::cuda::random_kernel(at::TensorIterator &, RNG) [with RNG=at::CUDAGeneratorImpl *]"" 
/home/realiti/virtualenvs/git-repos/pytorch/aten/src/ATen/native/cuda/DistributionRandomKernel.cu(44): here

```

 - PyTorch Version (e.g., 1.0): release/1.7 branch
 - OS (e.g., Linux): Ubuntu 20.04
 - How you installed PyTorch (`conda`, `pip`, source): source build in virtualenv
 - Build command you used (if compiling from source): python setup.py install
 - Python version: 3.8.5
 - CUDA/cuDNN version: 11.0 / 8.0.4
 - GPU models and configuration: rtx 2060
 - Any other relevant information: gcc 8.4



cc @ngimel","2020-10-15","2021-01-07","84","priorized",NULL,"Onur","imaginary-person","OTHERS"
183,"Confusing error message for torch.LongTensor([1], device = 'cuda')","```
torch.LongTensor([1], device = 'cuda')
# RuntimeError: legacy constructor for device type: cpu was passed device type: cuda, but device type must be: cpu
```
`device type: cpu was passed device type: cuda` is hard to pass. adding some punctuation would help!
","2020-10-09","2020-10-12","3","priorized",NULL,"Vadim Kantorov","partypyro","OTHERS"
184,"nn.Module.script","## üöÄ Feature

## Motivation

There is a pervasive pattern within the PyTorch community where users have the need to prepare a Module before calling torch.jit.script. An example here might include replacing an instance of an object with an instance of its JIT-able equivalent (e.g. [PyText ScriptVocab](https://github.com/facebookresearch/pytext/blob/master/pytext/torchscript/vocab.py#L12)) for deployment.

## Pitch

Add a method .script to nn.Module, which is called in a fashion similar to nn.Module.eval. 

The method can be overridden by the user to allow a user-defined preparation of the object for scripting. This might, for example, involve replacing or removing member variables that are not JIT-able.

## Alternatives

Concrete user-defined methods such as torchtext‚Äôs [to_ivalue](https://github.com/pytorch/text/blob/53e3ae293359bc2a629d27e195e99d1e7f7b85c3/test/data/test_functional.py#L93) or separate wrappers such as [PyText ScriptVocab](https://github.com/facebookresearch/pytext/blob/master/pytext/torchscript/vocab.py#L12)
## Additional context

The method should be out of place so the original module input, which the user may continue to use, is not modified. The method should be called within torch.jit.script (if present) so as not to expand the API surface for the end user.

cc @gmagogsfm @albanD @mruberry","2020-09-21","2020-11-10","50","low priority",NULL,"cpuhrsch","dzhulgakov","OTHERS"
185,"Replicate casing in python_dispatch.cpp","simple change that makes casing match DispatchKey casing in 
https://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/python_dispatch.cpp#L30-L35
and `test_dispatch.py`. ","2020-09-14","2020-09-22","8","priorized",NULL,"Ailing Zhang","Ailing Zhang","PROJECT PATTERNS"
186,"Outdated Instruction","https://github.com/pytorch/pytorch/blob/cce5982c4ceb77a0797e8bd4c717ebfec0681eab/tools/build_libtorch.py#L12

Such file does not exist anymore.","2020-09-08","2020-09-18","10","priorized",NULL,"Yan Li","bugra","OTHERS"
187,"Current behavior of `as_tuple` argument is inconsistent in `nonzero`","After `nonzero` started warning that `as_tuple` is a required argument (1.6) a few unfortunate things has happened:
1) the examples in docs started issuing a warning #43425
2) there's no way to pass `out` and `as_tuple` arguments together
```
print(torch.nonzero(a, as_tuple=False)) #ok
print(torch.nonzero(a, out=out)) #warns
print(torch.nonzero(a, as_tuple=False, out=out)) #errors out with `TypeError: nonzero() received an invalid combination of arguments - got unrecognized keyword arguments: out`
```
The rationale behind making `as_tuple` argument required is going through deprecation cycle to change the default value of `as_tuple` so that `torch.nonzero` matches `numpy.nonzero` without any additional args. However, even if implemented properly without problems noted above, this goal is misguided and forces users to modify their codes to avoid warnings for no good reason. 
Torch default behavior for `nonzero` should not match numpy. Numpy behavior can already be achieved by either using `as_tuple=True` or calling `unbind(1)` on results of `torch.nonzero()` call, that is a very light burden for someone who wants it. But to avoid it, we are making everyone who doesn't care jump through deprecation hoops. 
Making numpy behavior default is bad, because numpy returns a tuple of tensors, whereas many users want single tensor return. Going from a single tensor to a tuple is a matter of a cheap `.unbind` call, going from a tuple to a single tensor is memory copy. Default behavior should always pick single tensor. 
My proposal is to go back to making `as_tuple` optional argument and not strive to make `nonzero` match numpy behavior.

cc @albanD @mruberry @rgommers, @alband, @stas00 ","2020-09-07","2021-02-04","150","priorized",NULL,"Natalia Gimelshein","mruberry","OTHERS"
188,"Generalized Helper for Parsing Environment Variables from Process Group","## üöÄ Feature

We are introducing a new flag to guard the behavior of NCCL Async Error Handling. Users can set the environment variable NCCL_ASYNC_ERROR_HANDLING to control this feature (set to 1 if NCCL should crash instead of hanging due to timeouts/errors). A generalized helper function to parse environment variables and set the appropriate flags in `ProcessGroupNCCL` would help make the code more modular, since we already have other flags like NCCL_BLOCKING_WAIT.

Something like the following could work (for binary env vars):

```
void parseBinaryEnvVar(std::string varName, bool& flag) {
   char* varString = getenv(varName);
     if (varString != nullptr) {
       auto val = std::stoi(varString);
       if (val == 1) {
         flag = true;
       } else if (val != 0) {
         throw std::runtime_error(
             ""Invalid value for environment variable: "" +
             std::string(varName));
       }
    }
}
```


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","2020-09-04","2020-11-30","87","priorized",NULL,"Omkar Salpekar","osalpekar","OTHERS"
189,"Please stop the codecov bot from posting comments","Recently the `codecov` bot has started posting comments on PRs like:

<img width=""936"" alt=""image"" src=""https://user-images.githubusercontent.com/98330/92220575-801e4e00-ee9c-11ea-9fdd-a0b7301d1b3a.png"">

This is very spammy and provides zero extra information, it's already one click away in the CI integration:

<img width=""936"" alt=""image"" src=""https://user-images.githubusercontent.com/98330/92220671-9b895900-ee9c-11ea-861d-c4fa3b1e105d.png"">

The bot is very persistant, I dealt with this for NumPy a while back and the only robust solution was to ban the bot at the org level (should be at https://github.com/organizations/pytorch/settings/user_blocks).

EDIT: in addition, the comments are often filled with completely incorrect data, see e.g. https://github.com/pytorch/pytorch/pull/44868#issuecomment-703924115 (multiple files reported on were not touched).","2020-09-04","2022-03-02","544","priorized",NULL,"Ralf Gommers","malfet","OTHERS"
190,"In DDP's Reducer, directly use work.result() to retrieve allreduced tensor.","## Refactor `forwardPassWorkHandle` to use `work.result()` in DDP's `Reducer`
We currently have a hack to retrieve an allreduced tensor from Python in C++ in `reducer.cpp`, where we store the resulting tensor in a struct: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/c10d/reducer.cpp#L434

Now since https://github.com/pytorch/pytorch/pull/43386 is landed, we should refactor this code to simply pass the `work` object and retrieve the tensor with `result()`. 


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","2020-09-01","2020-09-22","21","priorized",NULL,"Rohan Varma","Rohan Varma","OTHERS"
191,"Fix exception chaining all over the codebase","I would like to suggest a small fix in the way that Python 3's exception chaining is used.

As described in detail in [this article](https://blog.ram.rachum.com/post/621791438475296768/improving-python-exception-chaining-with), exception chaining ([PEP 3134](https://www.python.org/dev/peps/pep-3134/)) can be used to make exceptions more user-friendly, and in that case, the syntax `raise new_exception from old_exception` needs to be used.

For example, the following should be used to chain exceptions:
```python
try:
    something_which_raises_TypeError
except TypeError as e:
    raise ValueError(""A more user-friendly exception message."") from e
```
instead of:
```python
try:
    something_which_raises_TypeError
except TypeError:
    raise ValueError(""A more user-friendly exception message."")
```

One example which needs to be fixed is:
https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35

I'd be happy to PR! Let me know your thoughts on this!","2020-08-28","2020-09-01","4","priorized",NULL,"Akihiro Nitta","akihironitta","OTHERS"
192,"Support work.result() to get result tensors for allreduce for Gloo, NCCL backends","## üöÄ Feature
This came up in code review for https://github.com/pytorch/pytorch/pull/42577, where we realized that the only work object that supports `result()` API is for sparse allreduce in Gloo. The contract for `result()` is that it should give us the `tensor(s)` that contain the output of the collective. It would be useful for that PR to have `result()` supported for general allreduce work in the different comm. backends, so that the code can be simplified and we don't have to manually keep track of the result tensor from Python to C++. 


## Pitch

If we limit this to only supporting `allreduce()` work which is the current use case, we can add it to `WorkNCCL` by simply returning the `outputs_` field that already exists. For Gloo, we would probably need to add it to `class AsyncAllreduceWork` in `ProcessGroupGloo`.



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","2020-08-17","2020-09-22","36","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
193,"Exception raised in backward() loses backtrace information","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

## To Reproduce

Steps to reproduce the behavior:
1. Run the code below

```
import torch

class Foo(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        return input

    @staticmethod
    def backward(ctx, *grad):
        raise ValueError(""something"")

t = torch.Tensor(20)
t.requires_grad_()
output = Foo.apply(t)

loss = torch.nn.MSELoss()
loss(output, torch.Tensor(20)).backward()
```

## Expected behavior

Stacktrace at the point of raise, showing the error to be from `Foo.backward`

## Actual output
```
Traceback (most recent call last):
  File ""/private/home/tbirch/bug.py"", line 17, in <module>
    loss(output, torch.Tensor(20)).backward()
  File ""/private/home/tbirch/.conda/envs/py38/lib/python3.8/site-packages/torch/tensor.py"", line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File ""/private/home/tbirch/.conda/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 98, in backward
    Variable._execution_engine.run_backward(
RuntimeError: something
```
## Environment
(same issue in 1.5.1 and 1.6.0)

Collecting environment information...
PyTorch version: N/A
Is debug build: N/A
CUDA used to build PyTorch: N/A

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.10.2

Python version: 3.7
Is CUDA available: N/A
CUDA runtime version: Could not collect
GPU models and configuration:
GPU 0: Quadro GP100
GPU 1: Quadro GP100

Nvidia driver version: 418.116.00
cuDNN version: Could not collect

Versions of relevant libraries:
[pip3] numpy==1.18.5
[pip3] torch==1.5.1
[pip3] torchtext==0.7.0
[pip3] torchvision==0.6.0a0+35d732a
[pip3] torchviz==0.0.1
[conda] blas                      1.0                         mkl
[conda] cudatoolkit               10.1.243             h6bb024c_0
[conda] mkl                       2019.4                      243
[conda] mkl-service               2.3.0            py38h516909a_0    conda-forge
[conda] mkl_fft                   1.1.0            py38hc1659b7_1    conda-forge
[conda] mkl_random                1.1.0            py38h962f231_0
[conda] numpy                     1.18.5           py38ha1c710e_0
[conda] numpy-base                1.18.5           py38hde5b4d6_0
[conda] pytorch                   1.5.1           py3.8_cuda10.1.243_cudnn7.6.3_0    pytorch
[conda] torchtext                 0.7.0                    pypi_0    pypi
[conda] torchvision               0.6.1                py38_cu101    pytorch
[conda] torchviz                  0.0.1                    pypi_0    pypi



cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen","2020-08-04","2020-09-04","31","high priority",NULL,"Tom Birch","albanD","OTHERS"
194,"Fix distributed documentation for asynchronous collective Work objects","## üìö Documentation

In our [distributed docs](https://pytorch.org/docs/stable/distributed.html) under ""Synchronous and asynchronous collective operations"" we document `work.wait()` and `work.is_completed()`, but the documentation is not completely accurate in the case of NCCL.

For a NCCL Work object, `wait()` does not in general block the process and instead synchronizes the NCCL stream with the default stream. `is_completed` does check if the kernel execution is finished but can return false even after calling wait. It might also be useful to clarify the blocking behavior can be achieved with `NCCL_BLOCKING_WAIT=1`.


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski @jlin27","2020-07-29","2020-10-08","71","priorized",NULL,"Rohan Varma","Rohan Varma","OTHERS"
195,"[Gloo, MPI] DDP communication hook: getFuture()","DDP communication hook implementation #39272 provides an API called `get_future` to retrieve a future associated with the completion of `c10d.ProcessGroupNCCL.work`. `get_future` API is currently only supported by NCCL #41596 , because most of the intensive models, where our gradient compression effort might be more useful, use NCCL. 

To support Gloo and MPI, we can modify some of the core ProcessGroupGloo and ProcessGroupMPI code to ensure we can do something similar there.

cc @ezyang @gchanan @zou3519 @bdhirsh @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @xush6528","2020-07-24","2021-05-15","295","high priority",NULL,"Sinan Nasir","wayi1","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
196,"Segfault when passing an empty input to torch.chain_matmul","## üêõ Bug

A segmentation fault occurs when passing an empty input to `torch.chain_matmul`

## To Reproduce

```python
import torch

torch.chain_matmul()
```

## Expected behavior

Seems other functions (e.g., `torch.broadcast_tensors()`) do handle an empty input without causing a segfault. I would expect the same for this function.

## Environment

PyTorch version: 1.5.0
Is debug build: No
CUDA used to build PyTorch: 10.1

OS: Ubuntu 18.04.4 LTS
GCC version: Could not collect
CMake version: version 3.14.0

Python version: 3.7
Is CUDA available: No
CUDA runtime version: No CUDA
GPU models and configuration: No CUDA
Nvidia driver version: No CUDA
cuDNN version: No CUDA

Versions of relevant libraries:
[pip] numpy==1.18.1
[pip] torch==1.5.0
[pip] torchvision==0.6.0a0+82fd1c8
[conda] blas 1.0 mkl
[conda] cudatoolkit 10.1.243 h6bb024c_0
[conda] mkl 2020.1 217
[conda] mkl-include 2020.1 217
[conda] mkl-service 2.3.0 py37he904b0f_0
[conda] mkl_fft 1.0.15 py37ha843d7b_0
[conda] mkl_random 1.1.0 py37hd6b4f25_0
[conda] numpy 1.18.1 py37h4f9e942_0
[conda] numpy-base 1.18.1 py37hde5b4d6_1
[conda] pytorch 1.5.0 py3.7_cuda10.1.243_cudnn7.6.3_0 pytorch
[conda] torchvision 0.6.0 py37_cu101 pytorch","2020-07-21","2020-08-26","36","priorized",NULL,"Mijung Kim","kshitij12345","OTHERS"
197,"Replace blacklist/whitelist in torch/quantization/quantize.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/quantization/quantize.py`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
198,"Replace blacklist/whitelist in torch/quantization/default_mappings.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/quantization/default_mappings.py`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
199,"Replace blacklist/whitelist in torch/quantization/_numeric_suite.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/quantization/_numeric_suite.py`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
200,"Replace blacklist/whitelist in torch/csrc/jit/passes/lower_tuples.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/jit/passes/lower_tuples.cpp`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
201,"Replace blacklist/whitelist in torch/csrc/jit/passes/quantization/helper.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/jit/passes/quantization/helper.cpp`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
202,"Replace blacklist/whitelist in test/backward_compatibility/check_backward_compatibility.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/backward_compatibility/check_backward_compatibility.py`. See issue #41443 for more information.","2020-07-20","2020-07-27","7","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
203,"Replace blacklist/whitelist in test/test_autograd.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/test_autograd.py`. See issue #41443 for more information.","2020-07-20","2020-07-24","4","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
204,"Replace blacklist/whitelist in caffe2/CMakeLists.txt","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/CMakeLists.txt`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","kalmufti","OTHERS"
205,"Replace blacklist/whitelist in torch/quantization/default_mappings.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/quantization/default_mappings.py`. See issue #41443 for more information.","2020-07-20","2020-09-04","46","priorized",NULL,"SplitInfinity","VinodSKumar","OTHERS"
206,"Replace blacklist/whitelist in torch/jit/_script.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/jit/_script.py`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
207,"Replace blacklist/whitelist in tools/clang_format_ci.sh","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/clang_format_ci.sh`. See issue #41443 for more information.","2020-07-20","2020-07-21","1","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
208,"Replace blacklist/whitelist in tools/code_analyzer/gen_op_registration_whitelist.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/code_analyzer/gen_op_registration_whitelist.py`. See issue #41443 for more information.","2020-07-20","2020-09-29","71","priorized",NULL,"SplitInfinity","Vishal487","OTHERS"
209,"Replace blacklist/whitelist in tools/clang_format_all.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/clang_format_all.py`. See issue #41443 for more information.","2020-07-20","2020-10-06","78","priorized",NULL,"SplitInfinity","REX51","OTHERS"
210,"Replace blacklist/whitelist in test/quantization/test_quantized_op.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/quantization/test_quantized_op.py`. See issue #41443 for more information.","2020-07-20","2020-07-21","1","priorized",NULL,"SplitInfinity","kenjihiraoka","OTHERS"
211,"Replace blacklist/whitelist in test/backward_compatibility/check_backward_compatibility.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/backward_compatibility/check_backward_compatibility.py`. See issue #41443 for more information.","2020-07-20","2021-03-03","226","priorized",NULL,"SplitInfinity","veeara282","OTHERS"
212,"Replace blacklist/whitelist in test/test_namedtuple_return_api.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/test_namedtuple_return_api.py`. See issue #41443 for more information.","2020-07-20","2020-07-27","7","priorized",NULL,"SplitInfinity","agniutkarsh","OTHERS"
213,"Replace blacklist/whitelist in cmake/Whitelist.cmake","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `cmake/Whitelist.cmake`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
214,"Replace blacklist/whitelist in caffe2/transforms/common_subexpression_elimination.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/transforms/common_subexpression_elimination.h`. See issue #41443 for more information.","2020-07-20","2020-07-23","3","priorized",NULL,"SplitInfinity","priyanshuwustl","OTHERS"
215,"Replace blacklist/whitelist in cmake/Codegen.cmake","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `cmake/Codegen.cmake`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","DanielCazarez","OTHERS"
216,"Replace blacklist/whitelist in caffe2/transforms/common_subexpression_elimination.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/transforms/common_subexpression_elimination.cc`. See issue #41443 for more information.","2020-07-20","2020-07-23","3","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
217,"Replace blacklist/whitelist in caffe2/predictor/transforms.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/predictor/transforms.cc`. See issue #41443 for more information.","2020-07-20","2020-07-21","1","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
218,"Replace blacklist/whitelist in caffe2/CMakeLists.txt","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/CMakeLists.txt`. See issue #41443 for more information.","2020-07-20","2020-07-27","7","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
219,"Replace blacklist/whitelist in aten/src/ATen/gen.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `aten/src/ATen/gen.py`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
220,"Replace blacklist/whitelist in .github/workflows/clang_format.yml","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `.github/workflows/clang_format.yml`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","kalmufti","OTHERS"
221,"Replace blacklist/whitelist in .circleci/cimodel/data/simple/binary_smoketest.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `.circleci/cimodel/data/simple/binary_smoketest.py`. See issue #41443 for more information.","2020-07-20","2020-07-23","3","priorized",NULL,"SplitInfinity","kenjihiraoka","OTHERS"
222,"Replace blacklist/whitelist in CMakeLists.txt","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `CMakeLists.txt`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
223,"Replace blacklist/whitelist in torch/onnx/symbolic_opset8.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/onnx/symbolic_opset8.py`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
224,"Replace blacklist/whitelist in torch/onnx/symbolic_opset7.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/onnx/symbolic_opset7.py`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
225,"Replace blacklist/whitelist in torch/onnx/symbolic_helper.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/onnx/symbolic_helper.py`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","ssooffiiaannee","OTHERS"
226,"Replace blacklist/whitelist in caffe2/python/onnx/onnxifi.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/python/onnx/onnxifi.py`. See issue #41443 for more information.","2020-07-20","2020-08-20","31","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
227,"Replace blacklist/whitelist in caffe2/python/pybind_state.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/python/pybind_state.cc`. See issue #41443 for more information.","2020-07-20","2020-07-29","9","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
228,"Replace blacklist/whitelist in test/backward_compatibility/check_backward_compatibility.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/backward_compatibility/check_backward_compatibility.py`. See issue #41443 for more information.","2020-07-20","2020-07-24","4","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
229,"Replace blacklist/whitelist in caffe2/opt/custom/glow_net_transform.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/custom/glow_net_transform.cc`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
230,"Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/tvm_transformer.cc`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
231,"Replace blacklist/whitelist in caffe2/onnx/onnx_exporter.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/onnx/onnx_exporter.h`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
232,"Replace blacklist/whitelist in torch/utils/hipify/hipify_python.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/utils/hipify/hipify_python.py`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
233,"Replace blacklist/whitelist in torch/utils/mobile_optimizer.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/utils/mobile_optimizer.py`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
234,"Replace blacklist/whitelist in torch/jit/_recursive.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/jit/_recursive.py`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
235,"Replace blacklist/whitelist in torch/csrc/utils/python_arg_parser.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/utils/python_arg_parser.cpp`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
236,"Replace blacklist/whitelist in torch/csrc/jit/passes/xnnpack_rewrite.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/jit/passes/xnnpack_rewrite.h`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
237,"Replace blacklist/whitelist in torch/csrc/jit/python/init.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/jit/python/init.cpp`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
238,"Replace blacklist/whitelist in torch/csrc/jit/passes/xnnpack_rewrite.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `torch/csrc/jit/passes/xnnpack_rewrite.cpp`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
239,"Replace blacklist/whitelist in tools/pyi/gen_pyi.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/pyi/gen_pyi.py`. See issue #41443 for more information.","2020-07-20","2020-07-27","7","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
240,"Replace blacklist/whitelist in tools/jit/gen_unboxing_wrappers.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/jit/gen_unboxing_wrappers.py`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
241,"Replace blacklist/whitelist in tools/autograd/gen_python_functions.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `tools/autograd/gen_python_functions.py`. See issue #41443 for more information.","2020-07-20","2020-08-18","29","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
242,"Replace blacklist/whitelist in test/test_type_hints.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/test_type_hints.py`. See issue #41443 for more information.","2020-07-20","2020-07-21","1","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
243,"Replace blacklist/whitelist in test/test_mobile_optimizer.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/test_mobile_optimizer.py`. See issue #41443 for more information.","2020-07-20","2020-10-03","75","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
244,"Replace blacklist/whitelist in test/test_jit.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/test_jit.py`. See issue #41443 for more information.

cc @suo @gmagogsfm","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
245,"Replace blacklist/whitelist in docs/cpp/source/Doxyfile","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `docs/cpp/source/Doxyfile`. See issue #41443 for more information.","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
246,"Replace blacklist/whitelist in test/run_test.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `test/run_test.py`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","nomanarshad94","OTHERS"
247,"Replace blacklist/whitelist in cmake/public/cuda.cmake","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `cmake/public/cuda.cmake`. See issue #41443 for more information.","2020-07-20","2020-10-06","78","priorized",NULL,"SplitInfinity","REX51","OTHERS"
248,"Replace blacklist/whitelist in cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
249,"Replace blacklist/whitelist in caffe2/python/onnx/frontend.py","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/python/onnx/frontend.py`. See issue #41443 for more information.","2020-07-20","2020-07-22","2","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
250,"Replace blacklist/whitelist in caffe2/python/pybind_state.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/python/pybind_state.cc`. See issue #41443 for more information.","2020-07-20","2020-08-18","29","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
251,"Replace blacklist/whitelist in caffe2/opt/custom/glow_net_transform.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/custom/glow_net_transform.h`. See issue #41443 for more information.","2020-07-20","2021-01-21","185","priorized",NULL,"SplitInfinity","admud","OTHERS"
252,"Replace blacklist/whitelist in caffe2/opt/custom/fakefp16_transform.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/custom/fakefp16_transform.cc`. See issue #41443 for more information.","2020-07-20","2021-02-18","213","priorized",NULL,"SplitInfinity","Rubix982","OTHERS"
253,"Replace blacklist/whitelist in caffe2/opt/custom/glow_net_transform.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/custom/glow_net_transform.cc`. See issue #41443 for more information.","2020-07-20","2021-01-21","185","priorized",NULL,"SplitInfinity","admud","OTHERS"
254,"Replace blacklist/whitelist in caffe2/opt/tvm_transformer.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/tvm_transformer.h`. See issue #41443 for more information.","2020-07-20","2020-09-18","60","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
255,"Replace blacklist/whitelist in caffe2/opt/onnxifi_transformer.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/onnxifi_transformer.h`. See issue #41443 for more information.","2020-07-20","2020-08-18","29","priorized",NULL,"SplitInfinity","admud","OTHERS"
256,"Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/tvm_transformer.cc`. See issue #41443 for more information.","2020-07-20","2020-09-18","60","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
257,"Replace blacklist/whitelist in caffe2/opt/onnxifi_transformer.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/onnxifi_transformer.cc`. See issue #41443 for more information.","2020-07-20","2020-08-18","29","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
258,"Replace blacklist/whitelist in caffe2/opt/backend_transformer_base.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/opt/backend_transformer_base.h`. See issue #41443 for more information.","2020-07-20","2020-07-28","8","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
259,"Replace blacklist/whitelist in caffe2/onnx/onnx_exporter.h","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/onnx/onnx_exporter.h`. See issue #41443 for more information.","2020-07-20","2020-07-31","11","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
260,"Replace blacklist/whitelist in caffe2/onnx/onnx_exporter.cc","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `caffe2/onnx/onnx_exporter.cc`. See issue #41443 for more information.","2020-07-20","2020-07-30","10","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
261,"Replace blacklist/whitelist in aten/src/ATen/native/cudnn/Conv.cpp","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `aten/src/ATen/native/cudnn/Conv.cpp`. See issue #41443 for more information.","2020-07-20","2020-07-27","7","priorized",NULL,"SplitInfinity","admud","OTHERS"
262,"Replace blacklist/whitelist in .jenkins/pytorch/dirty.sh","The goal of this issue is to replace all instances of *whitelist* and *blacklist* in `.jenkins/pytorch/dirty.sh`. See issue #41443 for more information.","2020-07-20","2020-07-20","0","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
263,"Remove torch/csrc/utils/future.h","We currently have two Future classes: https://github.com/pytorch/pytorch/blob/master/torch/csrc/utils/future.h#L29 and https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/ivalue_inl.h#L222.

As part of the consolidation work for the Future, my impression was that we only use the latter and not the former. In that case, can we get rid of former? Or are there still a bunch of places that use `torch/csrc/utils/future.h`? If so, are we tracking this work in a separate task?

cc @ezyang @gchanan @zou3519 @bdhirsh @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @xush6528","2020-07-17","2021-02-19","217","high priority",NULL,"Pritam Damania","rohan-varma","OTHERS"
264,"Replace blacklist/whitelist throughout the PyTorch codebase","## Goal
The goal of this issue is to track overall progress toward eliminating the use of the terms *whitelist* and *blacklist* from the PyTorch codebase. The recommended replacements for these terms are *allowlist* and *blocklist*, respectively. [See this numpy issue for more information.](https://github.com/numpy/numpy/pull/16669)

However, we should strive to use more specific and appropriate terms based on the context in which *whitelist* and *blacklist* are used. For example, if the code in question constructs a list of tests that are to be skipped and stores it into a variable called `test_blacklist`, `test_skiplist` would be a better choice than `test_blocklist`. But if there are no other options, *allowlist* and *blocklist* are the designated fallback terms.

## How To Help
1. Pick a file from the list below. Comment on the issue saying that you are working on it and assign it to yourself.
2. Work on your task. **Replace all instances of *blacklist* or *whitelist* as appropriate.** Submit a PR. **Make sure your PR complies with [the PyTorch code review values](https://github.com/pytorch/pytorch/wiki/Code-review-values)**.
3. Choose someone who is familiar with the code in question to review your PR. GitHub UI can help with this.
4. Merge your PR (or get someone from the team to merge it).

## Unexpected Issues
It is possible that a simple find-and-replace strategy will not work for some of the occurrences in the list below. A good example is `aten/src/ATen/gen.py`; this tool has command-line flags with `whitelist` in them that are consumed by other tools. We need to be more careful in these cases. For now, if you come across a case like this, leave a comment in the issue detailing your findings and edit this post to link the corresponding file to that issue or comment below.


## To-Do List

**Blacklist**
- [x] `.jenkins/pytorch/dirty.sh` (#41699)
- [x] `aten/src/ATen/native/cudnn/Conv.cpp` (#41700)
- [x] `caffe2/onnx/onnx_exporter.cc` (#41701)
- [x] `caffe2/onnx/onnx_exporter.h` (#41702)
- [x] `caffe2/opt/backend_transformer_base.h` (#41703)
- [x] `caffe2/opt/onnxifi_transformer.cc` (#41704)
- [x] `caffe2/opt/onnxifi_transformer.h` (#41705)
- [x] `caffe2/opt/tvm_transformer.cc` (#41706)
- [x] `caffe2/opt/tvm_transformer.h` (#41707)
- [ ] `caffe2/opt/custom/fakefp16_transform.cc` (#41708)
- [ ] `caffe2/opt/custom/glow_net_transform.cc` (#41709)
- [ ] `caffe2/opt/custom/glow_net_transform.h` (#41710)
- [x] `caffe2/python/pybind_state.cc` (#41711)
- [x] `caffe2/python/onnx/frontend.py` (#41712)
- [ ] `cmake/Modules_CUDA_fix/upstream/FindCUDA.cmake` (#41713)
- [ ] `cmake/public/cuda.cmake` (#41714)
- [x] `docs/cpp/source/Doxyfile` (#41715)
- [x] `test/run_test.py` (#41716)
- [x] `test/test_jit.py` (#41717)
- [x] `test/test_mobile_optimizer.py` (#41718)
- [x] `test/test_type_hints.py` (#41719)
- [x] `tools/autograd/gen_python_functions.py` (#41720)
- [x] `tools/jit/gen_unboxing_wrappers.py` (#41721)
- [x] `tools/pyi/gen_pyi.py` (#41722)
- [x] `torch/csrc/jit/passes/xnnpack_rewrite.cpp` (#41723)
- [x] `torch/csrc/jit/passes/xnnpack_rewrite.h` (#41724)
- [x] `torch/csrc/jit/python/init.cpp` (#41725)
- [x] `torch/csrc/utils/python_arg_parser.cpp` (#41726)
- [x] `torch/jit/_recursive.py` (#41727)
- [x] `torch/utils/mobile_optimizer.py` (#41728)
- [x] `torch/utils/hipify/hipify_python.py` (#41729)
- [x] `caffe2/onnx/onnx_exporter.h` (#41730)
- [x] `caffe2/opt/tvm_transformer.cc` (#41731)
- [ ] `caffe2/opt/custom/glow_net_transform.cc` (#41732)
- [x] `test/backward_compatibility/check_backward_compatibility.py` (#41733)
- [x] `caffe2/python/pybind_state.cc` (#41734)
- [x] `caffe2/python/onnx/onnxifi.py` (#41735)
- [ ] `torch/onnx/symbolic_helper.py` (#41736)
- [ ] `torch/onnx/symbolic_opset7.py` (#41737)
- [ ] `torch/onnx/symbolic_opset8.py` (#41738)

**Whitelist**
- [ ] `CMakeLists.txt` (#41739)
- [x] `.circleci/cimodel/data/simple/binary_smoketest.py` (#41740)
- [x] `.github/workflows/clang_format.yml` (#41741)
- [ ] `aten/src/ATen/gen.py` (#41742)
- [x] `caffe2/predictor/transforms.cc` (#41744)
- [x] `caffe2/transforms/common_subexpression_elimination.cc` (#41745)
- [x] `caffe2/transforms/common_subexpression_elimination.h` (#41746)
- [ ] `cmake/Codegen.cmake` (#41747)
- [x] `cmake/Whitelist.cmake` (#41748)
- [x] `test/test_namedtuple_return_api.py` (#41749)
- [x] `test/backward_compatibility/check_backward_compatibility.py` (#41750)
- [x] `test/quantization/test_quantized_op.py` (#41751)
- [x] `tools/clang_format_all.py` (#41752)
- [x] `tools/clang_format_ci.sh` (#41753)
- [x] `tools/code_analyzer/gen_op_registration_whitelist.py` (#41754)
- [x] `torch/jit/_script.py` (#41755)
- [x] `torch/quantization/default_mappings.py` (#41756)
- [x] `caffe2/CMakeLists.txt` (#41757)
- [x] `test/test_autograd.py` (#41758)
- [x] `torch/csrc/jit/passes/quantization/helper.cpp` (#41759)
- [x] `torch/csrc/jit/passes/lower_tuples.cpp` (#41761)
- [x] `torch/quantization/_numeric_suite.py` (#41762)
- [x] `torch/quantization/default_mappings.py` (#41763)
- [x] `torch/quantization/quantize.py` (#41764)","2020-07-14","2021-02-18","219","priorized",NULL,"SplitInfinity","SplitInfinity","OTHERS"
265,"GPU Skip decorators in common_distributed can misreport device information","## üêõ Bug

In `common_distributed.py` we have some widely-used utilities that skip tests if there are not a certain number of GPUs. The problem, however, is that many of them rely on the following code: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/common_distributed.py#L30 which hardcodes the error message as ""Need at least 2 CUDA devices"", but functions such a `skip_if_lt_x_gpu(x)` can be invoked with an arbitrary # of GPUs. We should enhance this test-skipping logic so that the correct no. of GPUs needed is reported when a test is skipped. Currently, it is quite confusing to get an error message such as ""Need at least 2 CUDA devices"" when you're running on (for example) a 4-GPU machine.



cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","2020-07-13","2020-08-26","44","priorized",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
266,"torch.distributed NCCL backend does not support bitwise reduction ops","## üêõ Bug

The documentation at https://pytorch.org/docs/stable/distributed.html specifies that BAND, BOR, BXOR are supported reduction operators, however, they do not work with `all_reduce` using the NCCL backend.

We can see in the [code](https://github.com/pytorch/pytorch/blob/c451ddaeda3e58d0f51d4026add4de1b3cf9657d/torch/lib/c10d/ProcessGroupNCCL.cpp#L39) that there is no mapping for the bitwise operators, and we [use this mapping](https://github.com/pytorch/pytorch/blob/c451ddaeda3e58d0f51d4026add4de1b3cf9657d/torch/lib/c10d/ProcessGroupNCCL.cpp#L744) to get the nccl operation to run. What happens when the mapping is not specified is that the map attempts to default construct a `ncclRedOp_t` type (https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/types.html#ncclredop-t) and ends up incorrectly mapping these reduction types to `ncclSum`. This will mean that if we use these bitwise reduction ops we will just end up doing a sum. 

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jiayisuse @agolynski","2020-07-13","2020-08-07","25","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
267,"In DDP's reducer merge `work` and `future_work`","DDP main structure communication hook #40848 uses separate `bucket.work` and `bucket.future_work`. These use async works `ProcessGroup::Work` and `jit::Future`, respectively.
https://github.com/pytorch/pytorch/blob/45d735208b29aaa9dcfdf4d2b426c219866a8097/torch/csrc/distributed/c10d/reducer.cpp#L592-L596

We will replace `work` by `future_work` and use `AllreduceHook` by default once `get_future` API is supported by Gloo, see #42048 . There is a PR for this issue (#41840) that can be merged `get_future` API after Gloo support and review.

NOTE: This can be done after https://github.com/pytorch/pytorch/issues/42048 is addressed.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse","2020-07-10","2021-06-08","333","priorized",NULL,"Sinan Nasir","wayi1","OTHERS"
268,"torch/library.h bad error message when registering with explicit namespace","```
  what():  1Explicitly provided namespace (torchvision) in schema string does not match namespace of enclosing TORCH_LIBRARY block (torchvision).  Move this definition to the (unique) TORCH_LIBRARY block corresponding to this namespace (and consider deleting the namespace from your schema string.)  (Error occurred while processing TORCH_LIBRARY block at /data/users/ezyang/vision/torchvision/csrc/vision.cpp:45)
```

I got this with

```
TORCH_LIBRARY(torchvision, m) {
  m.def(""torchvision::nms"", &nms);
  m.def(""torchvision::roi_align(Tensor input, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sampling_ratio, bool aligned) -> Tensor"",
      &roi_align);
  m.def(""torchvision::roi_pool"", &roi_pool);
  m.def(""torchvision::_new_empty_tensor_op"", &new_empty_tensor);
  m.def(""torchvision::ps_roi_align"", &ps_roi_align);
  m.def(""torchvision::ps_roi_pool"", &ps_roi_pool);
  m.def(""torchvision::deform_conv2d"", &deform_conv2d);
  m.def(""torchvision::_cuda_version"", &_cuda_version);
}
```","2020-06-29","2020-07-01","2","priorized",NULL,"Edward Z. Yang","Edward Z. Yang","PROJECT PATTERNS"
269,"Improve error reporting in ""Check for no AVX instruction by default""","When AVX/AVX2 instruction is encountered while running `basic` unit test, qemu segfaults, which does give developer a good idea why the test have failed.
This could be improved by re-running `qemu` with gdbserver and printing a backtrace + instruction that caused the failure. This can be done by modifying
https://github.com/pytorch/pytorch/blob/a80dd02a224ea0f2c9582f428bc12d4579deaa92/.circleci/verbatim-sources/job-specs/pytorch-job-specs.yml#L135 
To something along the following lines:
```
$ qemu-x86_64 -g 2345 -cpu Broadwell -E ATEN_CPU_CAPABILITY=default ./basic --gtest_filter=BasicTest.BasicTestCPU &
$ gdb ./basic -ex ""target remote :2345"" -ex ""continue"" -ex ""bt""
```

How to test that the change:
Add global AVX variable to any .cpp file, for example:
```
const __m256i foobar = _mm256_set_epi32(0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08);
```

","2020-06-19","2020-07-06","17","priorized",NULL,"Nikita Shulga","RockingJavaBean","PROJECT PATTERNS"
270,"Expose rpc._wait_all_workers as rpc.barrier","RPC uses `_wait_all_workers` as a barrier to do graceful shutdown. This might be a useful feature to applications as well, i.e., we can have `rpc.barrier()` for RPC and `dist.barrier()` for c10d. 

Note that this is different from #40107:

* The `rpc.barrier()` only makes sure that all workers reach the same point, while there could still be on-going concurrent execution in other RPC threads to satisfy futures created by the main thread. 
* The context manager collects and waits for all futures created in scope, but does not communicate with other workers.

https://github.com/pytorch/pytorch/blob/3fb1e73a4e928a0452c3b73ca6815f67127424e9/torch/distributed/rpc/api.py#L137-L198

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @xush6528","2020-06-17","2021-06-02","350","high priority",NULL,"mrshenli","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
271,"C API pollutes the global namespace","## üêõ Bug

`libtorch/include/c10/util/logging_is_not_google_glog.h` exposes `ERROR`, `FATAL`, etc in the global namespace.
This is included by `torch/script.h`.

This causes the conflict with a user code.

## Expected behavior

Put them in some namespace.

## Environment

build-hash: 4ff3872a2099993bf7e...
build-version: 1.5.0+cpu


cc @malfet @yf225 @glaringlee","2020-06-16","2020-06-24","8","priorized",NULL,"elbaro","hikatech","OTHERS"
272,"RPC package should provide a backend-agnostic helper to verify names/ids","In OSS, ProcessGroup RPC backend verifies all worker names are distinct and uses ranks as ids. So that wrong name-to-id mapping will be spotted at construction time. However, for other RPC backend without collective communication capabilities, it is not that easy to gather all information from all workers and check their correctness. 

An alternative is to let `torch.distributed.rpc` provide a backend-agnostic helper to check all worker names using `c10d::Store`, and assign ids using the `c10d::Store`. This should help prevent applications from introducing unintentional mapping errors.

cc @ezyang @gchanan @zou3519 @bdhirsh @jbschlosser @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @jjlilley @osalpekar @jiayisuse @agolynski @SciPioneer @H-Huang @mrzzd @cbalioglu @xush6528","2020-06-15","2021-03-07","265","high priority",NULL,"mrshenli","kiukchung","OTHERS"
273,"Distributed RPC profiling should support record_shapes","## üöÄ Feature
We currently don't carry over the `record_shapes` flag and return the recorded shapes over RPC when we remotely invoke the profiler, we should add support for this for parity with regular profiling.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @jiayisuse","2020-06-12","2020-09-26","106","priorized",NULL,"Rohan Varma","Rohan Varma","OTHERS"
274,"Add repr for torch.distributed.rpc.WorkerInfo","## üöÄ Feature
Currently [torch.distributed.rpc.WorkerInfo] does not have a str/repr function, which means that:
```
current_node = rpc.api.get_worker_info()
print(current_node)
```
does not show relevant information about the current node such as Name and Id. This would be 
useful to have for debug purposes. It could look like

```
>> current_node = rpc.api.get_worker_info()
>> print(current_node)
>> ""RPC worker with name {name} and id {id}""
```

This will speed up and improve the experience of debugging RPC-related issues.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-06-12","2020-06-15","3","low priority",NULL,"Rohan Varma","Rohan Varma","PROJECT PATTERNS"
275,"RPC should destroy/deinitialize dist autograd container when rpc is shutdown","## üöÄ Feature/Enhnacement

During RPC initialization, each participating process initializes a singleton `DistAutogradContainer`. During shutdown, we cleanup items such as the RRef context and RPC agent, but do not reset this singleton dist autograd container. This prevents RPC from being reinitialized (i.e. a group of processes cannot call `rpc.shutdown() ; dist.barrier() ; rpc.init_rpc(...)`. It will fail with the following error:

```
/test/distributed/rpc/faulty_agent/rpc_spawn_faulty#binary,link-tree
/torch/distributed/rpc/__init__.py"", line 85, in init_rpc
>     dist_autograd._init(rank)
> RuntimeError: Container is already initialized! Cannot initialize it twice!
```

## Motivation
I was looking into seeing if we could reuse spawned subprocesses across our RPC tests, which would decrease the amount of time it takes to run the RPC test suite; however, this would require processes to be able to re-init RPC, which is blocked by this error. Also, I was writing a set of similar tests in https://github.com/pytorch/pytorch/pull/38590, and wanted to use this pattern to reduce code duplication, but couldn't (the tests need to use different initializations of RPC since they test the RRef deletion behavior triggered by the shutdown sequence). 

## Pitch
I don't know if we necessarily need to destroy the container, maybe we could just de-initialize it, and then re-init if RPC is being started back up again.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-06-01","2020-08-24","84","priorized",NULL,"Rohan Varma","Rohan Varma","OTHERS"
276,"Remove all self.assertEqualIgnoreTypes instances","As `self.assertEqual` now compares tensor's dtypes as well as values, there are some number of tests which are unhappy with it. So we temporarily replaced such cases of `self.assertEqual` with `self.assertEqualIgnoreTypes`. 

However it is temporary measure, as `self.assertEqualIgnoreTypes` calls need to be reviewed on case-by-case basis and replaced with `self.assertEqual` calls and proper dtypes or `self.assertEqual` with `exact_dtype=False` wherever it is not possible by design.

All this tasks are searchable by: `TODO(#38095)` after landing of #38102

cc @mruberry","2020-05-07","2022-12-15","952","priorized",NULL,"Vitaly Fedyunin","kit1980","OTHERS"
277,"RuntimeError: CUDA error: device-side assert triggered","/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1320,0,0], thread: [102,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1586,0,0], thread: [44,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1187,0,0], thread: [3,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1231,0,0], thread: [36,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1453,0,0], thread: [73,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1098,0,0], thread: [65,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1497,0,0], thread: [106,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1142,0,0], thread: [98,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
/opt/conda/conda-bld/pytorch_1556653114079/work/aten/src/ATen/native/cuda/IndexKernel.cu:53: lambda ->auto::operator()(int)->auto: block: [1364,0,0], thread: [7,0,0] Assertion index >= -sizes[i] && index < sizes[i] && ""index out of bounds"" failed.
Traceback (most recent call last):
File ""train.py"", line 130, in
main()
File ""train.py"", line 120, in main
log_interval = cfg.log_config.interval
File ""/media/adas/File/wdx/dianyun/SA-SSD/tools/train_utils/init.py"", line 95, in train_model
log_interval = log_interval
File ""/media/adas/File/wdx/dianyun/SA-SSD/tools/train_utils/init.py"", line 57, in train_one_epoch
outputs = batch_processor(model, data_batch)
File ""/media/adas/File/wdx/dianyun/SA-SSD/tools/train_utils/init.py"", line 29, in batch_processor
losses = model(**data)
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in call
result = self.forward(*input, **kwargs)
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py"", line 150, in forward
return self.module(*inputs[0], **kwargs[0])
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in call
result = self.forward(*input, **kwargs)
File ""/media/adas/File/wdx/dianyun/SA-SSD/mmdet/models/detectors/base.py"", line 79, in forward
return self.forward_train(img, img_meta, **kwargs)
File ""/media/adas/File/wdx/dianyun/SA-SSD/mmdet/models/detectors/single_stage.py"", line 83, in forward_train
(x, conv6), point_misc = self.neck(vx, ret['coordinates'], batch_size)
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 493, in call
result = self.forward(*input, **kwargs)
File ""/media/adas/File/wdx/dianyun/SA-SSD/mmdet/models/necks/cmn.py"", line 109, in forward
x = x.dense()
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/spconv/init.py"", line 73, in dense
res = scatter_nd(self.indices.long(), self.features, output_shape)
File ""/home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/spconv/init.py"", line 41, in scatter_nd
ret[slices] = updates.view(*output_shape)
RuntimeError: CUDA error: device-side assert triggered
terminate called after throwing an instance of 'c10::Error'
what(): CUDA error: device-side assert triggered (insert_events at /opt/conda/conda-bld/pytorch_1556653114079/work/c10/cuda/CUDACachingAllocator.cpp:564)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x45 (0x7fc6cd77bdc5 in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: + 0x14792 (0x7fc6ca692792 in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10_cuda.so)
frame #2: c10::TensorImpl::release_resources() + 0x50 (0x7fc6cd76b640 in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #3: + 0x3067fb (0x7fc6cadb27fb in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch.so.1)
frame #4: + 0x14019b (0x7fc6fc79819b in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: + 0x3bfc84 (0x7fc6fca17c84 in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #6: + 0x3bfcd1 (0x7fc6fca17cd1 in /home/adas/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/lib/libtorch_python.so)

frame #24: __libc_start_main + 0xf0 (0x7fc70c3cf830 in /lib/x86_64-linux-gnu/libc.so.6)

Â∑≤ÊîæÂºÉ (Ê†∏ÂøÉÂ∑≤ËΩ¨ÂÇ®)
","2020-05-06","2020-05-07","1","priorized",NULL,"wdxpython","ssnl","OTHERS"
278,"[DISCUSSION] Stop exposing FutureMessage to Python land","Currently `FutureMessage` is used in several places:

1. `rpc_async` returns a `FutureMessage` object and we expose it as `torch.distributed.rpc.Future`. From applications perspective, they are expecting a `py::object` instead of a `Message`, and we do the conversion in the `Future.wait()` pybind method.
2. RPC autograd profiler takes `FutureMessage` and installs callbacks to it. The profiler actually only need a `Future<T>` and does not care what `T` is. (learnt from @rohan-varma)
3. `OwnerRRef` exposes a `getFuture()` API which returns a `FutureMessage`. This `FutureMessage` will be marked completed when the value referenced by the `OwnerRRef` is ready. `OwnerRRef` does not need it to be a Message type either, it actually creates an empty `Message` to mark the `Future`.

The above places are using `FutureMessage`, but they don't really need a `Message`, and `Message` is a communication layer type that applications or profiler or the RRef shouldn't be aware of. I was thinking about exposing an `FuturePyObj` and make `FutureMessage` a comm layer only type. The `FutureMessage` can mark `FuturePyObj` as completed when the message is ready. After a short discussion with the team, @xush6528 mentioned that `Future<IValue>` might be a better option as 1) IValue can hold `py::object` 2) it will make it easier to merge with jit Future.

Another motivation for making this change is that for async RPC UDF #36071, we are going to allow application to call `markCompleted` in Python. If we still use `FutureMessage`, then in the `markCompleted` pybind function, it needs to convert the provided `py::object` into a specific message type, which is leaking communication layer code to pybind functions. Even if this is doable, we will have two entities (RPC agent and pybind Python frontend) accessing the same request callback logic. This is too messy. 

I am going to try @xush6528's suggestion to replace `FutureMessage` with `Future<IValue>` in layers above comm layer. Please comment if you have concerns/questions/suggestions.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-04-28","2020-04-29","1","priorized",NULL,"mrshenli","mrshenli","OTHERS"
279,"Deprecation warnings appear inconsistently","Today in PyTorch our deprecation warnings sometimes use the TORCH_WARN macro and other times the TORCH_WARN_ONCE macro, creating inconsistent behavior. This issue is to discuss what behavior deprecation warnings should have. Two suggestions are:

- They should always warn once. Users complain of excessive warnings and they can spam logs. 
- They should always warn. Ensuring users see deprecation warnings is important, and users should address deprecated functionality to silence the warnings.

Although there may be reasonable schemes that sometimes warn and other times warn once.

The issue that prompted this discussion is https://github.com/pytorch/pytorch/issues/37065, which provides additional context on the user experience. 

cc @ngimel, @ilia-cher, @dzhulgakov, @gchanan","2020-04-26","2021-08-31","492","priorized",NULL,"Mike Ruberry","dzhulgakov","OTHERS"
280,"Seemingly unused functions of TH: THDiskFile.h","Seems all functions inside are unused","2020-04-21","2020-05-11","20","priorized",NULL,"Vadim Kantorov","ngimel","OTHERS"
281,"check-doxygen.sh suppress stderr output even when the command fails","Steps to reproduce:
1. Run `docs/cpp/source/check-doxygen.sh` without having doxygen present on your system

Expected result: error message saying doxygen is not supported

Actual result:
```       
+ popd                                                                                                                                            
~/local/pytorch-tmp                                                                                                                               
+ doxygen                                                                                                                                         
(/home/ezyang/local/pytorch-tmp-env) [ezyang@devvm066.ash0 ~/local/pytorch-tmp]
```","2020-04-20","2020-05-19","29","priorized",NULL,"Edward Z. Yang","Edward Z. Yang","OTHERS"
282,"refactor test/quantization/test_backward_compatibility.py with bundled_inputs","We have support for bundled inputs in
https://github.com/pytorch/pytorch/blob/master/torch/utils/bundled_inputs.py#L17
PR: https://github.com/pytorch/pytorch/pull/35631
we can refactor the backward_compatibility tests to use this.

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","2020-04-20","2023-11-17","1306","priorized",NULL,"Jerry Zhang","andrewor14","PROJECT PATTERNS"
283,"Retire torch/cuda/nccl.py","While investigating #36870, I just realized that there is another implementation to wrap NCCL APIs in `pytorch/torch/cuda/nccl.py` added > 2 years ago. These set of APIs are not tests or documented. We probably should just get rid of them.

cc @ngimel @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar","2020-04-20","2020-05-07","17","priorized",NULL,"mrshenli","ngimel","OTHERS"
284,"Make test_distributed use MultiProcessingTestCase and enable spawn mode","## üöÄ Feature
The tests in `test_distributed.py` currently have their own setup for multiprocessing and that has already resulted in some unforseen issues such as https://github.com/pytorch/pytorch/pull/36542. The rest of our distributed and RPC tests use the methods defined in `common_distributed.py`, so we should move test_distributed to do this too. 

We should then use this tooling to enable spawn mode for these tests, which will give us TSAN coverage.


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar","2020-04-15","2020-10-01","169","priorized",NULL,"Rohan Varma","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
285,"quantized::batch_norm2d should exist","Currently it is only referencible as quantized::batch_norm (which is asymmetric with how the 3d naming works)

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","2020-04-13","2020-04-14","1","priorized",NULL,"Edward Z. Yang","supriyar","SOFTWARE ARCHITECTURE, PATTERNS AND ARCHITECTURAL STYLES"
286,"quantized::conv2d and variants don't have explicit schema","```
    c10::RegisterOperators()
        .op(""quantized::conv2d"",
            c10::RegisterOperators::options().kernel<QConvInt8<2, false>>(
                DispatchKey::QuantizedCPU))
        .op(""_quantized::conv2d"",
            c10::RegisterOperators::options().kernel<QConvInt8<2, false>>(
                DispatchKey::QuantizedCPU))
        .op(""quantized::conv2d_relu"",
            c10::RegisterOperators::options().kernel<QConvInt8<2, true>>(
                DispatchKey::QuantizedCPU))
        .op(""_quantized::conv2d_relu"",
            c10::RegisterOperators::options().kernel<QConvInt8<2, true>>(
                DispatchKey::QuantizedCPU))
        .op(""quantized::conv3d"",
            c10::RegisterOperators::options().kernel<QConvInt8<3, false>>(
                DispatchKey::QuantizedCPU))
        .op(""quantized::conv3d_relu"",
            c10::RegisterOperators::options().kernel<QConvInt8<3, true>>(
                DispatchKey::QuantizedCPU));
```

While omitting the schema is a reasonable thing that can be done for user-defined operators, all operators in core PyTorch should have explicit schema.

cc @jerryzh168 @jianyuh @dzhulgakov @raghuramank100 @jamesr66a","2020-04-13","2020-04-17","4","priorized",NULL,"Edward Z. Yang","ezyang","SOFTWARE ARCHITECTURE, PATTERNS AND ARCHITECTURAL STYLES"
287,"Thread Joining Consistency in RPC Agent","Currently, `ProcessGroupAgent` and `ThriftRpcAgent` join their background threads and clean up state in their `shutdown` functions, whereas their parent class `RpcAgent` joins its background threads in a `cleanup` function that is only called by the destructor. 

Additionally, the atomic variable indicating whether or not the RPC Agent is running is set by the RPC Agent but unset by the derived classes in their `shutdown` function. The thread joining inconsistency causes a thread leak exposed by TSAN, where the `RpcAgent` destructor isn't properly called (I've verified this through logging). 

A more consistent shutdown implementation would be similar to how `start` works, as follows:
* `RpcAgent` implements `shutdown` and derived classes override the pure virtual `shutdownImpl`
* `RpcAgent::shutdown` cleans up generic state, unsets the atomic running variable, and calls the derived class `shutdownImpl`
* Derived classes clean up backend-specific state in their `shutdownImpl` implementations. We do not enforce how the derived classes implement this (no need to unset atomic)
* `RpcAgent` destructor only calls `shutdown` if the running atomic is set to True

With the above, the user facing API is still `shutdown`. It fixes the TSAN issues with thread joining, and also works in edge cases like `test_duplicate_names` where the `RpcAgent` object is constructed but `ProcessGroupAgent` is not.


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley","2020-04-08","2020-04-09","1","priorized",NULL,"Omkar Salpekar","osalpekar","OTHERS"
288,"Only Schedule Retries when RPC Agent is not shutdown","Currently, RPC retries can be scheduled even after the RPC Agent has been shutdown. https://github.com/pytorch/pytorch/pull/35263 handles errors if a send attempt is made for a retriable RPC, but an additional guard to not even schedule retries could be helpful.

This is safe because the guard will depend on the `rpcAgentRunning_` atomic. Even if the atomic is toggled to false right after scheduling: 
1. If the retry thread is processing retries, #35263 will ensure that performing sends when the agent is shutdown is handled gracefully.
1. If the retry thread is not processing retries (sleeping), then the retry function will simply return upon waking up and not process any further retries (https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/rpc_agent.cpp#L85).

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-04-01","2020-05-05","34","priorized",NULL,"Omkar Salpekar","osalpekar","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
289,"torch.onnx.export Pad ops with ONNX opset 11","## üêõ Bug

<!-- A clear and concise description of what the bug is. -->

I don't know if this is a bug exactly, but between ONNX opset 10 and 11, there was a change to Pad ops making the `pads` an `input` to the node instead of an `attribute`.

This causes the ONNX graphs exported from PyTorch to be pretty complicated, and `torch.onnx.export(model, ..., do_constant_folding=True)` doesn't provide much help to simplify it.

However,[onnx-simplifier](https://github.com/daquexian/onnx-simplifier) does do a good job of simplifying the model back into a single Pad node.

The complex models generated by `torch.onnx.export(..., opset_version=11)` are causing some issues with the TensorRT ONNX parser.

There's one sample issue that discusses the problem a bit more, and that's where the sample code is below is taken from: https://github.com/NVIDIA/TensorRT/issues/439

## To Reproduce

Steps to reproduce the behavior:

1. Run the following code snippet with `OPSET=10`
2. Run the following code snippet with `OPSET=11`
3. Observe the difference between the generated ONNX graphs.

```python
import onnx
import argparse
import torch
import torch.nn as nn

OPSET = 10
# OPSET = 11

class MinimalModel(nn.Module):
    def __init__(self):
        super(MinimalModel, self).__init__()
        self.constant_zero_pad = nn.ConstantPad2d((1, 0, 0, 0), 0)

    def forward(self, input_tensor):
        return self.constant_zero_pad(input_tensor)

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description='PSMNet')
    parser.add_argument('output_onnx')
    args = parser.parse_args()

    minimal_model = MinimalModel()
    minimal_model = nn.DataParallel(minimal_model)
    minimal_model.cuda()

    # Random deep feature
    input_tensor = torch.rand((1, 32, 128, 128))
    # Check model can do a forward pass
    minimal_model(input_tensor)
    # Export to onnx
    torch.onnx.export(
        minimal_model.module,
        (input_tensor),
        args.output_onnx,
        export_params=True, verbose=True, training=False, opset_version=OPSET
    )

    original_model = onnx.load(args.output_onnx)
    onnx.checker.check_model(original_model)
```

<!-- If you have a code sample, error messages, stack traces, please provide it here as well -->

## Expected behavior

<!-- A clear and concise description of what you expected to happen. -->

I expect a simple ONNX graph with a INPUT -> PAD -> OUTPUT when exporting the above model with `torch.onnx.export(..., onnx_opset=11)`

## Environment

 - PyTorch Version (e.g., 1.0): 1.4
 - OS (e.g., Linux): Ubuntu 18.04
 - How you installed PyTorch (`conda`, `pip`, source): pip
 - Python version: 3.6
 - CUDA/cuDNN version: 10.1/7.6
 - GPU models and configuration: V100
 - Any other relevant information: 

## Additional context

Model exported with ONNX Opset 10

![model-opset10](https://user-images.githubusercontent.com/21284872/77710946-c9cd4900-6f8c-11ea-93e0-0a8954f32697.PNG)

Model exported with ONNX Opset 11 + `do_constant_folding=True`

![model-constant-folding-opset11](https://user-images.githubusercontent.com/21284872/77710947-cb970c80-6f8c-11ea-8f4e-25af2612862f.PNG)


<!-- Add any other context about the problem here. -->


cc @suo @houseroad @spandantiwari @lara-hdr @BowenBao @neginraoof","2020-03-26","2023-07-15","1206","priorized",NULL,"Ryan McCormick","ahangchen","OTHERS"
290,"Autograd profiler never frees its underlying RangeEventLists","## üêõ Bug

The autograd profiler uses `shared_ptr`s to `RangeEventLists` to keep track of events (such as operator invocations) that are being profiled. These lists are only GC'd when their use count goes to 1: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/profiler.cpp#L231

However, in `getEventList` since we copy these `shared_ptr` by value the use count will always be 2 and thus will not be GC'd. Looking at the history https://github.com/pytorch/pytorch/blob/e7fe64f6a65cd427e503491f192c14476e18033b/torch/csrc/autograd/profiler.cpp, it looks like this has been the case for a long time and the events were not getting GC'd.

 I'm not sure about the potential repercussions of this, but it seems like it could lead to some unexpected extra memory usage.

## To Reproduce

Add `LOG(INFO) << list.use_count()` in `disableProfiler()` in `torch/csrc/autograd/profiler.cpp`. Run the profiler and see that the use count will be 2 even if only one thread uses the list, because it is copied

## Expected behavior

`RangeEventList`s should be GC'd when there is no thread holding a ref to it","2020-03-25","2020-07-30","127","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
291,"[autograd] multithread autograd follow up tasks","Record several follow ups that I will do after https://github.com/pytorch/pytorch/pull/33157

Make this high priority to flag, I would immediately work on follow ups after the above PR landed.

- [ ] save/restore the thread local ready queue (as well as other TLs) when we do async run of the engine (to address https://github.com/pytorch/pytorch/pull/33157#discussion_r393933924)

- [ ] use the new c10 threadpool init_thread api, to create PyThreadState there initially before any launch of the threads, to correct capture any python errors underlying. Context: https://github.com/pytorch/pytorch/pull/34845

- [ ] think of more test cases coverage to test the engine in mulithreaded / distributed case. 

cc @ezyang @gchanan @zou3519 @SsnL @albanD @gqchen","2020-03-18","2022-02-07","691","high priority",NULL,"Wanchao","Wanchao","OTHERS"
292,"Merge rpc.Future and rpc._pyFuture in torch.distributed.rpc package","Right now, if users call rpc_async(.., python_funciton, ...), it will return rpc.Future that is binded to Future<Message> in C++ land; if users call rpc_async(.., torch_function, ..), it will return rpc._pyFuture that is binded to ivalue::Future wrapper. 

Once c10::ivalue::Future and torch::utils::Future are merged in #34997, we should merge return type of rpc_async(.., python_funciton, ...) and rpc_async(.., torch_function, ..). e.g, make  rpc_async(.., python_funciton, ...)  to return Future<PyObject>,  rpc_async(.., torch_function, ..) to return Future<IValue>




cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-03-18","2020-05-09","52","priorized",NULL,"Yanli Zhao","Yanli Zhao","OTHERS"
293,"[Master Plan] Merge c10::ivalue::Future and torch::utils::Future<T>","Ideally we should have one Future abstraction in PyTorch codebase to have unified code paths regarding Future usage. 

torch::utils::Future<T> is a generic Future, we can specialize it as torch::utils::Future[IValue] and make torch::utils::Future[IValue] to be an IValue. After that, we can kill c10::ivalue::Future implementation

cc @suo @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar @ilia-cher @wanchaol ","2020-03-18","2021-02-19","338","priorized",NULL,"Yanli Zhao","rohan-varma","SOFTWARE ARCHITECTURE, PROJECT PATTERNS"
294,"Enhance failure reporting for distributed tests","## üöÄ Feature
Currently the way many of our tests fail in distributed/RPC are non-intuitive and time consuming. For example, if only one process encounters an exception, then we don't actually get that error back until all other processes are done timing out. Therefore, it is often the case that we have a simple test that fails, but it can take minutes to get the actual failure back, since we wait until the other processes time out.

We should fix this by immediately exiting if the unit test fails in one subprocess. 

## Motivation

This will improve our test productivity by a lot, since we can get quicker error messages and speed up debugging.


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley","2020-03-03","2020-07-30","149","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
295,"Improve ProcessGroupRpcBackendOptions Python constructor","Currently, applications need to do the following

```python
op = ProcessGroupRpcBackendOptions()
op.rpc_timeout = rpc_timeout
op.init_method = init_method
op.num_send_recv_threads = 32
init_rpc(...., rpc_backend_options=op)
```

We should make this easier, e.g.,

```python
init_rpc(...., rpc_backend_options=ProcessGroupRpcBackendOptions(num_send_recv_threads=32))
```

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-03-02","2020-03-03","1","priorized",NULL,"mrshenli","mrshenli","OTHERS"
296,"Timeouts don't work with rpc.remote/rref.to_here()","## üêõ Bug

In the RPC layer, timeouts currently only work for calls to `rpc.rpc_sync` and `rpc.rpc_async`, but not `rpc.remote()` or (`to_here` on the created RRef).

The reason is that `rpc.remote()` does not block and returns immediately, and then calling `to_here` results in the future created by `rpc.remote()` being completed with an exception in another thread, so we get the following error:

```
> terminate called after throwing an instance of 'std::runtime_errorterminate called after throwing an instance of ''
> std::runtime_error'
>   what():  RPC ran for more than 10 milliseconds and timed out.
>   what():  RPC ran for more than 10 milliseconds and timed out.
> W0225 21:24:58.467447 79400 ExceptionTracer.cpp:182] Invalid trace stack for exception of type: std::runtime_error
> terminate called after throwing an instance of 'std::runtime_errorW0225 21:24:58.467469 79401 ExceptionTracer.cpp:182] Invalid trace stack for exception of type: std::runtime_error
> '
>   what():  RPC ran for more than 10 milliseconds and timed out.
> terminate called after throwing an instance of 'std::runtime_error'
>   what():  RPC ran for more than 10 milliseconds and timed out.
> FAIL
```
## To Reproduce

```
@dist_init
    def test_remote_timeout(self):
        dst_rank = (self.rank + 1) % self.world_size
        dst_worker = ""worker{}"".format(dst_rank)
        rpc._set_rpc_timeout(timedelta(milliseconds=10))
        with self.assertRaisesRegex(RuntimeError, ""RPC ran for more than""):
            rref = rpc.remote(dst_worker, my_sleep_func, args=(2,))
            rref.to_here()
```

As we can see The future is timing out correctly but it's happening in a different thread, resulting in an uncaught exception in the thread.

## Expected behavior
Open to discussion on this. Should `to_here()` throw with the timeout if any futures corresponding to the RRef (such as confirming the RRef on the remote end)?


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-02-26","2020-06-11","106","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
297,"Race during ungraceful shutdown of RPC agent","## üêõ Bug

During RPC communication, if one agent is in the process of ungracefully shutting down, it may still be handling internal messages such as a request to clean up the `DistAutogradContext` on its node. However, in the beginning of shutdown we set `rpcRunning_ = false` so this agent can no longer send out messages, but it may try to send out messages while processing this internal request. This results in a crash:

```
E0224 16:22:34.651338 693062 request_callback.cpp:39] Received error while processing request type 19: ProcessGroupAgent hasn't started. (send at caffe2/torch/csrc/distributed/rpc/process_group_agent.cpp:294)
frame #0: <unknown function> + 0x21adf (0x7f07b4c34adf in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #1: std::function<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > ()>::operator()() const + 0x4d (0x7f07b4c3406d in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #2: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x43 (0x7f07b4c34363 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #3: torch::distributed::rpc::ProcessGroupAgent::send(torch::distributed::rpc::WorkerInfo const&, torch::distributed::rpc::Message&&) + 0x1d5 (0x7f07f5fde595 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #4: torch::distributed::autograd::DistAutogradContainer::sendReleaseContextRpc(long) + 0x1b1 (0x7f07ef555481 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_libtorch.so)
frame #5: torch::distributed::autograd::DistAutogradContainer::releaseContextIfPresent(long) + 0xc4 (0x7f07ef555254 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binI0224 16:22:34.648531 679221 process_group_agent.cpp:276] Rank 2 calling waitWorkComplete
I0224 16:24:12.188122 679221 process_group_agent.cpp:278] Rank 2 calling listenerThread join
I0224 16:24:12.188141 679221 process_group_agent.cpp:280] Listener thread joining, exiting shutdown() on rank 2
/data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/torch/_utils_internal.py:79: DeprecationWarning: This is a NOOP in python >= 3.7, its just too dangerous with how we write code at facebook. Instead we patch os.fork and multiprocessing which can raise exceptions if a deadlock would happen. 
  threadSafeForkRegisterAtFork()
ary,link-tree/libcaffe2_libtorch.so)
frame #6: torch::distributed::rpc::RequestCallbackImpl::processRpc(torch::distributed::rpc::RpcCommandBase&, torch::distributed::rpc::MessageType, long) const + 0x260a (0x7f07f666965a in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #7: torch::distributed::rpc::RequestCallbackImpl::processMessage(torch::distributed::rpc::Message&) const + 0x87 (0x7f07f666a417 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #8: torch::distributed::rpc::RequestCallback::operator()(torch::distributed::rpc::Message&) const + 0x46 (0x7f07efa85ac6 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_libtorch.so)
frame #9: <unknown function> + 0x6ed425 (0x7f07f6007425 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #10: <unknown function> + 0x6ed2e2 (0x7f07f60072e2 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #11: <unknown function> + 0x6ed252 (0x7f07f6007252 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #12: <unknown function> + 0x6ed210 (0x7f07f6007210 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #13: <unknown function> + 0x6ed1a6 (0x7f07f60071a6 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #14: <unknown function> + 0x6ececd (0x7f07f6006ecd in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2__C_impl_cuda.so)
frame #15: std::function<void ()>::operator()() const + 0x3e (0x7f07b4c44e9e in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #16: c10::ThreadPool::main_loop(unsigned long) + 0x28c (0x7f07b4c44bac in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #17: <unknown function> + 0x34028 (0x7f07b4c47028 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #18: <unknown function> + 0x33fbd (0x7f07b4c46fbd in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #19: <unknown function> + 0x33f4d (0x7f07b4c46f4d in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #20: <unknown function> + 0x33f25 (0x7f07b4c46f25 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #21: <unknown function> + 0x33ef5 (0x7f07b4c46ef5 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #22: <unknown function> + 0x33c89 (0x7f07b4c46c89 in /data/users/rvarm1/fbsource/fbcode/buck-out/dev/gen/caffe2/test/distributed/rpc/dist_autograd_spawn#binary,link-tree/libcaffe2_c10_c10.so)
frame #23: <unknown function> + 0xc86e0 (0x7f07a3ea16e0 in /usr/local/fbcode/platform007/bin/../lib/libstdc++.so.6)
frame #24: <unknown function> + 0x76b6 (0x7f0802fbc6b6 in /usr/local/fbcode/platform007/bin/../lib/libpthread.so.0)
frame #25: clone + 0x3f (0x7f0802599ebf in /usr/local/fbcode/platform007/bin/../lib/libc.so.6)
ERROR
```
## To Reproduce

[TODO - still working on an easily reproducible test case]


cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @gqchen @aazzolini @rohan-varma @xush6528 @jjlilley @osalpekar","2020-02-24","2020-04-22","58","priorized",NULL,"Rohan Varma","rohan-varma","OTHERS"
298,"Some c10d tests are broken on MacOS","## üêõ Bug

`test_ignored_output` and `test_ignored_output_with_unused_parameters` crash on MacOS, when pytorch is built locally:

```
Traceback (most recent call last):
  File ""/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py"", line 178, in wrapper
    fn(self)
  File ""test_c10d.py"", line 2786, in test_ignored_output
    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)
AttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'
exiting process with exit code: 10
ERROR:root:Caught exception:
Traceback (most recent call last):
  File ""/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py"", line 178, in wrapper
    fn(self)
  File ""test_c10d.py"", line 2786, in test_ignored_output
    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)
AttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'
exiting process with exit code: 10
FERROR:root:Caught exception:
Traceback (most recent call last):
  File ""/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py"", line 178, in wrapper
    fn(self)
  File ""test_c10d.py"", line 2828, in test_ignored_output_with_unused_parameters
    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)
AttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'
exiting process with exit code: 10
ERROR:root:Caught exception:
Traceback (most recent call last):
  File ""/Users/rvarm1/Desktop/pytorch/torch/testing/_internal/common_distributed.py"", line 178, in wrapper
    fn(self)
  File ""test_c10d.py"", line 2828, in test_ignored_output_with_unused_parameters
    process_group = c10d.ProcessGroupGloo(store, self.rank, self.world_size)
AttributeError: module 'torch.distributed' has no attribute 'ProcessGroupGloo'
exiting process with exit code: 10
```
## To Reproduce
Build pytorch locally on MacOS (have to build without gloo) and run `python test/test_c10d.py -v`. These 2 tests fail, but the other tests that require gloo are correctly skipped. I think this is missing the requires_gloo decorator.

I am not sure why these aren't failing in the MacOS CI currently. Perhaps we have some other flag that is correctly set in CI so these tests get skipped

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar","2020-02-19","2020-03-16","26","priorized",NULL,"Rohan Varma","rohan-varma","PROJECT PATTERNS"
299,"Make distributed autograd and distributed optimizer APIs functional.","Code while using distributed autograd and distributed optimizer currently looks like this:

```
with dist_autograd.context() as context_id:
  // forward pass.
  dist_autograd.backward([loss.sum()])
  dist_optim.step()
```

`dist_autograd.backward` and `dist_optim.step` currently assume the thread local `context_id` as the context id that we would like to work with. This is a bit magical to users currently and relies on an implementation detail where `context_id` is a thread_local. We should make these APIs more functional where we pass the context_id explicitly. This way the APIs are more functional and it also provides more flexibility for users such that they can call these APIs in a separate thread if they wish to.

cc @pietern @mrshenli @pritamdamania87 @zhaojuanmao @satgera @rohan-varma @gqchen @aazzolini @xush6528 @osalpekar @jjlilley","2020-02-19","2020-02-27","8","priorized",NULL,"Pritam Damania","Pritam Damania","OTHERS"
300,"Delete zero_dim_dispatch_when_scalar","This is for legacy TH. It is currently used by the following TH functions:

- [ ] `_th_scatter_` #24621 #24757
- [ ] `_th_fmod` #24565 #24701
- [x] `_th_remainder` #24753 #24615

cc @ezyang @gchanan @zou3519","2020-02-07","2020-05-04","87","high priority",NULL,"Edward Z. Yang","ezyang","OTHERS"
301,"Migrate `masked_select` from TH to ATen (CUDA)","Porting TH operators is essential for code simplicity and performance reasons.

Porting guides and Q&A are available in umbrella issue: #24507

Feel free to add @VitalyFedyunin as a reviewer to get a prioritized review.

cc @ezyang @gchanan @zou3519 @anjali411 @dylanbespalko as this affects complex tensor bring up","2020-02-06","2020-04-10","64","high priority",NULL,"Edward Z. Yang","kurtamohler","OTHERS"
